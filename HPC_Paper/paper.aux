\relax 
\pgfsyspdfmark {pgfid1}{4736286}{17717152}
\citation{grobelny2007fase}
\citation{wang2009simulation}
\citation{wang2013towards}
\citation{snavely2002framework}
\citation{bailey2005performance}
\citation{barker2009using}
\citation{ye2010analyzing}
\citation{bailey2005performance}
\citation{beckman2008benchmarking}
\citation{de2007identifying}
\citation{lofstead2010managing}
\@writefile{toc}{\contentsline {section}{\numberline {1}\uppercase {Introduction}}{2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces A description of the system parameters being considered in the IOZone tests. There are three categorical settings and four continuous settings. Notice that the ranges of values for continuous settings are different.}}{3}}
\newlabel{tab:data_type}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Approximation}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}\ \ \hspace  {1.5pt}{Multivariate Adaptive Regression Splines}}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}\ \ \hspace  {1.5pt}{Multi-Layer Perceptron Regressor}}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}\ \ \hspace  {1.5pt}{Support Vector Regressor}}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Interpolation}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}\ \ \hspace  {1.5pt}{Delaunay}}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}\ \ \hspace  {1.5pt}{Linear Shepard}}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\uppercase {Methodology}}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Data}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Dimensional Analysis}{3}}
\bibstyle{scsproc}
\bibdata{paper}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Most models experience a decay in predictive performance as the dimension of the data increases. This is expected because higher dimension input has exponentially more possible patterns to identify. The MLP Regressor and SVR experience the worst decay in performance with increasing dimension.}}{4}}
\newlabel{tab:perf_by_dim}{{2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Prediction}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces In this figure, it can be seen how the distribution of prediction errors for each algorithm is affected by the quantity of training data provided. Notice that for all amounts of training data, Delaunay has the largest likelihoods around 0 error.}}{5}}
\newlabel{fig:tt_ratio}{{1}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces All models experience a reduction in error with increasing amounts of training data. This suggests that the data being modeled is pattern-dense, over-fitting is \textit  {not} occurring, and that oversimplifications will tend towards worse predictions.}}{6}}
\newlabel{tab:perf_by_tt}{{3}{6}}
\bibcite{bailey2005performance}{\citeauthoryear {Bailey and Snavely}{Bailey and Snavely}{2005}}
\bibcite{barker2009using}{\citeauthoryear {Barker, Davis, Hoisie, Kerbyson, Lang, Pakin, and Sancho}{Barker et\nobreakspace  {}al.}{2009}}
\bibcite{beckman2008benchmarking}{\citeauthoryear {Beckman, Iskra, Yoshii, Coghlan, and Nataraj}{Beckman et\nobreakspace  {}al.}{2008}}
\bibcite{de2007identifying}{\citeauthoryear {De, Kothari, and Mann}{De et\nobreakspace  {}al.}{2007}}
\bibcite{grobelny2007fase}{\citeauthoryear {Grobelny, Bueno, Troxel, George, and Vetter}{Grobelny et\nobreakspace  {}al.}{2007}}
\bibcite{lofstead2010managing}{\citeauthoryear {Lofstead, Zheng, Liu, Klasky, Oldfield, Kordenbrock, Schwan, and Wolf}{Lofstead et\nobreakspace  {}al.}{2010}}
\bibcite{snavely2002framework}{\citeauthoryear {Snavely, Carrington, Wolter, Labarta, Badia, and Purkayastha}{Snavely et\nobreakspace  {}al.}{2002}}
\bibcite{wang2009simulation}{\citeauthoryear {Wang, Butt, Pandey, and Gupta}{Wang et\nobreakspace  {}al.}{2009}}
\bibcite{wang2013towards}{\citeauthoryear {Wang, Khasymski, Krish, and Butt}{Wang et\nobreakspace  {}al.}{2013}}
\bibcite{ye2010analyzing}{\citeauthoryear {Ye, Jiang, Chen, Huang, and Wang}{Ye et\nobreakspace  {}al.}{2010}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\uppercase {Results}}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}I/O Throughput Mean}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}I/O Throughput Variance}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Increasing Dimension}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\uppercase {Discussion}}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Modeling the System}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Quantifying Variability}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Extending the Analysis}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\uppercase {Future Work}}{7}}
