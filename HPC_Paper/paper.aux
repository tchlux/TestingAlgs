\relax 
\pgfsyspdfmark {pgfid1}{4736286}{17717152}
\@writefile{toc}{\contentsline {section}{\numberline {1}\uppercase {Introduction}}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Approximation}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}\ \ \hspace  {1.5pt}{Multivariate Adaptive Regression Splines}}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}\ \ \hspace  {1.5pt}{Multi-Layer Perceptron Regressor}}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}\ \ \hspace  {1.5pt}{Support Vector Regressor}}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Interpolation}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}\ \ \hspace  {1.5pt}{Delaunay}}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}\ \ \hspace  {1.5pt}{Linear Shepard}}{2}}
\bibstyle{scsproc}
\bibdata{paper}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces }}{3}}
\newlabel{tab:data_type}{{1}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\uppercase {Related Work}}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\uppercase {Methodology}}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Data}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Dimensional Analysis}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Prediction}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces In this figure, it can be seen how the distribution of prediction errors for each algorithm is affected by the quantity of training data provided. Notice that for all amounts of training data, Delaunay has the largest likelihoods around 0 error.}}{4}}
\newlabel{fig:tt_ratio}{{1}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Most models experience a decay in predictive performance as the dimension of the data increases. This is expected because higher dimension input has exponentially more possible patterns to identify. The MLP Regressor and SVR experience the worst decay in performance with increasing dimension.}}{5}}
\newlabel{tab:perf_by_dim}{{2}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces All models experience a reduction in error with increasing amounts of training data. This suggests that the data being modeled is pattern-dense, over-fitting is \textit  {not} occurring, and that oversimplifications will tend towards worse predictions.}}{6}}
\newlabel{tab:perf_by_tt}{{3}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\uppercase {Results}}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}I/O Throughput Mean}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}I/O Throughput Variance}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Increasing Dimension}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\uppercase {Discussion}}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Modeling the System}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Quantifying Variability}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Extending the Analysis}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {6}\uppercase {Future Work}}{7}}
