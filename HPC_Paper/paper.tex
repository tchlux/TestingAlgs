%**************************************************************************
%* SpringSim 2017 Author Kit
%*
%* Word Processing System: TeXnicCenter and MiKTeX
%*
%**************************************************************************

\documentclass{scspaperproc}

\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{mathptmx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{amsthm}
\usepackage{algorithm}     % needed for the algorithm block
\usepackage{algpseudocode} % algorithmicx psuedo-code styling
%\usepackage[pdftex,colorlinks=true,urlcolor=blue,citecolor=black,anchorcolor=black,linkcolor=black]{hyperref}
%% \usepackage[dvips,colorlinks=true,urlcolor=blue,citecolor=black,%
%% anchorcolor=black,linkcolor=black]{hyperref}

% custom hyphenation rules
\usepackage{hyphenat}
\hyphenation{op-tical net-works semi-conduc-tor}

% theorem style
\newtheoremstyle{scsthe}% hnamei
{8pt}% hSpace abovei
{8pt}% hSpace belowi
{\it}% hBody fonti
{}% hIndent amounti1
{\bf}% hTheorem head fontbf
{.}% hPunctuation after theorem headi
{.5em}% hSpace after theorem headi2
{}% hTheorem head spec (can be left empty, meaning `normal')i
\theoremstyle{scsthe}
\newtheorem{theorem}{Theorem}
\renewcommand{\thetheorem}{\arabic{theorem}}
\newtheorem{corollary}[theorem]{Corollary}
\renewcommand{\thecorollary}{\arabic{corollary}}
\newtheorem{definition}{Definition}
\renewcommand{\thedefinition}{\arabic{definition}}
\newcommand{\rpm}{\raisebox{.2ex}{$\scriptstyle\pm$}}

% avoid overrunning the right margin
\sloppy

%% ***** NOTE *****

%% The use of the long citation format (e.g. "Brown and Edwards
%% (1993)" rather than "[5]") and at the same time using the hyperref
%% package can lead to hard to trace bugs in case the citation is
%% broken accross the line (usually this will mark the entire
%% paragraph as a hyperlink (clickable) which is easily noticeable and
%% fixed if using colorlinks, but not if the color is black -- as it
%% is now). Worse yet, if a citation spans page boundary, LaTeX
%% compilation can fail, with an obscure error message. Since this
%% depends a lot on the flow of the text and wording, these bugs come
%% and go and can be extremely hard for a beginner to trace. The error
%% message can look like this:
%%
%%    ! pdfTeX error (ext4): \pdfendlink ended up in different nesting
%%    level than \pdfstartlink.  \AtBegShi@Output ...ipout \box
%%    \AtBeginShipoutBox \fi \fi
%%    l.174 
%%    ! ==> Fatal error occurred, no output PDF file produced!
%%
%% and can be universally fixed by putting an \mbox{} around the
%% citation in question (in this case, at line 174) and maybe adapting
%% the wording a little bit to improve the paragraph typesetting,
%% which is perhaps not immediately obvious.
%****************************************************************************

% begin document
\begin{document}

% Page header (author list)
\SCSpagesetup{Lux, Watson, Chang, Bernard, Li, Xu, Back, Butt, Cameron, and Hong}

% Conference info
\def\SCSconferenceacro{SpringSim}
\def\SCSpublicationyear{2018}
\def\SCSconferencedates{April 15-18}
\def\SCSconferencevenue{Baltimore, MD, USA}
\def\SCSsymposiumacro{HPC} % High Performance Computing Symposium

% title
\title{Predictive Modeling of I/O Characteristics \\ in High
  Performance Computing Systems}

% AUTHOR LIST
% *** NOTE: May need to adjust titlevboxsize in the preamble
\author{Thomas C. H. Lux \\ [12pt]
Dept. of Computer Science \\
Virginia Polytechnic Institute\\
\& State University \\
Blacksburg, VA 24061 \\
tchlux@vt.edu \\
\and
Layne T. Watson \\[12pt]
Dept. of Computer Science\\
Dept. of Mathematics\\
Dept. of Aerospace \& Ocean Eng.\\ 
Virginia Polytechnic Institute\\
\& State University \\
\and
Tyler H. Chang\\
Jon Bernard\\
Bo Li\\[12pt]
Dept. of Computer Science\\ 
Virginia Polytechnic Institute\\
\& State University \\
\and
Li Xu\\[12pt]
Dept. of Statistics\\ 
Virginia Polytechnic Institute\\
\& State University \\
\and
Godmar Back\\
Ali R. Butt\\
Kirk W. Cameron\\[12pt]
Dept. of Computer Science\\ 
Virginia Polytechnic Institute\\
\& State University \\
\and
Yili Hong\\[12pt]
Dept. of Statistics\\ 
Virginia Polytechnic Institute\\
\& State University \\
}

\maketitle

\section*{Abstract}

Each of high performance computing, cloud computing, and computer
security have their own interests in modeling and predicting the
performance of computers with respect to how they are configured. An
effective model might infer internal mechanics, minimize power
consumption, or maximize computational throughput of a given
system. This paper analyzes a four-dimensional dataset measuring the
input/output (I/O) characteristics of a cluster of identical computers
using the benchmark IOzone. The I/O performance characteristics are
modeled with respect to system configuration using multivariate
interpolation and approximation techniques. The analysis reveals that
accurate models of I/O characteristics for a computer system may be
created from a small fraction of possible configurations, and that
some modeling techniques will continue to perform well as the number
of system parameters being modeled increases. These results have
strong implications for future predictive analyses based on more
comprehensive sets of system parameters.

\textbf{Keywords:} Regression, approximation, interpolation,
performance modeling


%     Introduction     
%======================
\section{Introduction and related work}
\label{sec:introduction}

Performance tuning is often an experimentally complex and time
intensive chore necessary for configuring HPC systems. The procedures
for this tuning vary largely from system to system and are often
subjectively guided by the system engineer(s). Once a desired level of
performance is achieved, an HPC system may only be incrementally
reconfigured as required by updates or specific jobs. In the case that
a system has changing workloads or non stationary performance
objectives that range from maximizing computational throughput to
minimizing power consumption and system variability, it becomes clear
that a more effective and automated tool is needed for configuring
systems. This scenario presents a challenging and important
application of multivariate approximation and interpolation
techniques.

Predicting the performance of an HPC system is a challenging problem
that is primarily attempted in one of two ways: (1) build a
statistical model of the performance by running experiments on the
system at select settings or (2) run artificial experiments using a
simplified simulation of the target system to estimate architecture
and application bottlenecks. In this paper the proposed multivariate
modeling techniques rest in the first category, and they represent a
notable increase in the ability to model complex interactions between
system parameters.

Many previous works attempting to model system performance have used
simulated environments to estimate the performance of a system
\shortcite{grobelny2007fase,wang2009simulation,wang2013towards}. Some
of these works refer to statistical models as being oversimplified and
not capable of capturing the true complexity of the underlying
system. This claim is partially correct, noting that a large portion
of predictive statistical models rely on simplifying the model to one
or two parameters
\shortcite{snavely2002framework,bailey2005performance,barker2009using,ye2010analyzing}.
These limited statistical models have provided satisfactory
performance in very narrow application settings. Many of the
aforementioned statistical modeling techniques claim to generalize,
while simultaneously requiring additional code annotations, hardware
abstractions, or additional application level understandings in order
to generate models. The approach presented here requires no
modifications of the application, no architectural abstractions, nor
any structural descriptions of the input data being modeled. The
techniques used are purely mathematical and only need performance data
as input.

Among the statistical models presented in prior works
\shortciteN{bailey2005performance} specifically mention that it is
difficult for the simplified models to capture variability introduced
by I/O. System variability in general has become a problem of
increasing interest to the HPC and systems communities, however most
of the work has focused on operating system (OS) induced variability
\shortcite{beckman2008benchmarking,de2007identifying}. The work that
has focused on managing I/O variability does not use any sophisticated
modeling techniques \shortcite{lofstead2010managing}. Hence, this
paper presents a case study applying advanced mathematical and
statistical modeling techniques to the domain of HPC I/O
characteristics. The models are used to predict the mean throughput of
a system and the variance in throughput of a system. The discussion
section outlines how the techniques presented can be applied to any
performance metric and any system.

%% Multivariate models for an HPC system would be a function of the
%% tunable parameters built to accurately model some desired performance
%% metric.

%% \begin{enumerate}
%% \item The value of multivariate Modeling
%% \item The data context
%% \item The proposed method for using multivariate models
%% \item The impact of effective models
%% \end{enumerate}

In general, this paper compares five multivariate approximation
techniques that operate on inputs in $\mathbb{R}^d$ ($d$-tuples of
real numbers) and produce predicted responses in $\mathbb{R}$. In
order to provide coverage of the varied mathematical strategies that
can be employed to solve the continuous modeling problem, three of the
techniques are regression based and the remaining two are
interpolants. The sections below outline the mathematical formulation
of each technique and provide computational complexity bounds with
respect to the size (number of points and dimension) of input
data. Throughout the sections, $d$ will refer to the dimension of the
input data, $n$ is the number of points in the input data, $x^{(i)}
\in \mathbb{R}^d$ is the $i$-th input data point, $x^{(i)}_j$ is the
$j$-th component of $x^{(i)}$, and $f(x^{(i)})$ is the response value
of the $i$-th input data point.

The remainder of the paper is broken up into four major parts. Section
\ref{sec:multivariate} provides an overview of the multivariate
modeling techniques, Section \ref{sec:methodology} outlines the
methodology for comparing and evaluating the performance of the
models, Section \ref{sec:results} presents the IOzone predictions,
Section \ref{sec:discussion} discusses the obvious and subtle
implications of the models' performance, and Section
\ref{sec:conclusion} concludes and offers directions for future work.

\section{Multivariate Models}
\label{sec:multivariate}

\subsection{Regression}
Multivariate approximations are capable of accurately modeling a
complex dependence of a response (in $\mathbb{R}$) on multiple
variables (represented as a points in $\mathbb{R}^{d}$). The
approximations to some (unknown) underlying function $f: \mathbb{R}^d
\rightarrow \mathbb{R}$ are chosen to minimize some error measure
related to data samples $f(x^{(i)})$. For example, least squares
regression uses the sum of squared differences between modeled
response values and true response values as an error measure.

\subsubsection{Multivariate Adaptive Regression Splines}
This approximation was introduced in
\shortciteN{friedman1991multivariate} and subsequently improved to its
current version in \shortciteN{stanford1993fast}, called fast
multivariate adaptive regression splines (Fast MARS). In Fast MARS, a
least squares fit model is iteratively built by beginning with a
single constant valued function and adding two new basis functions at
each iteration of the form

$$ B_{2s-1}(x) = B_l(x) [c(x_i-v)]_+ ,$$
$$ B_{2s}(x) = B_k(x) [c(x_i-v)]_- ,$$

where $s$ is the iteration number, $B_l(x)$ and $B_k(x)$ are basis
functions from the previous iteration, $c, v \in \mathbb{R}$, $w_+ =
\{w, w \geq 0, 0, w < 0\},$ and $w_+ = (-w)_+$. After iteratively
constructing a model, MARS then iteratively removes basis functions
that do not contribute to goodness of fit. In effect, MARS creates a
locally component-wise tensor product approximation of the data. The
overall computational complexity of Fast MARS is $\mathcal{O}(n d
m^3)$ where $m$ is the maximum number of underlying basis
functions. This paper uses an implementation of MARS
\shortcite{rudy2017pyearth} with $m = 200$.

\subsubsection{Multilayer Perceptron Regressor}
The neural network is a well studied and widely used method for both
regression and classification tasks
\shortcite{hornik1989multilayer}. When using the rectified linear unit
(ReLU) activation function \shortcite{dahl2013improving} and training
with the BFGS minimization technique \shortcite{moller1993scaled}, the
model built by a multilayer perceptron uses layers $l : \mathbb{R}^{i}
\rightarrow \mathbb{R}^{j}$ defined by

$$ l(u) = \big( u^t W_l \big)_+ $$

where $W_l$ is the $i$ by $j$ weight matrix for layer $l$. In this
form, the multi layer perceptron (MLP) produces a piecewise linear
model of the input data. The computational complexity of training a
multi layer perceptron is $\mathcal{O}(n d m)$, where $m$ is
determined by the sizes of the layers of the network and the stopping
criterion of the BFGS minimization used for finding weights. This
paper uses the scikit-learn MLP regressor \shortcite{scikit-learn}, a
single hidden layer with 100 nodes, ReLU activation, and BFGS for
training.

\subsubsection{Support Vector Regressor}
Support vector machines are a common method used in machine learning
classification tasks that can be adapted for the purpose of regression
\shortcite{basak2007support}. How to build a support vector regressor
(SVR) is beyond the scope of this summary, but the resulting
functional fit $p : \mathbb{R}^d \rightarrow \mathbb{R}$ has the form

$$ p(x)  = \sum_{i=1}^{n}a_i K(x,x^{(i)}) + b ,$$

%% $$ \text{Minimize } \bigg\{ \frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}
%% a_i a_j K(x_i, x_j) + \epsilon \sum_{i=1}^{n} a_i - \sum_{i=1}^{n} y_i
%% a_i \bigg\} $$

%% $$ \text{Subject to } \sum_{i=1}^{n}a_i = 0 \text{ } \text{ and }
%% \text{ } a_i \in [0,C] $$

where $K$ is the selected kernel function, $a \in \mathbb{R}^n$, $b
\in \mathbb{R}$ are coefficients to be solved for simultaneously with
$b \in \mathbb{R}$, and $\epsilon$ is the error tolerance. The
computational complexity of the SVR is $\mathcal{O}(n^2dm)$, with $m$
being determined by the minimization convergence criterion. This paper
uses the scikit-learn SVR \shortcite{scikit-learn} with a polynomial
kernel function.

\subsection{Interpolation}

In some cases it is desirable to have a model that can recreate the
input data exactly. This is especially the case when the confidence in
the response values for known data is high. Both interpolation models
analyzed in this paper are based on linear functions.

\subsubsection{Delaunay}

The Delaunay method of interpolation is a well studied geometric
technique for producing an interpolant \shortcite{lee1980two}. The
Delaunay triangulation of a set of data points into simplices is such
that the sphere defined by the vertices of each simplex contains no
data points in the sphere's interior. For a $d$-simplex S with
vertices $v^{(0)}, v^{(1)}, \ldots, v^{(d)}$, $x \in S$, and data
values $f(v^{(i)})$, $i=0,\ldots,d$, $x$ is a unique convex
combination of the vertices,

$$ x = \sum_{i=0}^{d} w_i v^{(i)}; \quad \sum_{i=0}^{d} w_i = 1; \quad
w_i \geq 0; \quad i=0,\ldots,d $$

and the Delaunay interpolant to $f$ at $x$ is given by

$$ p(x) = \sum_{i=0}^{d} w_i f(v^{(i)}). $$

The computational complexity of the Delaunay triangulation (for the
implementation used here) is $\mathcal{O}(n^{\lceil d/2 \rceil})$,
which is not scalable to $d > 10$ \shortcite{sartipizadeh2016computing}.
The scipy interface \shortcite{scipy} to the QuickHull implementation
\shortcite{barber1996qhull} of the Delaunay triangulation is used here.

\subsubsection{Linear Shepard}

The linear Shepard method (LSHEP) is a blending function using local
linear interpolants, a special case of the general Shepard algorithm
\shortcite{thacker2010algorithm}. The interpolant has the form

$$ p(x) = \frac{\sum_{k=1}^{n}W_k(x)P_k(x)}{\sum_{k=1}^{n}W_k(x)} ,$$

where $W_k(x)$ is a locally supported weighting function and $P_k(x)$
is a local linear approximation to the data satisfying $P_k(x^{(k)} =
f(x^{(x)})$. The computational complexity LSHEP is
$\mathcal{O}(n^2d^3)$. This paper uses the FORTRAN95 implementation of
LSHEP in SHEPPACK \shortcite{thacker2010algorithm}.

%% %     Related Work     
%% %======================
%% \section{Related Work}
%% \begin{enumerate}
%% \item Not sure how much to include here? Shooting for thoroughness or
%%   simply necessary coverage? How much background should I expect the
%%   readers of this paper to have in the ``multivariate modeling of
%%   systems'' area?
%% \end{enumerate}

%     Methodology     
%=====================
\section{Methodology}
\label{sec:methodology}
\subsection{Data}
In order to evaluate the viability of multivariate models for
predicting system performance, this paper presents a case study of a
four-dimensional dataset produced by executing the IOzone benchmark
from \shortciteN{iozone} on a homogeneous cluster of computers. The
system performance data was collected by executing IOzone 40 times for
each of a select set of system configurations. A single IOzone
execution reports the max I/O throughput seen for the selected
test. The 40 executions for each system configuration are converted
into the mean and sample variance, both values in $\mathbb{R}$ capable
of being modeled individually by the multivariate approximation
techniques presented in Section \ref{sec:multivariate}. The summary of
data used in the experiments for this paper can be seen in Table
\ref{tab:data_type}.  Distributions of raw throughput values being
modeled can be seen in Figure \ref{fig:raw_throughput}.

\begin{table}
  \centering
  \begin{tabular}{c|c}
    \hline
    \textbf{System Parameter} & \textbf{Values}\\
    \hline
    File Size & 64, 256, 1024\\
    Record Size & 32, 128, 512\\
    Thread Count & 1, 2, 4, 8, 16, 32, 64, 128, 256\\
    Frequency & \{12, 14, 15, 16, 18, 19, 20, 21, 23, 24, 25, 27, 28, 29, 30, 30.01\} $\times 10^5$\\
    \hline
    \textbf{Response Values} & \\
    \hline
    Throughput Mean & [$2.6 \times 10^5$, $5.9 \times 10^8$]\\
    Throughput Sample Variance & [$5.9\times 10^{10} $, $4.7 \times 10^{16}$]\\
    \hline
  \end{tabular}
  \caption{A description of the system parameters being considered in
    the IOzone tests. Record size must not be greater than file size
    and hence there are only six valid combinations of the two. In
    total there are $6 \times 9 \times 16 = 864$ unique system
    configurations.}
  \label{tab:data_type}
\end{table}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth,trim={0 .5in 0 1in}]{Raw_Throughput.pdf}
  \caption{Histograms of 100-bin reductions of the PMF of I/O
    throughput mean (top) and I/O throughput sample variance
    (bottom). In the mean plot, the first 1\% bin (truncated in plot)
    has a probability mass of .45. In the variance plot, the first 1\%
    bin has a probability mass of .58. It can be seen that the
    distributions of throughputs are primarily of lower magnitude with
    occasional extreme outliers.}
  \label{fig:raw_throughput}
\end{figure}

\subsection{Dimensional Analysis}
This work utilizes an extension to standard $k$-fold cross validation
that allows for a more thorough investigation of the expected model
performance in a variety of real-world situations. Alongside
randomized splits, two extra components are considered: the amount of
training data provided, and the dimension of the input data. It is
important to consider that algorithms that perform well with less
training input also require less experimentation, although the amount
of training data required may change as a function of the number of
input dimensions, and this needs to be studied as well. The framework
used here will be referred to as a multidimensional analysis (MDA) of
the IOzone data.

\subsubsection{Multidimensional Analysis}
This procedure combines random selection of training and testing
splits with changes in the input dimension and the ratio of training
size to testing size. Given an input data matrix with $n$ rows
(points) and $d$ columns (features), the multidimensional analysis
(MDA) is performed as follows:
\begin{enumerate}
\item for all $k = 1, \ldots, d$ and for all subsets of input features
  $F = \{ j_1, j_2, \ldots j_k \}$; reduce the input data to unique
  points $z \in \mathbb{R}^k$ with $f(z) = E[ \{ f(x^{(i)}) \quad
    \forall i \quad s.t. \quad (x^{(i)}_F = z) \} ]$.
\item for all $r$ in $\{5, 10, \ldots, 95\}$; generate $N$ random
  splits $(train, test)$ of the reduced data with $r$ percentage for
  training and $100 - r$ percentage for testing.
\item when generate each random $(train, test)$ split, ensure that all
  points from $test$ are on or inside the convex hull of points in
  $train$; also ensure that the points in $train$ are well spaced.
\end{enumerate}

In order to ensure that the testing points are inside the convex hull
of training points, this paper identifies the set of (reduced
dimension) points that compose the convex hull and forcibly places
those points in the training set. In order to ensure that training
points are well spaced, this paper relies on the statistical method
for picking points from \shortciteN{amos2014algorithm} that procedes
as follows:
\begin{enumerate}
\item Generate a sequence of all pairs of points sorted by ascending
  pairwise $L_2$ distance between points, ${(x^{(i_1)},x^{(j_1)}),
    (x^{(i_2)},x^{(j_2)}), \ldots}$ such that
  $||x^{(i_k)}-x^{(j_k)}||_2 \leq ||x^{(i_{k+1})}-x^{(j_{k+1})}||_2$
\item Iteratively remove points from candidacy until only $N$ remain
  by sequentially selecting one point from the pair $(x^{(i_k)},
  x^{(j_k)})$ for $k = 1,\ldots$ if both $x^{(i_k)}$ and $x^{(j_k)}$
  are still candidates.
\end{enumerate}

Given the large number of constraints, level of reduction, and use of
randomness in the MDA procedure, occasionally $N$ unique training
testing splits may not be created or may not exist. In these cases, if
there are fewer than $N$ possible splits, those deterministically
generated splits are used. Otherwise after $3*N$ attempts, only the
unique splits are kept for analysis. The MDA procedure has been
implemented in python3 while most regression and interpolation methods
are FORTRAN wrapped with python. All randomness has been seeded for
reproducability.

For any input feature subset $F$ (of size $k$) and selected value of
$r$, MDA will generate up to $N$ predictions that each multivariate
model has made for each point $z^{(i)} \in \mathbb{R}^k$. There may be
fewer than $N$ predictions made for any given point. Points in the
convex hull for the selected subest of features will always be used
for training, never for testing. Points that do not have any close
neighbors will often be used for training in order to ensure well
spacedness. Finally, as mentioned before, some subsets of features do
not readily generate $N$ unique training and testing splits.

MDA results in a distribution of possible estimates that a given
multivariate model could make for a point in $\mathbb{R}^k$, $k \leq
d$ given feature set $F$. The summary results presented in this work
use the expected value of the distributions at each point as the model
estimate for error analysis.

%% is open source, and is freely available at the url
%% \textit{https://github.com/tchlux/VarSys/HPC\_Paper/Code/multi\_dim\_analysis.py}

%% \begin{enumerate}
%% \item Cycling the categorical settings
%% \item Selecting subsets of 1,2,3 up to 4 dimensions
%% \item Cycling different training : testing ratios (5:95 $\rightarrow$ 95:5)
%% \item Generating 200 random training : testing splits
%% \item Ensuring the testing points are on/inside the convex hull of the training.
%% \item Ensuring the training points are well-spaced.
%% \end{enumerate}

%% \subsection{Prediction}
%% \begin{enumerate}
%% \item For each file generated from the dimensional analysis, train on
%%   the training data, evaluate at the testing data points
%% \end{enumerate}

%     Results     
%=================
\section{Results}
\label{sec:results}

A naive multivariate prediction technique such as nearest neighbor
could experience relative errors in the range $[0,10^m]$ where $m$ is
$log_{10}(\text{max}_f - \text{min}_f)$ when modeling a function $f$
from data. The IOzone mean data response values span 3 orders of
magnitude (as can be seen in Table \ref{tab:data_type}) while variance
data response values span 6 orders of magnitude. It is expected
therefore, that all studied multivariate models perform better than a
naive approach, achieving relative errors strictly less than $10^3$
for mean throughput and $10^6$ when for throughput sample
variance. Ideally, models will yield relative errors significantly
smaller than 1.

\subsection{I/O Throughput Mean}

Almost all multivariate models analyzed make predictions with a
relative error less than 1 for most system configurations when
predicting the mean I/O throughput of a system given varying amounts
of training data. The overall best of the multivariate models,
Delaunay, consistently makes predictions with relative error less than
$.05$ (5\% error). In figure \ref{fig:mean_tt_ratio} it can also be
seen that the Delaunay model consistently makes good predictions even
with as low as 5\% training data (43 system configurations) regardless
of the dimension of the data.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth,trim={0 1in 0 1in}]{Mean_TT_Ratio.pdf}
  \caption{These box plots show the prediction error of means with
    increasing amounts of training data provided to the models. Notice
    that the response values being predicted span 3 orders of
    magnitude and hence relative error in that range would not be
    unexpected. For SVR the top box whisker goes from around 100 to 50
    from left to right and is chopped in order to maintain focus on
    the more performant models.}
  \label{fig:mean_tt_ratio}
\end{figure}

\subsection{I/O Throughput Variance}

The prediction results for variance resemble that of predicting I/O
throughput mean, with each of the errors being scaled proportional to
the change in scale of the response values. Delaunay remains the best
overall predictor with LSHEP closely competing, but yielding much more
outliers.

\subsection{Increasing Dimension}

As can be seen in figure \ref{fig:mean_dim}, all of the models suffer
increasing error rates in higher dimension. This is expected, because
the number of possible interactions to model grows
exponentially. However, LSHEP and Delaunay maintain the slowest
increase in relative error. The particularly slow increase in error
seen for Delaunay appears to suggest that it is capable of making
predictions with a range of relative errors approximately spanning
\rpm $.2*\text{\textit{dim}}$. 

\begin{figure}
  \centering
  \includegraphics[width=\textwidth,trim={0 1in 0 2in}]{Mean_Dim.pdf}
  \caption{These box plots show the prediction error of means with
    increasing dimension. }
  \label{fig:mean_dim}
\end{figure}

%     Discussion     
%====================
\section{Discussion}
\label{sec:discussion}

\subsection{Modeling the System}
\begin{enumerate}
\item Promise of the models, especially Delaunay
\item Possible reasons for the performance differences between models
\item The need for as few as 50 experiments to get less than 5\% error
\item The unfortunate inability of Delaunay to scale to higher dimension
\end{enumerate}

\subsection{Extending the Analysis}
\begin{enumerate}
\item Measuring different system characteristics other than I/O
\item Recording system performance constantly to maintain model accuracy
\end{enumerate}

\section{Conclusion}
\label{sec:conclusion}
\begin{enumerate}
\item Delaunay is the best model, capable of predicting the
  performance of 800 different system configurations accurately after
  only being trained on a well-spaced set of 50 configurations.
\item More comparisons of models similar to Delaunay should be done to
  evaluate what mathematical characteristics of multivariate models are
  most effective.
\end{enumerate}

%     Future Work     
%=====================
\subsection{Future Work}
The most severe limitation to the presented work is the present
inability to model the relationship between categorical system
parameters. This could be seen as a limitation of the models selected,
because only MARS is capable of handling non-numeric variables, or as
a limitation of the MDA framework. Future works could attempt to
identify the viability of reducing categorical variables to numerical
ones to expand set of predictable system configurations.

\begin{enumerate}
\item Using models to predict more complex responses, such as functions
\end{enumerate}

%% \newpage
%% \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\\

%% \vspace{3in}

%% \texttt{\textbf{L}oc\textbf{A}ll\textbf{Y} li\textbf{NE}ar
%%   \textbf{W}eighted \textbf{A}pproxima\textbf{T}ion\textbf{S}
%%   \textbf{O}f fu\textbf{N}tions}
%% %% L  A  Y   NE   W        A        T   S O    N
%% \newpage

\bibliographystyle{scsproc}
\bibliography{paper}

\end{document}


%%  ALGORITHM     MEAN ABSOLUTE ERROR    MEAN RELATIVE ERROR
                                                      
%% 1 Dimension                                        
%%  Delaunay         2060267             0.03 
%%  MARS             2176368             0.03 
%%  MLPRegressor     2461097             0.06 
%%  LSHEP            6353148             0.06 
%%  SVR             19306731             0.96 
                                                      
%% 2 Dimensions                                       
%%  Delaunay         2916224             0.07 
%%  MARS             6213369             0.61 
%%  MLPRegressor     8883580             0.81 
%%  LSHEP           13038882             1.52 
%%  SVR             78213963             18.49
                                                      
%% 3 Dimensions                                       
%%  Delaunay         3001794             0.09 
%%  MARS             4024964             0.65 
%%  LSHEP            5446207             1.13 
%%  MLPRegressor    11190977             2.03 
%%  SVR             97761266             31.27
                                                      
%% 4 Dimensions                                       
%%  Delaunay         3134779             0.10 
%%  MARS             4497414             1.01 
%%  LSHEP            5133648             1.07 
%%  MLPRegressor    11831721             2.20 
%%  SVR            113551300             37.26
                                                      
%% [5 - 21%] Training Data               
%%  Delaunay         5274933             0.11 
%%  MARS             6543153             0.61 
%%  LSHEP           13218469             2.53 
%%  MLPRegressor    17348982             2.58 
%%  SVR            170906944             49.45
                                                      
%% [25 - 46%] Training Data              
%%  Delaunay         2898330             0.08 
%%  MARS             4324916             0.80 
%%  LSHEP            4936381             0.85 
%%  MLPRegressor    12504957             2.29 
%%  SVR             99851846             31.28
                                                      
%% [50 - 74%] Training Data              
%%  Delaunay         1518477             0.07 
%%  LSHEP            2251409             0.26 
%%  MARS             3115943             0.95 
%%  MLPRegressor     5292177             1.17 
%%  SVR             51565405             19.63
                                                      
%% [75 - 96%] Training Data              
%%  Delaunay          886346             0.08 
%%  LSHEP            1415034             0.15 
%%  MARS             2591451             0.74 
%%  MLPRegressor     2996253             1.02 
%%  SVR             27125130             12.64
                                                      
                                                      
%% ALGORITHM   AVG ABSOLUTE MEAN ERROR   AVG RELATIVE ERROR
%% Delaunay         2711394              0.08  
%% MARS             4185947              0.72  
%% LSHEP            6474147              0.94  
%% MLPRegressor     9063718              1.48  
%% SVR             82285323              25.12 





