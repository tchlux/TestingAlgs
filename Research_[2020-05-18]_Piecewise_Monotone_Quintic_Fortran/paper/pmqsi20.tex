% Algorithm XXX: PMQSI---Piecewise monotone quintic spline interpolation,
% Thomas C.H. Lux, Layne T. Watson, Tyler H. Chang, Joshua Detwiler, 
% Kirk W. Cameron, and Yili Hong
% ACM Trans. Math. Software, ?/?/2018.
% 
%
\magnification=1000 \hsize=30pc \vsize=47pc
\parindent=10pt \parskip=0pt plus 3pt
\baselineskip=13pt plus 2pt minus 1pt \lineskiplimit=2pt
\lineskip=2pt plus 2pt                                  %\raggedright
\tolerance=800
%\abovedisplayskip=6pt plus 6pt minus 3pt
%\abovedisplayshortskip=0pt plus 3pt
%\belowdisplayskip=6pt plus 6pt minus 3pt
%\belowdisplayshortskip=6pt plus 2pt minus 3pt

% Color definitions.
\input colordvi
\def\BEGINC{\textPeach} \def\ENDC{\textBlack}
\def\red#1{\BEGINC#1\ENDC}
\let\beginred=\BEGINC
\let\endred=\ENDC

% fonts
\font\itVIII=cmti8 \font\slVIII=cmsl8 \font\bfVIII=cmbx8
\font\ttVIII=cmtt8 \font\caps=cmcsc10 \font\capsVIII=cmcsc10 at 8pt
\font\ttXVIII=cmtt12 at 18pt \font\eightmi=cmmi8 \font\eightsy=cmsy8
\font\rmeight=cmr8  \def\rmVIII{\rmeight \baselineskip=10pt plus 2pt
minus 1pt \lineskiplimit=2pt \lineskip=2pt plus 1pt
\parskip=0pt \textfont0=\rmeight \textfont1=\eightmi \textfont2=\eightsy}
\font\hlvXVIII=phvr at 18pt \font\hlvVIII=phvr at 8pt \font\hlv=phvr

\input epsf %Postscript figures.
\def\lcaption#1#2{{\noindent\rmVIII Fig.~#1. #2\par}}
\def\ltabcaption#1#2{{\noindent\rmVIII Table #1. #2\par}}


% other definitions.
\def\h{\par\hangindent=10pt\hangafter=1\noindent}
\def\heading#1{\par\bigskip\leftline{\hlv#1}\nobreak\smallskip
\noindent\ignorespaces}
\newdimen\headdigit{\setbox11=\hbox{\hlv 8. }\global\headdigit=\wd11}
\def\footnoterule{\bigskip\kern-3pt \hrule \kern 2.6pt \smallskip}
\def\bullitem{\par\vskip 2pt plus 2pt\hangindent=15pt \hangafter=1
\noindent\hbox to 15pt{---\hfil}\ignorespaces}
\def\bulllitem{\par\vskip 2pt plus 2pt\hangindent=30pt
\hangafter=1 \noindent\hskip 15pt\hbox to
15pt{---\hfil}\ignorespaces}
\def\Th#1 #2\par{\par\medskip{\it{\caps#1}. #2}\par\medskip}
\def\btable#1{\topinsert \rmVIII \let\bf=\bfVIII \textfont0=\scriptfont0
\scriptfont0=\scriptscriptfont0 \textfont1=\scriptfont1
\scriptfont1=\scriptscriptfont1 \centerline{#1}}
\let\xpar=\par
\def\ds{\displaystyle}\def\ab{\allowbreak}

% math definitions
\def\tn#1{\left\Vert #1\right\Vert_2}
\def\mn#1{\left\Vert #1\right\Vert_\infty}
\def\lee{\mathrel{\vcenter{\hbox{$\scriptstyle\mathord<$}\nointerlineskip
\vskip 1pt\hbox{$\scriptstyle\mathord=$}}}}
\def\gee{\mathrel{\vcenter{\hbox{$\scriptstyle\mathord>$}\nointerlineskip
\vskip 1pt\hbox{$\scriptstyle\mathord=$}}}}
\def\real{\mathop{\rm I\!R}\nolimits}
\def\R{{\bf R}}
\def\Rd{{\bf R}^d}
\def\Rdd{{\bf R}^{d\times d}}
%\def\bull{\hfill \vrule height6pt width6pt depth0pt}
\def\:{\mathrel{:\mathord=}}

% OUTPUT
\def\rightheadline{\hfill\hlvVIII\rightrh\hskip20pt$\scriptstyle\bullet$\hskip
20pt\folio}
\def\leftheadline{\hlvVIII\folio\hskip20pt$\scriptstyle\bullet$\hskip
20pt\leftrh\hfill}
\footline={\hfill}%no page number on first page
\output{\plainoutput}
\def\plainoutput{\shipout\vbox{\makeheadline\pagebody\makefootline}
  \advancepageno \global\footline={\hfill}
  \ifodd\pageno
        \global\headline={\rightheadline}
  \else
        \global\headline={\leftheadline}
  \fi
  \ifnum\outputpenalty>-20000 \else\dosupereject\fi}
\def\pagebody{\vbox to\vsize{\boxmaxdepth\maxdepth \pagecontents}}
\def\makeheadline{\vbox to 0pt{\vskip-22.5pt
  \line{\vbox to8.5pt{}\the\headline}\vss}\nointerlineskip}
\def\makefootline{\baselineskip24pt\vskip-6pt\line{\the\footline}}
\def\dosupereject{\ifnum\insertpenalties>0pt
  \line{}\kern-\topskip\nobreak\vfill\supereject\fi}
%\def\footnoterule{\vskip10pt\kern-3pt \hrule width 3em \kern 2.6pt}

\def\leftrh{Chang et al.}
\def\rightrh{Algorithm XXX: PMQSI}

{\baselineskip=24pt \leftline{\hlvXVIII Algorithm XXX: PMQSI: Piecewise Monotone}
\leftline{\hlvXVIII Quintic Spline Interpolation}
\bigskip\medskip
\leftline{\hlv THOMAS C.H. LUX, LAYNE T. WATSON, TYLER H. CHANG, JOSHUA DETWILER,}
\smallskip
\leftline{\hlv KIRK W. CAMERON, AND YILI HONG}
\smallskip
\leftline{\hlv Virginia Polytechnic Institute and State University}
\bigskip

\hrule\bigskip\smallskip \footnote{}{\hskip
-\parindent{\parindent=0pt\rmVIII Authors' addresses:
T. C. H. Lux, T. H. Chang, J. Ditwiler, K. W. Cameron,
Department of Computer Science, L. T. Watson,
Departments of Computer Science, Mathematics, and Aerospace and Ocean
Engineering, Y. Hong, Department of Statistics, Virginia
Polytechnic Institute \& State University, Blacksburg, VA 24061;
e-mail: {\ttVIII tchlux@vt.edu}. \xpar
% Permission to make digital/hard copy of
%part or all of this work for personal or classroom use is granted
%without fee provided that the copies are not made or distributed for
%profit or commercial advantage, the copyright notice, the title of
%the publication, and its date appear, and notice is given that
%copying is by permission of the ACM, Inc. To copy otherwise, to
%republish, to post on servers, or to redistribute to lists, requires
%specific permission and/or fee. \xpar\copyright\ 2018 by the
%Association for Computing Machinery, Inc. \xpar 
}}

% Abstract and Introduction
{\rmVIII\parindent=0pt PMQSI contains a serial code written in Fortran
  1995 for constructing piecewise monotone quintic spline interpolants
  to data. Using sharp theoretical monotonicity constraints, first and
  second derivative estimates at data provided by a quadratic facet
  model are refined to produce a $C^2$ piecewise monotone interpolant.
  This paper includes algorithm and implementation details, complexity
  and sensitivity analyses, usage information, and a brief performance
  study.

\medskip
Categories and Subject Descriptors: G.1.1 [{\bfVIII Numerical Analysis}]:
Interpolation --- Spline and piecewise polynomial interpolation;
J.2 [{\bfVIII Computer Applications}]: Physical Science and Engineering
--- {\itVIII Mathematics};
G.4 [{\bfVIII Mathematics of Computing}]: Mathematical Software

\medskip
General Terms: Algorithms, Monotonicity, Documentation

\medskip
Additional Key Words and Phrases: Quintic Spline, interpolation

}
\bigskip\hrule\bigskip\medskip
\heading{1. INTRODUCTION}

The Delaunay triangulation is an unstructured simplicial mesh that
is widely studied in the field of computational geometry. Due to its
many favourable properties, the Delaunay triangulation finds wide use
as a mesh for multivariate interpolation in the fields of geographic
information systems (GIS), civil engineering, physics, and computer
graphics. See Section 9.6 of [de Berg et al.\ 2008] for a brief
discussion of the usefulness of Delaunay triangulations, and see
[Schaap et al.\ 2000] for a specific usage example from computational
physics. The viability of Delaunay triangulations as a means for
interpolating arbitrary nonlinear functions in the context of data science
and machine learning has been explored by Omohundro [1990]. However,
this particular usage of the Delaunay triangulation never gained
widespread popularity, most likely due to the exponential computational
complexity of high-dimensional Delaunay triangulations. More recent
works have shown Delaunay triangulations to be effective for
interpolating real-world computer system data [Chang et al.\ 2018a 
and Lux et al.\ 2018], outperforming several common multivariate
interpolation and approximation techniques.

In this work, the main problem of interest is the multivariate
interpolation problem. Interpolation via meshes such as triangulations
and other tessellations is a classic practice. Delaunay triangulations
are widely considered optimal simplicial meshes for many meshing
applications, including interpolation. See the first chapter of Cheng
et al.\ [2012] for an overview of Delaunay meshing applications and
theory, and see Rajan [1994] for specific theorems on the optimality of
Delaunay triangulations in arbitrary dimension. 

In two dimensions, the Delaunay triangulation of $n$ points can be
efficiently computed in ${\cal O}\big( n \log n \big)$ time
[Su et al.\ 1995]. After the Delaunay triangulation has been computed,
the cost of evaluating each interpolation point is reduced to the cost
of point location. In two dimensions, point location can be performed
in ${\cal O}\big( n^{1\over 3} \big)$ time [M{\"u}cke et al.\ 1999],
so the total cost of interpolating at $m$ points in two dimensions is
${\cal O}\big( n\log n + n^{1\over 3}m \big)$. However, Klee [1980]
showed that in $\Rd$, the worst case size of the Delaunay triangulation
is ${\cal O}\big(n^{\lceil d/2 \rceil}\big)$. Even in the generic case,
the Delaunay triangulation still tends to grow exponentially with the
dimension, a phenomenon often associated with 
{\it the curse of dimensionality}.

Despite the complexity of high-dimensional Delaunay triangulations,
there are currently a wide variety of algorithms for computing them.
The first algorithm capable of computing Delaunay triangulations in
arbitrary dimension was proposed independently by both Bowyer [1981]
and Watson [1981]. Perhaps the most widely used algorithm for computing
Delaunay triangulations in arbitrary dimension is the Quickhull algorithm
proposed by Barber et al.\ [1996]. Quickhull is a time-efficient
algorithm running in $\Theta \big(n\log n + k\big)$ time, where $k$
denotes the size of the Delaunay triangulation. Quickhull also boasts
a highly optimized numerically stable implementation. An alternative
to Quickhull is the graph based algorithm proposed by Boissonnat et
al.\ [2009]. An implementation of this algorithm, contained in the
Computational Geometry Algorithms Library (CGAL), stores the
Delaunay triangulation in a memory efficient graph structure, at the
cost of a slightly greater compute time. One final algorithm of interest
is the DeWall algorithm, proposed by Cignoni et al.\ [1998]. The DeWall 
algorithm, though not in widespread use, features a unique
divide-and-conquer paradigm, and was a major inspiration behind this work.

Due to the exponential growth of Delaunay triangulations in high
dimensions, none of the above mentioned algorithms are intended to
scale past six or seven dimensions. In fact, this failure to scale
to high dimensions is suffered by nearly every mesh based approximation.
Consequently, high-dimensional approximation is generally dominated by
mesh free methods such as multivariate polynomials, radial basis
functions, low order splines, inverse distance weightings, kernel
methods, and machine learning techniques such as support vector
regressors and artificial neural networks (see [Cheney et al.\ 2009]).
By leveraging the sparse nature of the interpolation problem, the
DELAUNAYSPARSE package aims to add the high-dimensional Delaunay mesh
based interpolant to the numerical analyst's toolbox.

The rest of this paper is organized as follows. Section 2 describes
the Delaunay interpolant in greater detail and outlines a novel algorithm
for computing it. Section 3 details the computational aspects of the
algorithm, with an emphasis on numerical stability and efficiency.
Section 4 describes the serial implementation of the algorithm and its
additional features. Section 5 describes the parallel implementation,
which uses OpenMP in a shared memory paradigm. Section 6 contains usage
information and package organization details. Section 7 shows performance
statistics, demonstrating the scalability of the algorithm. For additional
information on the properties of Delaunay triangulations and algorithms for
computing them, two excellent references are [Aurenhammer et al.\ 2013]
and [Cheng et al.\ 2012].

\heading{2. INTERPOLATION VIA THE DELAUNAY TRIANGULATION}

Let $P$ be a set of $n>d$ data points in $\Rd$. A $d$-dimensional
triangulation is defined as a mesh of $d$-simplices that
(1) are disjoint except on their shared boundaries,
(2) whose set of vertices is $P$, and 
(3) whose union is the convex hull of $P$, denoted $CH(P)$.
The interpolation problem is: given values $f(p)$ for all points
$p \in P$ where $f : \Rd \rightarrow \R^m$, find an approximation
${\hat f} \approx f$ such that ${\hat f}(p) = f(p)$ for all $p\in P$,
where ${\hat f}$ has support in $CH(P)$.

Let $T(P)$ be a $d$-dimensional triangulation of $P$. To define an
interpolant in terms of $T(P)$, let $q \in CH(P)$ be an interpolation
point, and let $S$ be a simplex in $T(P)$ with vertices 
$s_1$, $\ldots$, $s_{d+1}$ such that $q \in S$. Then there exist weights
$w_1$, $\ldots$, $w_{d+1}$ such that $q = \sum_{i=1}^{d+1} w_i s_i$,
$\sum_{i=1}^{d+1} w_i = 1$, and $w_i \geq 0$ for $i=1$, $\ldots$, $d+1$,
and the interpolant ${\hat f}_T$ is given by
$$
{\hat f}_T(q) = w_1f(s_1) + w_2f(s_2) + \ldots + w_{d+1}f(s_{d+1}).
\eqno(1)
$$

In DELAUNAYSPARSE, the interpolant ${\hat f}_{DT}$ is computed, where
$DT(P)$ denotes a Delaunay triangulation of $P$. The Delaunay
triangulation is often defined as the geometric dual of the Voronoi
diagram, also called the Dirichlet tessellation. Here the following
equivalent definition is preferred. For a $d$-simplex $S$,
let $B_S$ denote the open ball whose center and radius are given
by the $(d-1)$-sphere circumscribing $S$. Then a Delaunay triangulation
$DT(P)$ of a finite set of points $P \subset \Rd$ is any triangulation
of $P$ such that for each $S \in DT(P)$, $B_S$ satisfies
$B_S \cap P = \emptyset$. Remarks 2.1--3 below describe several key
properties of a Delaunay triangulation.

\medskip{\sl Remark 2.1.}\enspace
Given a set of $d+1$ vertices in $P$ that define a
$d$-simplex $S$, the condition that $B_S \cap P = \emptyset$ is not
only necessary, but also sufficient to conclude that $S \in DT(P)$
for {\it some} Delaunay triangulation $DT(P)$.

\medskip{\sl Remark 2.2.}\enspace
Let $F$ be a facet of a simplex $S \in DT(P)$. Let 
$p_1$, $\ldots$, $p_\ell$ be a sequence of points that are in $P$
and are in the same halfspace $H$ with hyperplane boundary
containing $F$. Define the open circumballs $B_1$, $\ldots$, $B_\ell$
such that each $B_i$ circumscribes $F$ and $p_i$. Assume 
$p_1$, $\ldots$, $p_\ell$ satisfies $p_i \in B_{i+1}$ for all 
$1 \leq i < \ell$.
Then $B_1 \cap H \subset B_2 \cap H \subset \cdots \subset B_\ell \cap H$.

\medskip{\sl Remark 2.3.}\enspace
In randomly generated data, the cases where $DT(P)$ does not exist
or is not unique occur with probability zero. Therefore, for
algorithmic analysis, it is common to make the simplifying assumption
that $P$ is in {\it general position}, meaning $DT(P)$ exists and is
unique. Furthermore, note that the case where $DT(P)$ does not exist
occurs only if all the points in $P$ are contained in some 
lower-dimensional linear manifold. In the context of interpolation,
this corresponds to an overparameterization of the underlying function
and can be resolved with dimension reduction techniques.
The case where $DT(P)$ is not unique can occur in real-world problems
and will be addressed in the implementation, discussed in Section 3.
\medskip

Note that given a set of $m$ interpolation points $Q$, at most $m$
simplices in $DT(P)$ are needed to compute ${\hat f}_{DT}(q)$ for
all $q\in Q$. Therefore, for this particular problem, it is possible
to ``cheat'' the curse of dimensionality when $m$ is significantly
less than the size of $DT(P)$. To do so, it suffices to compute
a sparse subset of $DT(P)$ such that $Q$ is contained in the subset.

As previously mentioned, one of the major inspirations behind this
work was the DeWall algorithm proposed by Cignoni et al.\ [1998].
The DeWall algorithm features a divide-and-conquer paradigm where
construction of each Delaunay simplex is carefully guided so as to
construct a ``wall'' of simplices, bisecting the data set. Each
successive simplex is completed from a facet of a previously constructed
simplex, using the same methodology as the classic gift-wrapping
approach (described in Section 5.6 of [Cheng et al.\ 2012]).

The two key components of the DeWall and gift-wrapping algorithm
that are used in DELAUNAYSPARSE are the growth of the seed simplex
and the completion of an open Delaunay facet. After iteratively
constructing a seed simplex, the idea is to perform a 
{\it visibility walk} to the simplex containing each interpolation
point, as described by Devillers et al.\ [2002]. Since the complete
triangulation is never computed, each step of the walk is performed by
completing the Delaunay facet designated by the visibility walk protocol.
Once each interpolation point $q$ has been located, each response value
${\hat f}_{DT}(q)$ can be computed using (1). A detailed description
and analysis of this algorithm is in Section 3 of [Chang et al.\ 2018b].
Pseudocode for interpolating at a single point follows.

{\parindent =0pt \parskip= 0pt
\smallskip
\leftskip 20pt
{\sl Algorithm 1}
\smallskip
$P$ contains $n$ $d$-dimensional data points;\par
$q \in CH(P)$ is the interpolation point;\par
$S$ denotes the current $d$-simplex; \par
$F$ denotes the facet of $S$ from which $q$ is visible. \par
\smallskip
{\bf begin}
Grow an initial seed $d$-simplex $S$, as described in Section 3.1.\par
{\bf while} $q \not\in S$ {\bf do}\par
\leftskip 40pt
Select the facet $F$ of $S$ from which $q$ is visible as
described in Section 3.2;\par
complete a new $d$-simplex $S^*$ from the facet $F$ as described 
in Section 3.3;\par
update $S \leftarrow S^*$.\par
\leftskip 20pt
{\bf enddo}\par
Since the loop has terminated, $q\in S$.
Compute ${\hat f}_{DT}(q)$ using (1).\par
\smallskip}

The advantages of this technique are maximized when the interpolation
points are sparse with respect to the size of the triangulation. In
both expectation and practice, the number of simplices constructed
during the walk to each interpolation point is a polynomial function
of $d$ and often seemingly independent of $n$. Given the exponential
nature of the problem, this makes for an effective sparse solution,
particularly in high dimensions.

\heading{3. COMPUTATIONAL ASPECTS}

In this section, the computational operations referenced in Algorithm 1
will be fully detailed.
These operations are the growth of the seed simplex, the visibility walk,
and flipping across a Delaunay facet.

\heading{3.1 \enspace Growing the Seed Simplex}

The seed simplex is constructed through a greedy algorithm, as detailed
in Section 3.1 of Chang et al.\ [2018b]. The initial vertex $s_1$ is
chosen to be the closest point in the data set $P$ to the interpolation
point $q$, and ties are resolved by choosing the point in $P$ with the
lowest index.
The second vertex $s_2 \in P \setminus \{s_1\}$ is chosen such that
$$
\|s_2 - s_1\| = \min_{p \in P, \atop p\ne s_1} \|p - s_1\|.
$$
Each subsequent vertex is chosen to minimize the radius of the minimum
radius circumsphere for the resulting vertex list.

For $2 \leq j \leq d$ and $p \in P\setminus \{s_1$, $\ldots$, $s_j\}$,
define the $j \times d$ matrix 
$$
A^{(j,p)} = \left[ \matrix{
  \left(s_2 - s_1\right)^T \cr
  \vdots                   \cr
  \left(s_j - s_1\right)^T \cr
  \left( p - s_1 \right)^T } \right]
$$
and the $j$-vector
$$
b^{(j,p)} = \left[ \matrix{
  \displaystyle{\|s_2 - s_1\|^2 \over 2} \cr
  \vdots                    \cr
  \displaystyle{\|s_j - s_1\|^2 \over 2} \cr
  \displaystyle{\|p - s_1\|^2 \over 2}  } \right].
$$

If rank~$A^{(j,p)} = j$, then the minimum norm solution to the
underdetermined system
$$
A^{(j,p)} x = b^{(j,p)} \eqno{(2)}
$$
is $x^* = c - s_1$, where $c$ denotes the center of the
minimum radius circumsphere about $s_1$, $\ldots$, $s_j$, $p$.
So, each subsequent vertex $s_{j+1}$ is given by the 
$p^* \in P \setminus \{s_1$, $\ldots$, $s_j\}$ such
that solving (2) with $A^{(j,p^*)}$ and $b^{(j,p^*)}$ produces the
minimum 2-norm solution $x^*$.

If rank~$A^{(j,p)} < j$, then $s_1$, $\ldots$, $s_j$, $p$ are not
the vertices of a $j$-simplex, and $p$ cannot be a vertex of any
$d$-simplex with vertices $s_1$, $\ldots$, $s_j$.
Hence, $p$ can be skipped and need not be considered again when
constructing the seed simplex. Due to memory constraints, there is no
mechanism for marking a $p$ that can be skipped, and hence any such $p$
could be revisited in the future, though it will always be skipped.

If $P$ is in general position, then there will always exist a unique
point $p^*$ that minimizes $\|x^*\|$.
However, if $P$ lies in a $(j-1)$-dimensional linear manifold where
$j \leq d$, then any set of $j+1$ or more points in $P$ will be affinely
dependent.
Therefore rank~$A^{(j,p)} < j$ for {\it all}
$p \in P \setminus \{s_1$, $\ldots$, $s_j\}$, and no solution $p^*$
can exist. Indeed, $DT(P)$ does not exist as discussed in Remark 2.3.
Furthermore, $DT(P)$ is not unique if there exist $d+2$ or more
points in $P$ that lie on the same circumball, since there may be more
than one $p^* \in P$ that minimize $\|x^*\|$. For the purpose of
interpolation, all of these solutions are equally suitable, and the
decision between any number of such candidate solutions can be made
arbitrarily. In particular, these cases are resolved by choosing
the candidate $p^*$ with the lowest index in $P$. Pseudocode for this
process follows.

{\parindent =0pt \parskip= 0pt
\smallskip
\leftskip 20pt
{\sl Algorithm 2}
\smallskip
$P$ contains $n$ $d$-dimensional data points;\par
$q \in CH(P)$ is the interpolation point;\par
$\{s_1$, $\ldots$, $s_{d+1}\}$ denote the vertices of the seed simplex. \par
\smallskip
{\bf begin}\par
$s_1 \leftarrow {\arg\min \atop {p\in P}} \|q - p\|$;\par
$s_2 \leftarrow {\arg\min \atop {p\in P, \atop p\neq s_1}} \|s_1 - p\|$;\par
{\bf for} $j = 2$, $\ldots$, $d$ {\bf do}\par
\leftskip 40pt
{\bf initialize} $r_{min} \leftarrow \infty$; $p^* \leftarrow null$;\par
{\bf for all} $p \in P \setminus \{s_1$, $\ldots$, $s_j\}$ {\bf do}\par
\leftskip 60pt
Compute $A^{(j,p)}$ and $b^{(j,p)}$;\par
{\bf if} rank~$A^{(j,p)} < j$ {\bf then} \par
\leftskip 80pt
Skip this point; \par
\leftskip 60pt
{\bf else if} rank~$A^{(j,p)} = j$ {\bf then} \par
\leftskip 80pt
Compute the minimum norm solution $x^*$ to (2);\par
{\bf if} $\|x^*\| < r_{min}$ {\bf then}\par
\leftskip 100pt
$r_{min} \leftarrow \|x^*\|$;\par
$p^* \leftarrow p$;\par
\leftskip 80pt
{\bf endif}\par
\leftskip 60pt
{\bf endif}\par
\leftskip 40pt
{\bf enddo}\par
$s_{j+1} \leftarrow p^*$;\par
\leftskip 20pt
{\bf enddo}\par
{\bf if} $p^* \neq null$ {\bf then}\par
\leftskip 40pt
{\bf return} $\{s_1$, $\ldots$, $s_{d+1}\}$;\par
\leftskip 20pt
{\bf else}\par
\leftskip 40pt
{\bf return} error (points in a lower-dimensional linear manifold);\par
\leftskip 20pt
{\bf endif}
\smallskip}

The dominant costs of the above algorithm are determining the rank of
$A^{(j,p)}$ and finding the minimum norm solution $x^*$ to (2). Both of
these computations can be done using a $LQ$ factorization of $A^{(j,p)}$
with row pivoting. Such a factorization has a computational
complexity of at most ${\cal O}\big(d^3\big)$ (in the case where $j=d$),
so the total complexity of Algorithm 2 (for growing a seed $d$-simplex)
is ${\cal O}\big(nd^4\big)$.

\medskip{\sl Remark 3.1.1.}\enspace
To determine the rank of $A^{(j,p)}$ using a $LQ$ factorization with row
pivoting, consider the final term on the diagonal of $L$, $L_{jj}$. 
By construction, rank~$A^{(j,p)}$ is at least $j-1$, and therefore rank~$L$
must also be at least $j-1$. Therefore, $L_{jj}$ is the exact magnitude of
the smallest perturbation that could cause rank-deficiency in $L$, and
equivalently, in $A^{(j,p)}$. To allow for floating point error,
$A^{(j,p)}$ is considered singular if $|L_{jj}| < \varepsilon$, where
$\varepsilon$ is a scale/machine dependent constant.
\medskip

\heading{3.2 \enspace The Visibility Walk}

After constructing the seed simplex, DELAUNAYSPARSE advances on the
simplex containing an interpolation point $q$ by following a visibility
walk. A facet $F$ of a simplex $S$ is said to be {\it visible} to $q$ if
there exists a point $\rho \in$~int~$S$ such that the line segment drawn
from $\rho$ to $q$ intersects $F$. A visibility walk is a sequence
of ``flips'' that always occur across a facet from which $q$ is visible.
Note that each flip in a visibility walk is generally not unique, as it
is possible for $q$ to be visible from multiple facets, and a flip across
{\it any} visible facet constitutes a valid step in a visibility walk.
Edelsbrunner [1989] showed that in a Delaunay triangulation, every
visibility walk is acyclic and therefore must converge for any
$q \in CH(P)$. The mechanics of performing each flip in the visibility
walk will be detailed in Section 3.3. In this section, the processes
of identifying each visible facet and terminating the visibility walk
will be explored.

For a simplex $S$ with vertices $s_1$, $\ldots$, $s_{d+1}$, define
the $d \times d$ matrix
$$
A^{(S)} = \left[ \matrix{
  \left(s_2 - s_1\right) \enspace
  \cdots 
  \enspace \left(s_{d+1} - s_1\right) } \right].
$$
Let $x_i$ denote the $i$th entry in the $d$-vector $x$, given by
the solution to the linear system
$$
A^{(S)}x = q - s_1. \eqno{(3)}.
$$
Then the vector of affine weights $w = [w_1$, $\ldots$, $w_{d+1}]^T$
for generating $q$ as a combination of $s_1$, $\ldots$, $s_{d+1}$ is given
by
$$
w = \left[ \matrix{
  \left(1 - \sum_{i=1}^d x_i\right) \cr
  x_1 \cr
  \vdots \cr
  x_d } \right].
$$

If $w_i \geq 0$ for $i=1$, $\ldots$, $d+1$, then $q\in S$ and
$w$ contains the interpolation weights in (1). If any $w_i < 0$, then
dropping the corresponding vertex $s_i$ leaves the vertices of a
facet of $S$ from which $q$ is visible. If $S$ is a valid Delaunay
simplex, $A^{(S)}$ is nonsingular and (3) can be solved via $LU$
factorization.

\medskip{\sl Remark 3.2.1.}\enspace
To account for floating point errors, the condition that $w_i \geq 0$
for $i=1$, $\ldots$, $d+1$ should be replaced with $w_i \geq -\varepsilon$,
where $\varepsilon$ is a scale/machine dependent constant, similarly as in
Remark 3.1.1.
\medskip

The cost of a single $LU$ factorization for solving (3) is insignificant
compared to the cost of performing a flip, as described in the next
section, which requires up to $n$ $LU$ factorizations. However, the total
length of the visibility walk (in number of flips) will be important in
determining the computational complexity of the DELAUNAYSPARSE algorithm.
This length $k$ can only be analytically bounded by the total size of
$DT(P)$. However, Bowyer [1981] claimed without proof that when
starting a visibility walk from the center of a Delaunay triangulation,
$k$ is ${\cal O}\big(n^{1/d}\big)$.
M{\"u}cke et al.\ [1999] proved that in up to three-dimensions, a stronger
claim can be made for some variations of the standard visibility walk.
Chang et al.\ [2018b] showed empirically that for pseudo-randomly
generated data points, when starting from a simplex grown off the nearest
data point to $q$, $k$ tends to grow polynomially with dimension and has
no dependence on $n$ for large values of $n$ and $d$.

\heading{3.3 \enspace Flipping Across a Facet}

Let $F = CH(\{s_1$, $\ldots$, $s_d\})$ be a facet of a previously
constructed Delaunay simplex from which the interpolation point $q$
is visible. Let $H(F)$ denote the hyperplane containing $F$, and let
$H_q(F)$ denote the open halfspace (with respect to $H(F)$) that
contains $q$. The goal of this section is to ``flip toward'' $q$, by
constructing a new Delaunay $d$-simplex with vertices
$s_1$, $\ldots$, $s_{d+1}$, where $s_{d+1} \in P \cap H_q(F)$.

Recall from Remark 2.1 that a $d$-simplex $S$ is Delaunay if and only
if $B_S \cap P = \emptyset$. Since at least one Delaunay simplex
(of which $F$ is a facet) has already been constructed, $DT(P)$ exists.
Therefore, if $F$ is {\bf not} a facet of $CH(P)$, there must
be at least one point $p^*\in P \cap H_q(F)$ such that the simplex
$S^*$ with vertices $\{s_1$, $\ldots$, $s_d$, $p^*\}$ is Delaunay,
satisfying $B_{S^*} \cap P = \emptyset$. If no such $p^*$ exists,
then it can be inferred that $q \not\in CH(P)$.
So, to perform a ``flip'' to a new Delaunay simplex
closer to $q$, it suffices to check inside the circumball of the
simplex with vertices $s_1$, $\ldots$, $s_d$, $p$, for each
$p \in P \cap H_q(F)$. By exploiting the property described in
Remark 2.2, this can be done in a single pass over $P$.

For a facet $F$ with vertices $s_1$, $\ldots$, $s_d$, let the
$(d-1) \times d$ matrix
$$
A^{(F)} = \left[ \matrix{
  \left(s_2 - s_1\right)^T \cr
  \vdots \cr
  \left(s_d - s_1\right)^T } \right].
$$
To obtain a normal to $H(F)$, it suffices to take any nontrivial
vector in the nullspace of $A^{(F)}$.
Since $F$ is a Delaunay facet, rank~$A^{(F)} = d-1$. Therefore, using a
$LQ$ factorization with row pivoting, ${\cal P}A^{(F)} = LQ$, yields
the final row $v^T = Q_{d\cdot}$ of $Q$ as a unit normal to $H(F)$.

Given the normal vector $v$ for $H(F)$, consider the function
$$
\sigma_F(p) = \hbox{sgn}
\bigl((p - s_1)^T v\bigr). \eqno{(4)}
$$
For each $p\in P$, $p\in H_q(F)$ if and only if $\sigma_F(p) = \sigma_F(q)$.

\medskip{\sl Remark 3.3.1.}\enspace
To account for floating point errors, the additional condition that 
$\bigl|(p - s_1)^T v\bigr| > \varepsilon$ should be imposed, where
$\varepsilon$ is a scale/machine dependent constant, similarly as in
Remarks 3.1.1 and 3.2.1.
\medskip

Consider now those $p \in P \setminus \{s_1$, $\ldots$, $s_d\}$ such that
$\sigma_F(p) = \sigma_F(q)$, i.e., $p \in H_q(F)$.
Similarly as in Section 3.1, the center of the circumball about $F$ and
$p$ is given by $c = x^* + s_1$, and the radius of the circumball is given
by $r = \|x^*\|$, where $x^*$ is a solution to the system
$$
A^{(d,p)}x = b^{(d,p)}. \eqno{(5)}
$$
Since $F$ is a valid Delaunay facet, the first $d-1$ columns of $A^{(d,p)}$
must be linearly independent. Furthermore, if the condition described in
Remark 3.1.1 has been satisfied, then it is safe to assume that $A^{(d,p)}$
is full-rank, and (5) has a unique solution. Then $p^*$ is any point in
$P \cap H_q(F)$ that satisfies $B_{\|x^*\|}(c) \cap P = \emptyset$, where
$B_{\|x^*\|}(c)$ denotes the open ball centered at $c$ with radius
$\|x^*\|$. Pseudocode for this entire process follows.

\medskip{\sl Remark 3.3.2.}\enspace
If $P$ is in general position, then $p^*$ is unique. If there exist
$d+2$ or more cospherical points in $P$, then $p^*$ may not be unique.
However, any $p^*\in P \cap H_q(F)$ that satisfies
$B_{\|x^*\|}(c) \cap P = \emptyset$ can be chosen in union with
$\{s_1$, $\ldots$, $s_d\}$ to form the vertex set for a valid simplex
in {\it some} Delaunay triangulation. Such situations can be resolved
by choosing the $p^*$ with the greatest index in $P$.
\medskip

{\parindent =0pt \parskip= 0pt
\smallskip
\leftskip 20pt
{\sl Algorithm 3}
\smallskip
$P$ contains $n$ $d$-dimensional data points;\par
$q \in CH(P)$ is the interpolation point;\par
$\{s_1$, $\ldots$, $s_{d+1}\}$ denote the vertices of the new simplex;\par
$F$ is the current Delaunay facet;\par
\smallskip
{\bf begin}
$\{s_1$, $\ldots$, $s_d\}$ are given by the vertices of $F$;\par
Compute the $LQ$ factorization ${\cal P}A^{(F)}=LQ$
and set $v^T \leftarrow Q_{d\cdot}$; \par
Compute $\sigma_F(q)$ using (4); \par
{\bf initialize} $r_{min} \leftarrow \infty$;
$c_{min} \leftarrow {\vec 0}$;
$p^* \leftarrow null$;\par
{\bf for all} $p \in P \setminus \{s_1$, $\ldots$, $s_d\}$ {\bf do}\par
\leftskip 40pt
{\bf if} $\sigma_F(p) \neq \sigma_F(q)$ {\bf or} 
$\bigl|(p - s_1)^T v\bigr| < \varepsilon$ {\bf then}\par
\leftskip 60pt
Skip this point;\par
\leftskip 40pt
{\bf else if} $\|p - c_{min}\| < r_{min}$ {\bf then}\par
\leftskip 60pt
Update $A^{(d,p)}$ and $b^{(d,p)}$ and compute $x^*$, the solution to (5);\par
$r_{min} \leftarrow \|x^*\|$;\par
$c_{min} \leftarrow x^* + s_1$;\par
$p^* \leftarrow p$;\par
\leftskip 40pt
{\bf endif}\par
\leftskip 20pt
{\bf enddo}\par
{\bf if} $p^* = null$ {\bf then} \par
\leftskip 40pt
{\bf return} error, $q \not\in CH(P)$;\par
\leftskip 20pt
{\bf else}\par
\leftskip 40pt
{\bf return} $s_{d+1} \leftarrow p^*$;\par
\leftskip 20pt
{\bf endif}
\smallskip}

The dominant cost for Algorithm 3 is repeatedly solving (5). Since
$A^{(d,p)}$ is always full-rank, each instance of (5) can be solved
using an $LU$ factorization, with computational complexity
${\cal O}\big(d^3\big)$. So the worst-case complexity of Algorithm 3
is ${\cal O}\big(nd^3\big)$.
Note that Algorithm 3 is called once in each iteration of the simplex
walk. Therefore, from Section 3.1 and 3.2, the overall computational
complexity for locating a single interpolation point is
${\cal O}\big(nd^4 + knd^3\big)$, where $k$ is the length of the
visibility walk.
Since $k$ is typically much greater than $d$, this can be simplified
to ${\cal O}\big(knd^3\big)$.

\heading{4. SERIAL IMPLEMENTATION}

The serial subroutine {\tt DELAUNAYSPARSES} is implemented in ISO Fortran
2003. For efficient numerically stable linear algebra, {\tt DELAUNAYSPARSES}
uses LAPACK [Anderson et al.\ 1999]. 

\heading{4.1 \enspace Handling Multiple Interpolation Points}

The serial subroutine {\tt DELAUNAYSPARSES} performs interpolation at $m$
points $Q = \{q_1$, $\ldots$, $q_m\}$ using Algorithms 1--3, as described
in Sections 2 and 3. By default, {\tt DELAUNAYSPARSES} will perform these
interpolations sequentially with no modification to Algorithms 1--3.
However, an optional argument can be set to ``daisy chain'' the visibility
walks, i.e., for the $i$th interpolation point $q_i$ where $i > 1$, the
last constructed Delaunay simplex (typically the simplex containing 
$q_{i-1}$) is used as the first simplex for walking to $q_i$, replacing
Algorithm 2. In general, this behavior can greatly increase the length of
each visibility walk and is not recommended. However, if the interpolation
points $Q$ are tightly clustered in a relatively small region of
$DT(P)$ or if the size of $DT(P)$ is relatively small, this behavior can
slightly improve performance by avoiding the expense of Algorithm 2.

Additionally, note that after computing the $LU$ factorization of $A^{(S)}$
for solving (3), it requires relatively little additional computation to
check whether $S$ contains any future interpolation points. Thus if $S$
is a simplex with vertices $s_1$, $\ldots$, $s_{d+1}$ that has been
constructed during the visibility walk to an interpolation point $q_i$,
{\tt DELAUNAYSPARSES} will check whether $q_j \in S$, where
$i < j \leq n$ and $q_j$ has not already been found in some simplex
of $DT(P)$, by solving
$$
A^{(S)}x = q_j - s_1, \eqno{(6)}
$$
and using the same criterion described in Section 3.2.
If any $q_j \in S$, then $s_1$, $\ldots$, $s_{d+1}$ and the corresponding
interpolation weights for (1) are saved, and $q_j$ is marked as found and
will not be considered again. Because of its cost effectiveness, this
behavior is always active during an execution of {\tt DELAUNAYSPARSES}.
However, it is most effective for tightly clustered interpolation points.

\heading{4.2 \enspace Extrapolation}

Often, it is reasonable to make predictions for extrapolation points
that are {\it slightly} outside $CH(P)$. In these cases, the most
reasonable solution is to project each extrapolation point onto
$CH(P)$ and interpolate the projection, provided the residual is small.
Let $z$ be an extrapolation point, and let $W$ be a $d\times n$ matrix
whose columns are points in $P$. Then the projection ${\hat z}$ of $z$
onto $CH(P)$ is given by ${\hat z} = Wx^*$, where $x^*$ is the solution
to the linearly constrained least squares problem
$$
\min_{x\in\R^n} \|Wx - z\| \quad\hbox{subject to}\quad
x \ge 0 \quad\hbox{and}\quad \sum_{i=1}^n x_i = 1. \eqno{(7)}
$$
Hanson et al.\ [1982] provide an efficient solution to (7)
based on a slack variable formulation. The most recent version
of their subroutine {\tt DWNNLS} is available in the SLATEC
software package, and included with the DELAUNAYSPARSE files.

\medskip{\sl Remark 4.2.1.}\enspace
Note that the visibility walk described in Section 3.2 is only guaranteed
to converge for $q\in CH(P)$. In particular, if a projection $\hat z$
is left within floating point error of $CH(P)$, and the matrix $A^{(S)}$
for a nearby Delaunay simplex has smallest singular value
${\cal O}(\varepsilon)$, then it is
possible for a visibility walk to fail by repeatedly calling for a flip
that would lead outside of the convex hull. This is an extremely rare
situation. However, in these situations, {\tt DELAUNAYSPARSE} will first
try to flip in other potential directions (i.e., by dropping different
negatively weighted vertices). Then, if no ``good'' direction can be
found, the correct interpolation weights must ultimately be computed by
a second {\tt DWWNLS} projection onto the current simplex.

\medskip

Given the above solution, the residual is given by $r = \|z - {\hat z}\|$.
When $r$ is small with respect to the scale of the data, it is reasonable
to perform extrapolation at $z$ by interpolating at $\hat z$.  However,
when $r$ is large it is impossible to make any reasonable prediction for
$f(z)$. By default, when {\tt DELAUNAYSPARSES} encounters an extrapolation
point $z$, it computes the projection ${\hat z}$ and residual $r$ using
{\tt DWNNLS}. If $r$ is smaller than some percentage of the diameter of
$P$, {\tt DELAUNAYSPARSES} resumes interpolation using $q={\hat z}$. If
$r$ is greater than that percentage of the diameter, the extrapolation
point $z$ is skipped and an appropriate error is returned.

By default, the threshold for extrapolation is $10\%$ of the diameter of
$P$, but this percentage can be adjusted using an optional input argument.
Furthermore, setting this optional value to $0\%$ of the diameter of $P$
will short-circuit the extrapolation process, preventing ${\hat z}$
and $r$ from ever being computed and preventing any {\tt DWNNLS}
work arrays from being allocated. Note that the time and space demands
of {\tt DWNNLS} can be significantly greater than those of
{\tt DELAUNAYSPARSES}. So, for large problems or in cases where
computational resources are limited, it is often appropriate to turn off
extrapolation by setting the extrapolation threshold to $0\%$ of the
diameter of $P$.

\medskip{\sl Remark 4.2.2.}\enspace
For similar reasons as in Remark 4.2.1, it is possible that for poorly
spaced data points $P$, {\tt DELAUNAYSPARSES} may incorrectly call for
a projection of an interpolation point that is within floating point
error of the boundary of $CH(P)$. After unnecessarily performing the
projection and finding that $r=0$, such situations are easily
detected retrospectively. However, if the extrapolation threshold is
set to 0\% of the data diameter, the projection will short circuit
and an interpolation point that is within $\varepsilon$ of the convex
hull $CH(P)$ could be incorrectly skipped. The conditions that could
lead to such an error are pathological, but might still occur.
\medskip

\heading{4.3 \enspace Data Scaling and Sensitivity Analysis}

Recall from Remarks 3.1.1, 3.2.1, 3.3.1, and 4.2.1 that a small
scale/machine dependent constant $\varepsilon > 0$ is used to account
for floating point error. Affine operations do not affect the Delaunay
triangulation or interpolation results, so to account for scaling, 
{\tt DELAUNAYSPARSES} rescales and shifts the data points $P$ and
the interpolation points $Q$ on input. First, the points in $P$ are
shifted so that their barycenter is at the origin, then they are rescaled
so that they are contained in the unit hypersphere. This ensures
that $\varepsilon$ can be chosen without accounting for data scale.
The interpolation points $Q$ must then be shifted and scaled by the same
amounts to maintain relative positions.

After scaling, the default value $\varepsilon$ can be chosen based
only on machine precision. By default, $\varepsilon = \sqrt{\mu}$
where $\mu$ denotes the unit round-off. This is the minimum appropriate
value of $\varepsilon$ for most applications, and an optional argument
can be used to increase $\varepsilon$ where appropriate. For the rescaled
data, {\tt DELAUNAYSPARSES} is backward stable for perturbations less
than $\varepsilon$.

\heading{4.4 \enspace Memory Usage}

{\tt DELAUNAYSPARSES} uses assumed-shape arrays where appropriate.
To ensure expected behavior, the dimensions of each dummy array are
checked against user-specified values of $d$, $n$, and $m$ on input.
Due to the size of $DT(P)$ in high dimensions, the space complexity of
any Delaunay triangulation algorithm is equally as important as its
time complexity. The computational operations described in Section 3
do not require any work arrays larger than ${\cal O}\big(d^2\big)$,
making DELAUNAYSPARSE a space efficient algorithm.

However, to take full advantage of LAPACK code optimizations, one
larger work array is required. Therefore, {\tt DELAUNAYSPARSES}
uses one allocatable work array, whose size is determined at runtime
based on LAPACK querries. Other allocatable work arrays of size
${\cal O}\big(nd\big)$ are required by {\tt DWNNLS} for extrapolation,
but are only allocated if an extrapolation is performed.

\heading{4.5 \enspace The Cost of Robustness and Correctness}

{\tt DELAUNAYSPARSES} is designed to be robust for a wide variety of
use cases and usage errors. In particular, {\tt DELAUNAYSPARSES} uses
the diameter of $P$ to judge extrapolation residuals, as discussed in
Section 4.2. Also, the minimum pairwise distance between points in
$P$ is used to catch bad inputs, since after rescaling, any two points
that are closer than $\varepsilon$ will be indistinguishable from
the perspective of {\tt DELAUNAYSPARSES} and could cause hard to find
bugs. The computation of the diameter and minimum pairwise distance
is performed while the points are being rescaled, as discussed in
Section 4.3. The computational complexity of these distance computations
is ${\cal O}\big(n^2d\big)$. Recall that the computational complexity of
the DELAUNAYSPARSE algorithm is ${\cal O}\big(knd^3)$, where $k$ is
independent of $n$ for uniformly spaced $P$. Therefore, in situations
where $n^2d$ is significantly larger than $knd^3$, the
complexity of {\tt DELAUNAYSPARSES} can be dominated by nonessential
distance computations, used only for robustness and extrapolation
checks.

\heading{5. PARALLEL IMPLEMENTATION}

The parallel subroutine {\tt DELAUNAYSPARSEP} is based on the serial
subroutine {\tt DELAUNAYSPARSES} and shares the implementation details
discussed in Sections 4.1--5. {\tt DELAUNAYSPARSEP} uses OpenMP 4.5 
[OpenMP ARB, 2015]
to implement a shared memory paradigm. It is also possible to achieve
distributed parallelism by breaking up the interpolation points $Q$ into
separate batches $Q = Q_1 \cup \ldots \cup Q_B$, then distributing these
batches $Q_i$ across available nodes (along with copies of the data points 
$P$). Each batch can then be evaluated independently on its corresponding
node.

\medskip{\sl Remark 5.1.}\enspace
Recall from Section 4.1 that code optimizations for handling multiple
interpolation points are most effective when the points are clustered
in $DT(P)$. Therefore, optimal distributed memory performance is achieved
when each $Q_i$ represents a cluster of interpolation points from $Q$.
Note that clustering is an open problem, and the above described
distributed memory parallelism can be implemented trivially using separate
calls to {\tt DELAUNAYSPARSES} or {\tt DELAUNAYSPARSEP}. Therefore, a
distributed implementation with clustering of $Q$ is not provided, and
left entirely to the user.
\medskip

For the OpenMP shared memory implementation, two levels of parallelism
are targeted. The first level of parallelism is the loop over all $m$
interpolation points $Q$. The second level of parallelism applies to
the various loops over all $n$ data points $P$ and the loop over all
unresolved interpolation points, as computed by (6) and described in
Section 4.1.
There is also a loop over the $n$ data points for computing the scale
factor (as discussed in Section 4.3) and a pair of nested loops over
the $n$ data points for computing the diameter and minimum pairwise
distance of $P$ (as discussed in Section 4.5).
These loops can be parallelized independently of either level of
parallelism using a static scheduler.

If $m$ is small with respect to the number of available processors,
level one parallelism will not saturate the available computational
resources. In the extreme case where $m=1$, level one parallelism is
not available. However, when available, level one parallelism is
significantly more efficient than level two parallelism. Therefore, the
default behavior for {\tt DELAUNAYSPARSEP} is to exploit level one
parallelism whenever it is available (if $m > 1$), and to exploit level
two parallelism otherwise (if $m = 1$). If this is not the desired
behavior, the type of parallelism can be set manually via an optional
argument, and for advanced
users, both levels can be activated at the same time resulting in
nested parallelism.

\medskip{\sl Remark 5.2.}\enspace
In order for OpenMP to apply nested parallelism, the environment variable
{\tt OMP\_NESTED} must be set to {\tt TRUE}. Furthermore, note that if
a team of $t_1$ threads is deployed at the first level and a team of $t_2$
threads is deployed at the second level, then a total of $t_1 \cdot t_2$
threads could be active at any time. $t_1$ and $t_2$ can be set by
assigning the environment variable {\tt OMP\_NUM\_THREADS=}$t_1$,$t_2$.
\medskip

\heading{5.1 \enspace Level 1 Parallelism}

The first level of parallelism is the loop over $m$ interpolation points
in $Q$: {\bf for all} $q \in Q$ {\bf do}, from Algorithm 1. Since
the variation in the length $k$ of each visibility walk could be large,
{\tt DELAUNAYSPARSEP} uses a dynamic scheduler with a chunk size of one
to parallelize this $Q$ loop. The only dependencies between iterations of
the $Q$ loop occur when implementing the code optimizations described in
Section 4.1.

First, when ``checking ahead'' for future interpolation points
$q_j\in Q$, there is a possible race condition since another thread
could already be performing a visibility walk toward $q_j$. Since these
dependencies are minimal, an OpenMP {\tt CRITICAL} lock is used with
minimal modification to the serial code to ensure safe sequentially
consistent memory accesses. Once any thread of {\tt DELAUNAYSPARSEP} has
begun constructing the first simplex in the walk toward $q_j$, no other
threads will continue to test $q_j$ when ``checking ahead.''
Additionally, if daisy chaining is activated, each thread in the team
must maintain a private copy of the seed simplex. Therefore, only
previously constructed simplices of the active thread are considered
for seeding future visibility walks.

Other than the minor issues discussed above, level one
parallelism is dependency free and dynamically load balanced.
Therefore, under ideal conditions, {\tt DELAUNAYSPARSEP} is capable of
weak scaling with respect to the problem dimension $m$, with negligible
overhead.

\heading{5.2 \enspace Level 2 Parallelism}

The second level of parallelism applies primarily to the various loops
over $n$ data points in $P$: {\bf for all} $p\in P \setminus \{s_1$,
$\ldots$, $s_j(s_d)\}$, as appear in Algorithms 2 and 3. Note that
these $P$ loops are not totally free of dependencies. Therefore, to
parallelize the $P$ loops, private copies of certain variables (such as
$r_{min}$ and $p^*$) must be maintained. Then, after completing these loops
in parallel and producing private solutions, a reduction can be done to
determine the global solutions. Note that this will result in some
redundant computations. There is relatively little room for performance
variation between iterations of the $P$ loops, so {\tt DELAUNAYSPARSEP}
parallelizes them with a static scheduler and a fixed chunk size of
$\lceil n/t_2 \rceil$, where $t_2$ denotes the number of threads in
each level two team.

The one exception to the above methodology is the loop over all future
interpolation points, as described in Section 4.1. This loop is dependency
free and can be parallelized using a static scheduler with a fixed chunk
size of $\lceil(m-i)/t_2\rceil$, where $i$ is the index of the current
interpolation
point. However, if level one parallelism is active, parallelizing this
loop results in significant conflict. Therefore, in the case of nested
(level one and two) parallelism, this loop executes serially within
each level one thread.

\medskip{\sl Remark 5.2.1}\enspace
There is no true dependency between iterations of the above loop over
remaining interpolation points. However, the OpenMP {\tt CRITICAL}
directive used in Section 5.1 locks code segments, as opposed to
variable addresses and cannot distinguish between loop
iterations, inducing a ``false'' conflict. As mentioned above, when
level one parallelism is available, it is recommended that all
available threads be devoted to level one parallelism. Therefore, in
the recommended use case, this loop would not offer significant
parallelism, and serializing it is no significant loss.
\medskip

Due to interloop dependencies, exploiting level two parallelism can
significantly increase the total number of computations performed by
DELAUNAYSPARSE. Furthermore, there are significant regions of serial
code separating each level two parallel block. So, 
the parallel efficiency of {\tt DELAUNAYSPARSEP} with level two
parallelism can be poor.

\heading{6. ORGANIZATION AND USAGE INFORMATION}

The physical organization of the DELAUNAYSPARSE package is described
in its included README file. Most notably, DELAUNAYSPARSE is distributed
with a copy of the REAL\_PRECISION module (from HOMPACK90, ACM TOMS
Algorithm 777), for approximately 64-bit arithmetic on all known
machines. For completeness, all required LAPACK and BLAS subroutines
are included, along with {\tt DWNNLS} and all its dependencies from
the SLATEC library. The included copies of the SLATEC subroutines have
been updated in accordance with the Fortran 2003 standard. Additionally,
legacy implementations for two of {\tt DWNNLS}'s BLAS dependencies 
({\tt DROTM} and {\tt DROTMG}) have
been included under different names. Finally, sample main programs
for the serial and parallel versions illustrating the use of the
optional arguments have been included.
Sample data sets for these main programs are real data sets (with points
not in general position) from the VarSys project on high performance
computing system performance variability [Cameron et al.\ 2019].

The master module DELSPARSE\_MOD includes the REAL\_PRECISION module
and interface blocks for both {\tt DELAUNAYSPARSES} and
{\tt DELAUNAYSPARSEP}, as well as an interface block for the updated
subroutine {\tt DWNNLS}, which may be of separate interest. Note that by
default, {\tt DELAUNAYSPARSES} and {\tt DELAUNAYSPARSEP} do not actually
compute the Delaunay interpolant, but return the containing simplex and
weights for computing (1). This behavior was chosen to accommodate a wide
variety of use cases, including those where the function values $f(x)$
cannot be expressed as real-valued vectors (e.g., when $f(x)$ is a function
$g(\omega;x)$ parameterized by $x$). When the
values $f(x)$ {\it can} be expressed as real-valued vectors, an optional
input argument can be supplied with the response values, and then an
optional output argument must appear to collect interpolation results.
Additionally, recall that $P$ and $Q$ are shifted and rescaled on input.
So, if the original $P$ and $Q$ are needed, copies should
be made prior to execution.

\heading{7. PERFORMANCE}

The approximation accuracy of the Delaunay interpolant has already been
explored in numerous other publications (see Chang et al.\ [2018a],
Lux et al.\ [2018], Omohundro et al.\ [1990], and Schaap et al.\ [2000]).
So, this section will focus on the runtime of {\tt DELAUNAYSPARSES} and
{\tt DELAUNAYSPARSEP}. For reference, first consider Table I, which presents
performance data (as reported by Boissonnat et al.\ [2009]) in up to
six dimensions for computing the complete Delaunay triangulation of uniform
randomly distributed data points in the unit cube using
Quickhull [Barber et al.\ 1996] and the graph based algorithm proposed
by Boissonnat et al.\ [2009]. Boissonnat et al.\ gathered this data
using a 2.6 GHz Intel processor with 6MB of level 2 cache and 4 GB of
DDR2 RAM. The purpose of including Table I is not for direct comparison,
as the problem of computing the complete Delaunay triangulation
is significantly harder than that of locating a single interpolation
point. Indeed, recall that standard Delaunay triangulation algorithms
are not capable of scaling past six or seven dimensions for sufficiently
large problems. Rather, this data is intended to clarify the issues addressed
by DELAUNAYSPARSE, and inform users on when DELAUNAYSPARSE is an appropriate
choice over algorithms that compute the complete Delaunay triangulation.

\topinsert\rmVIII
{\narrower\noindent Table I.
Time and space requirements (as reported in Boissonnat et al.\ [2009])
for computing the complete Delaunay
triangulation using Quickhull and the Delaunay Graph algorithm.
Entries containing the word ``swap'' indicate that the process
exceeded RAM limitations.}
\tabskip=0pt
$$\vbox{\settabs \+ \quad X dimensions, XX,000 points\quad &
\quad space \quad & \quad Delaunay Graph \quad &
\quad Delaunay Graph \quad & \cr
\+ \hfil Problem Sizes ($d$ \& $n$): \hfil & & \hfil Algorithms: \hfil & \cr
\vskip 1pt \hrule \vskip 3pt
\+ & & \hfil Quickhull \hfil & \hfil Delaunay Graph \hfil & \cr
\vskip 1pt \hrule \vskip 3pt
\+ \hfil 5 dimensions, 2,000 points\hfil & \hfil time \hfil
& \hfil 3.2 sec.\ \hfil & \hfil 58 sec.\ \hfil & \cr
\+ & \hfil space \hfil & \hfil 52 MB \hfil & \hfil 10.1 MB \hfil & \cr
\vskip 1pt \hrule \vskip 3pt
\+ \hfil 5 dimensions, 32,000 points\hfil & \hfil time \hfil
& \hfil 76 sec.\ \hfil & \hfil 1463.46 sec.\ \hfil & \cr
\+ & \hfil space \hfil & \hfil 973 MB \hfil & \hfil 106 MB \hfil & \cr
\vskip 1pt \hrule \vskip 3pt
\+ \hfil 6 dimensions, 32,000 points\hfil & \hfil time \hfil
& \hfil swap \hfil & \hfil 28,296 sec.\ \hfil & \cr
\+ & \hfil space \hfil & \hfil swap \hfil & \hfil 267 MB \hfil & \cr
\vskip 1pt \hrule
}$$
\endinsert

To test the performance of {\tt DELAUNAYSPARSES}, runtimes have been
gathered on AMD CPUs @2.3 GHz.
Table II presents runtimes for interpolating at a single interpolation
point (the center of the unit hypercube) using {\tt DELAUNAYSPARSES}, for
various problem sizes ($n$) and dimensions ($d$) with uniform randomly
distributed data in the unit hypercube. To account for performance
variance, each runtime represents an average over 20 independent
runs of {\tt DELAUNAYSPARSES}, each with a different data set of the
same size and dimension. Note that in the higher
dimensions, the data points ($P$) are extremely sparse, even for large
values of $n$. For such problems, it is typical to employ some
intelligent experimental design. Therefore, in the higher dimensions,
the uniform randomly spaced data used for testing becomes increasingly
unrepresentative of real-world data. However, this data is sufficient
for discussing how the runtime of {\tt DELAUNAYSPARSES} scales with $n$
and $d$ and is comparable to the data used to generate Table I.
Since extrapolation presents additional computational complexities,
any data set that does not contain the interpolation
point ($q = [0.5$, $0.5$, $\ldots$, $0.5]^T$) in its convex hull is
discarded and regenerated (an unlikely occurence for the
problem sizes shown).
Note that the distance computations discussed in Section 4.5 cause
an overall computational complexity of ${\cal O}\big(n^2\big)$ in
the lower dimensions.
However, in higher dimensions, the cost of the DELAUNAYSPARSE
algorithm dominates, and the runtimes approach linear growth
with respect to the number of data points $n$, as predicted in 
Section 3.

\topinsert\rmVIII
{\narrower\noindent Table II. Serial runtimes (in seconds).
Values marked ``n/a'' represent problem dimensions for which
$DT(P)$ does not exist ($n < d+1$), and values marked ``slow''
were aborted due to excessive runtimes (greater than 3600 seconds).}
\tabskip=0pt
$$\vbox{\settabs \+ Problem size ($n$):\qquad\qquad& \quad 00.000 \quad
&\quad 00.000 \quad&\quad 00.000 \quad&\quad 00.000 \quad
&\quad 00.000 \quad&\cr
\+ & Problem dimension ($d$) \cr
\vskip 1pt
\+ Problem size ($n$) & \hfil 2 \hfil & \hfil 8 \hfil
& \hfil 32 \hfil & \hfil 64 \hfil & \hfil 128 \hfil &\cr
\vskip 3pt \hrule \vskip 5pt
\+ \hfill 100 \qquad\qquad & \hfil 0.001 \hfil & \hfil 0.007 \hfil
& \hfil 0.307 \hfil & \hfil 0.800 \hfil & \hfil n/a \hfil & \cr
\vskip 2pt
\+ \hfill 500 \qquad\qquad & \hfil 0.022 \hfil & \hfil 0.063 \hfil
& \hfil 1.822 \hfil & \hfil 25.563 \hfil & \hfil 316.566 \hfil & \cr
\vskip 2pt
\+ \hfill 2000 \qquad\qquad & \hfil 0.337 \hfil & \hfil 0.672 \hfil
& \hfil 8.326 \hfil & \hfil 109.754 \hfil & \hfil 1443.694 \hfil & \cr
\vskip 2pt
\+ \hfill 8000 \qquad\qquad & \hfil 5.219 \hfil & \hfil 9.448 \hfil 
& \hfil 50.067 \hfil & \hfil 481.307 \hfil & \hfil slow \hfil & \cr
\vskip 2pt
\+ \hfill 16,000 \qquad\qquad & \hfil 20.930 \hfil & \hfil 37.078 \hfil
& \hfil 157.123 \hfil & \hfil 1080.377 \hfil & \hfil slow \hfil & \cr
\vskip 2pt
\+ \hfill 32,000 \qquad\qquad & \hfil 83.394 \hfil & \hfil 148.078 \hfil
& \hfil 504.267 \hfil & \hfil 2443.431 \hfil & \hfil slow \hfil & \cr
\vskip 3pt \hrule
}$$
\endinsert

To compare the performance of {\tt DELAUNAYSPARSEP} at all
levels of parallelism against the performance of
{\tt DELAUNAYSPARSES}, runtimes have been gathered over a cluster of
eight NUMA nodes with four AMD cores per node @2.3 GHz (each core
identical to when timing {\tt DELUNAYSPARSES}).
Figures 1--3 plot the average elapsed wallclock time (in seconds) for
20 independent runs of {\tt DELAUNAYSPARSEP} against the number of
cores used by OpenMP. The data for these experiments was generated
using a randomized Latin hypercube design, as described in Amos et
al.\ [2014]. Then the interpolation points were generated from random
convex combinations of $d+1$ randomly selected points from the design.

Figure 1 presents runtimes for interpolating at 1024 points in a
10-dimensional design with 1000 data points, reflecting workloads
where $m$ is large.
Figure 2 presents runtimes for interpolating at 64 points in a
10-dimensional design with 10,000 data points, reflecting workloads
where $n$ is large.
Figure 3 presents runtimes for interpolating at 64 points in a
20-dimensional design with 200 data points, reflecting workloads
where $d$ is large.
Note that Figures 1--3 are presented at $\log_2$ scale.
For nested parallel runs with four, eight, 32 total active cores,
the number of level one (two) threads is two, four, eight
(two, four, four), respectively.

\topinsert
\centerline{\epsfxsize=4.5truein \epsffile{vt_logo.eps}}
{\narrower\noindent\rmVIII Fig.\ 1.
Average runtime in seconds plotted against number of active cores
for a 10-dimensional problem, with $n=1000$ and $m=1024$.
\par}
\endinsert

\midinsert
\centerline{\epsfxsize=4.5truein \epsffile{vt_logo.eps}}
{\narrower\noindent\rmVIII Fig.\ 2.
Average runtime in seconds plotted against number of active cores
for a 10-dimensional problem, with $n=10,000$ and $m=64$.
\par}
\endinsert

\midinsert
\centerline{\epsfxsize=4.5truein \epsffile{vt_logo.eps}}
{\narrower\noindent\rmVIII Fig.\ 3.
Average runtime in seconds plotted against number of active cores
for a 20-dimensional problem, with $n=200$ and $m=64$.
\par}
\endinsert

In all three figures, level one parallelism achieves the fastest average
runtimes, as expected. Furthermore, note that overapplying level two
parallelism can significantly increase overall run time, due to the
significant number of added computations as well as communication overhead.
However, one can see that level two parallelism is most applicable for
extremely large values of $n$. Furthermore, for situations where the
number of interpolation points is sufficient to achieve some level one
parallelism but insufficient to saturate the system,
nested parallelism is able to compete with level one parallelism.
It is somewhat surprising to observe that despite having the longest
overall runtime, the problem size used for Figure 1 ($d=10$, $n=1000$, 
$m=1024$) offered the least parallelism for all modes {\tt DELAUNAYSPARSEP}.
In particular, it may seem surprising that level one parallelism did not
perform better at this problem size, given that level one parallelism
is maximized for large values of $m$.

In fact, when the cost of performing each individual flip is relatively
small and the thread count per contention group is relatively high, level
one threads can be blocked to the point of serialization by {\tt CRITICAL}
locks during the loop to ``check ahead,'' as described in Section 5.1. This
presents a trade-off since checking ahead does not result in significant
conflict for relatively large values of $n$ and $d$ and could offer
significant performance benefits when $Q$ is clustered. In the cases
where the cost of each flip is small, it is recommended that users
partition the interpolation points into
$Q = Q_1 \cup \ldots \cup Q_B$. Then each $Q_i$ can be handled by a
separate call to {\tt DELAUNAYSPARSEP} with level one parallelism
and an appropriate thread count.
For example, in the case of Figure 1, for optimal performance over
32 active cores, {\tt DELAUNAYSPARSEP} could be called twice, with 16
threads and 512 interpolation points per call.
In general, the optimal choices for $B$, the thread count, and the
physical partition are highly problem dependent and beyond the scope
of this work.

\heading{\hlvVIII BIBLIOGRAPHY} {\let\caps=\capsVIII \let\sl=\slVIII
\def\ys{\hskip 1em minus .5em}\let\rm=\rmeight \rmVIII
\newcount\refnum \refnum=0 \parskip=2pt plus 2pt minus 2pt
\frenchspacing

%\refj is for regular journal articles
\def\refj#1#2#3#4#5{\noindent\hangindent=10pt\hangafter=1
%\global\advance\refnum by 1[\number\refnum]
{\caps #1}\ys #2.\ys {\rm #3}. {\sl #4}, #5.\par}

%\refb is for books
\def\refb#1#2#3#4{\noindent\hangindent=10pt\hangafter=1
%\global\advance\refnum by 1[\number\refnum]
{\caps #1}\ys #2.\ys{\sl #3}. {\rm #4}.\par}

%\refp is for private communication
\def\refp#1#2#3{\noindent\hangindent=10pt\hangafter=1
%\global\advance\refnum by 1[\number\refnum]
{\caps #1}\ys #2.\ys{\rm #3}.\par}

%\reft is for tech. reports, theses, and conferences
\def\reft#1#2#3#4{\noindent\hangindent=10pt\hangafter=1
%\global\advance\refnum by 1[\number\refnum]
{\caps #1}\ys #2.\ys{\rm #3}. {\rm #4}.\par}

%%%%% top of refs
\reft{Amos, B. D., Easterling, D. R., Watson, L. T., Thacker, W. I.,
Castle, B. S., Trosset, M. W.}{2014}{Algorithm XXX: QNSTOP: Quasi-Newton
algorithm for stochastic optimization}{Technical Report 2014-07,
Virginia Polytechnic Institute and State University, Blacksburg, VA}

\refb{Anderson, E., Bai, Z., Bischof, C., Blackford, S., Demmel, J.,
Dongarra, J., Du Croz, J., Greenbaum, A., Hammarling, S., McKenney, A.,
and Sorensen, D.}{1999}{LAPACK Users' Guide Third Edition}
{SIAM, Philidelphia, PA}

\refb{Aurenhammer, F., Klein, R., and Lee, D. T.}{2013}
{Voronoi diagrams and Delaunay triangulations}
{World Scientific Publishing Co., Hackensack, NJ}

\refj{Barber, C. B., Dobkin, D. P., and Huhdanpaa, H.}{1996}
{The Quickhull algorithm for convex hulls}{ACM Trans. Math.
Softw. 22}{469--483}

\reft{Boissonnat, J.-D., Devillers, O., and Hornus, S.}{2009}
{Incremental construction of the Delaunay triangulation and the
Delaunay graph in medium dimension}{In {\sl Proc. Twenty-fifth Annual Symp.
on Computational Geometry}, ACM, New York, NY, 208--216}

\refj{Bowyer, A.}{1981}{Computing Dirichlet tessellations}
{The Computer Journal 24}{162--166}

\refj{Cameron, K. W., Anwar, A., Cheng, Y., Xu, L., Li, B., Ananth, U.,
Bernard, J., Jearls, C., Lux, T., Hong, Y., Watson, L. T.,
and Butt, A. R.}{2019}{MOANA: modeling and analyzing I/O variability
in parallel system experimental design}
{IEEE Trans. Parallel Distrib. Systems}{to appear}

\reft{Chang, T. H., Watson, L. T., Lux, T. C. H., Bernard, J., Li, B.,
Xu, L., Back, G., Butt, A. R., Cameron, K. W., and Hong, Y.}{2018a}
{Predicting system performance by interpolation using a high-dimensional
Delaunay triangulation} {In {\sl Proc. 2018 Spring Simulation Multiconference,
26th High Performance Computing Symp.}, Soc. for Modelling and Simulation
Internat., Vista, CA, Article No. 2}

\reft{Chang, T. H., Watson, L. T., Lux, T. C. H., Li, B., Xu, L., 
Butt, A. R., Cameron, K. W., and Hong, Y.}{2018b}{A polynomial time
algorithm for multivariate interpolation in arbitrary dimension via
the Delaunay triangulation} {In {\sl Proc. ACM 2018 Southeast Conference
(ACMSE '18)}, ACM, New York, NY, Article No. 12}

\refb{Cheney, E. W. and Light, W. A.}{2009}
{A Course in Approximation Theory}{American Mathematical Soc., 
Providence, RI}

\refb{Cheng, S. W., Dey, T. K., and Shewchuk, J.}{2012}
{Delaunay Mesh Generation}{Computer and Information Science
Series, CRC Press, New York, NY}

\refj{Cignoni, P., Montani, C., and Scopigno, R.}{1998}
{DeWall: A fast divide \& conquer Delaunay triangulation algorithm in $E^d$}
{Computer-Aided Design 30}{333--341}

\refb{de Berg, M., Cheong, O., van Kreveld, M., and Overmars, M.}{2008}
{Computational Geometry: Algorithms and Applications}{Springer-Verlag,
Berlin, Germany}

\refj{Devillers, O., Pion, S., and Teillaud, M.}{2002}
{Walking in a triangulation}{Internat. Journal of Foundations of
Computer Science 13}{181--199}

\reft{Edelsbrunner, H.}{1989}{An acyclicity theorem for cell complexes in
d dimensions} {In {\sl Proc. Fifth Annual Symp. on Computational Geometry}, 
ACM, New York, 145--151}

\refj{Hanson, R. J. and Haskell, K. H.}{1982}
{Algorithm 587: Two algorithms for the linearly constrained least
squares problem}{ACM Trans. Math. Softw. 8}{323--333}

\refj{Klee, V.}{1980}{On the complexity of d-dimensional Voronoi diagrams}
{Archiv der Mathematik 34}{75--80}

\reft{Lux, T. C. H., Watson, L. T., Chang, T. H., Bernard, J., Li, B.,
Xu, L., Back, G., Butt, A. R., Cameron, K. W., and Hong, Y.}{2018}
{Predictive modeling of I/O characteristics in high performance computing
systems}{In {\sl Proc. 2018 Spring Simulation Multiconference, 26th High 
Performance Computing Symp.}, Soc. for Modelling and Simulation Internat.,
Vista, CA, Article No. 8}

\refj{M{\"u}cke, E. P., Saias, I., and Zhu, B.}{1999}
{Fast randomized point location without preprocessing in two- and 
three-dimensional Delaunay triangulations}
{Computational Geometry 12}{63--83}

\refj{Omohundro, S. M.}{1990}{Geometric learning algorithms}
{Physica D: Nonlinear Phenomena 42}{307--321}

\reft{OpenMP Architecture Review Board (ARB)}{2015}
{OpenMP Application Programming Interface (version 4.5)}
{OpenMP}

\refj{Rajan, V. T.}{1994}
{Optimality of the Delaunay triangulation in $R^d$}
{Discrete \& Computational Geometry 12}{189--202}

\refj{Schaap, W. E. and van de Weygaert, R.}{2000}
{Continuous fields and discrete samples: reconstruction through
Delaunay tessellations}{Astronomy and Astrophysics 363}{L29--L32}

\refj{Su, P. and Drysdale, R. L. S.}{1997}
{A comparison of sequential Delaunay triangulation algorithms}
{Computational Geometry 7}{361--385}

\refj{Watson, D. F.}{1981}{Computing the n-dimensional Delaunay tessellation
with application to Voronoi polytopes}
{The Computer Journal 24}{167--172}

}\vfill\eject\end
%%%%% bottom

