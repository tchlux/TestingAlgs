import os, pickle, time
import numpy as np
from util.data import read_struct
from util.stats import cdf_fit_func, ks_diff, ks_p_value
from util.algorithms import VoronoiMesh, MaxBoxMesh, Delaunay, \
    CLASS_NAME, SMALL_NUMBER

CLEAN_DATA_CSV = "clean_data.csv"
CLEAN_DATA_CSV_PKL = "clean_data.csv.pkl"
DATA_OUTPUT_FILE = "results.csv"
EXAMPLE_PREDICTION_FILE = "Delaunay_Example_Prediction.csv"

ALGORITHMS = Delaunay, MaxBoxMesh, VoronoiMesh
TRAINING_PERCS = np.linspace(95,5,19)
NUM_FOLDS = 10
RANDOM_SEED = 1
DEFAULT_TRAINING_PERC = 80
MAX_COMPUTE_TIME = 60*60*3
P_VALUES = [0.05, 0.01, 0.001, 1e-6]
KS_VALUES = [0.1568, 0.1879, 0.2251, 0.3110]

RUN_PERFORMANCE_ANALYSIS = False
OPTIMIZE_FEATURE_WEIGHTS = False
FIND_NEW_OPTIMAL_SOLUTIONS = False
DELAUNAY_EXAMPLE_PREDICTION = True

if not os.path.exists(CLEAN_DATA_CSV_PKL):
    data = read_struct(CLEAN_DATA_CSV)
    print("Saving '%s'..."%(CLEAN_DATA_CSV_PKL))
    with open(CLEAN_DATA_CSV_PKL, "wb") as f:
        pickle.dump(data,f)
else:
    print("Loading '%s'..."%(CLEAN_DATA_CSV_PKL))
    with open(CLEAN_DATA_CSV_PKL, "rb") as f: 
        data = pickle.load(f)

header = list(data.dtype.names)
h_config = header[:4]
h_test = header[4]
h_values = header[5:]
tests = sorted(np.unique(data[h_test]))
test_len = max(map(len, tests))
num_configs = len(h_config)

print("Tests:", tests)
print("Number of configuration columns:", num_configs)
print()

# print("Converting '%s' to numbers via the following map:"%h_test)
# test_avg_throughput = {}
# for t in tests:
#     subdata = data[data[h_test] == t]
#     subdata = np.vstack((subdata[h] for h in h_values)).T
#     mean = np.mean(subdata)
#     test_avg_throughput[t] = mean
# # Identify the range of average throughputs for each test (for normalization)
# shift = min(test_avg_throughput.values())
# scale = max(test_avg_throughput.values()) - shift
# # Print out the mapping used to generate the numeric values for each test
# for t in tests:
#     test_avg_throughput[t] = (test_avg_throughput[t]-shift) / scale
#     print(("%"+str(test_len)+"s")%(t), test_avg_throughput[t])
# print()
# # Generate the new numeric column and combine all numeric data into one array
# new_column = np.array([test_avg_throughput[row[h_test]] for row in data])
# numeric_data = np.vstack(tuple(data[h] for h in h_config)+(new_column,)+
#                          tuple(data[h] for h in h_values)).T
# num_configs = len(h_config)+1

if RUN_PERFORMANCE_ANALYSIS:
    # Code for doing some manual testing before full executions. Pick
    # which algorithms, fold numbers, and training percentages to test.
    testing = False
    test_testnms = {"readers"}
    test_classes = {"Delaunay", "MaxBoxMesh"}
    test_trainpc = {95}
    test_foldnum = {2}
    # Open up the output data file (in append mode in case I forgot to
    # do something with the old contents)
    # 
    # WARNING: DO NOT EDIT THIS FILE BEFORE EXECUTION COMPLETES,
    #          PYTHON WILL STOP WRITING TO THE FILE IF EDITS ARE SAVED
    # 
    with open(DATA_OUTPUT_FILE, "a") as data_output_file:
        print(*(h_config+[h_test, "Algorithm", "Train Percentage" ,"Train Size",
                          "Test Size", "Fit Time", "Eval Time",
                          "KS Statistic", "Average Absolute Error",
                          "Median Error", "Average Error"]),
              sep=",", file=data_output_file, flush=True)
        # Cycle all the different tests
        for test_name in tests:
            # Identify those rows of the source data that have the assigned test
            subdata = data[data[h_test] == test_name]
            # Convert that subset of "data" to a numeric-valued matrix
            numeric_data = np.vstack((subdata[h] for h in h_config+h_values)).T
            # Normalize the configuration columns of the numeric data
            config_shift = np.min(numeric_data, axis=0)[:num_configs]
            numeric_data[:,:num_configs] -= config_shift
            config_scale = np.max(numeric_data, axis=0)[:num_configs]
            numeric_data[:,:num_configs] /= config_scale
            print()
            print("Shape of data for '%s':"%(test_name), numeric_data.shape)
            # Calculate the linearly-interpolated CDF fit functions for all
            # of the rows of numerical data (excluding configuration columns)
            cdf_funcs = [cdf_fit_func(row[num_configs:]) for row in numeric_data]
            # Calculate the "Truth" CDF fit function values for each row.
            func_evaluations = np.array([list(f(row[num_configs:])) 
                                         for (f,row) in zip(cdf_funcs, numeric_data)])
            print()
            for alg in ALGORITHMS:
                print(CLASS_NAME(alg), flush=True)
                # Keep an array of selected indices for trianing / testing
                indices = np.arange(len(numeric_data))
                # Seed each algorithm with the same seed number for repeatability
                np.random.seed(RANDOM_SEED)
                for train_perc in TRAINING_PERCS:
                    # Identify the appropriate size of training data based on percentage
                    train_size = int(round((train_perc/100) * len(numeric_data)))
                    test_size = len(numeric_data) - train_size
                    print("","Train size, test size:",train_size, test_size, flush=True)
                    # Keep a holder for the prediction performance on
                    # each configuration (all configurations should have
                    # more than one prediction as a result of the folds)
                    stats = {}
                    # Run random folds of predictions (all results will be averaged)
                    for fold in range(1,NUM_FOLDS+1):
                        # The only source of randomness in this entire
                        # procedure, the shuffling of selection indices
                        np.random.shuffle(indices)
                        # Skipping some algorithms for testing (after
                        # shuffle to make seeded randomness consistent)
                        if (testing and (
                                (test_name not in test_testnms) or
                                (CLASS_NAME(alg) not in test_classes) or
                                (round(train_perc) not in test_trainpc) or
                                (fold not in test_foldnum))): continue
                        # Track the time required for each fold (for user purposes)
                        fold_time = time.time()
                        print("","","Fold: %i"%fold, end="", flush=True)
                        # Collect the train and test data, as well as the
                        # training functions and the testing func evaluations
                        train = numeric_data[indices[:train_size]]
                        test = numeric_data[indices[train_size:]]
                        train_funcs = [cdf_funcs[i] for i in indices[:train_size]]
                        test_func_vals = func_evaluations[indices[train_size:]]
                        # Construct the model over the points and
                        # evaluate it to get the convex weights of
                        # source points to generate an interpolant
                        model = alg()
                        start = time.time()
                        model.fit(train[:,:num_configs])
                        fit_time = time.time() - start
                        start = time.time()
                        points, weights = model.points_and_weights(test[:,:num_configs])
                        eval_time = time.time() - start
                        # Generate function approximations based on provided
                        # convex interpolation weights for each test config.
                        for (row, pts, wts, true_cdf_vals) in zip(
                                test, points, weights, test_func_vals):
                            approximation = sum(train_funcs[p](row[num_configs:])*w
                                                for (p,w) in zip(pts, wts) 
                                                if (w > SMALL_NUMBER))
                            errors = approximation - true_cdf_vals
                            # De-normalize the configuration for record-keeping
                            config = tuple("%.0f"%val for val in (
                                row[:num_configs]*config_scale + config_shift))
                            # Add this row of performance to the data holder
                            if config not in stats: stats[config] = []
                            stats[config] += [[fit_time, eval_time, max(abs(errors)),
                                               sum(abs(errors))/len(errors),
                                               sorted(errors)[len(errors)//2],
                                               sum(errors)/len(errors)]]
                        # END for (row, pts, wts, true_cdf_vals) ...
                        print(" -- (%.1f, %.1f)"%(fit_time,eval_time),"%.1f"%(time.time() - fold_time), flush=True)
                    # END for fold in range(1,NUM_FOLDS+1)

                    # Write a data entry for each predicted point, averaging
                    # over the results for the random folds.
                    for config in sorted(stats.keys()):
                        # Average over the FOLDS in order to get results for this train_percentage
                        stats[config] = np.mean(np.array(stats[config]), axis=0)
                        row  = list(config)
                        row += [test_name, CLASS_NAME(alg), round(train_perc), 
                                train_size, test_size, 
                                "%.1e"%stats[config][0],
                                "%.1e"%stats[config][1]]
                        row += list(stats[config][2:])
                        print(*row, sep=",", file=data_output_file, flush=True)
                    # END for config in sorted(stats.keys())
                # END for train_perc in TRAINING_PERCS
            # END for alg in ALGORITHMS
        # END for test_name in tests
    # END with open(DATA_OUTPUT_FILE, "a") as data_output_file
# END if RUN_PERFORMANCE_ANALYSIS


#      Optimization as Feature Selection     
# ===========================================

d_weights = [0.0102623,      1.98442716, 2.,         0.32333087]
m_weights = [0.002407,       1.71148387, 0.80171701, 0.12522154]
v_weights = [1.19830984e-03, 2.,         1.79176691, 1.53159404]
best_weights = {"Delaunay":d_weights, "MaxBoxMesh":m_weights, "VoronoiMesh":v_weights}

# Given training data, testing data, the functions for each row in
# training data, the function evaluations for each row in testing
# data, and the algorithm to build the model with, calculate the KS
# statistic of the prediction performance at each testing point and
# return the list of KS statistics.
def eval_model(train_data, test_data, train_funcs, test_vals, alg):
    # Build the model, fit the model, and evaluate the model
    model = alg()
    model.fit(train_data[:,:num_configs])
    points, weights = model.points_and_weights(test_data[:,:num_configs])
    perfs = []
    # Generate function approximations based on provided
    # convex interpolation weights for each test config.
    for (row, pts, wts, true_cdf_vals) in zip(
            test_data, points, weights, test_vals):
        approximation = sum(train_funcs[p](row[num_configs:])*w
                            for (p,w) in zip(pts, wts) 
                            if (w > SMALL_NUMBER))
        errors = approximation - true_cdf_vals
        perfs.append( max(abs(errors)) )
    return perfs

# Given data, functions for each row, function evaluations for each
# row, and an algorithm, perform folded evaluation of the KS
# statistic of the algorithm.
def folded_evaluation(data, funcs, func_evals, alg,
                      train_perc=DEFAULT_TRAINING_PERC, num_folds=NUM_FOLDS):
    train_size = round(len(data)*train_perc/100)
    # Keep an array of selected indices for trianing / testing
    indices = np.arange(len(data))
    perfs = []
    # Run random folds of predictions (all results will be averaged)
    for fold in range(1,num_folds+1):
        # The only source of randomness in this entire
        # procedure, the shuffling of selection indices
        np.random.shuffle(indices)
        # Collect the training and testing data, as well as the
        # training and testing function evaluations
        train_data = data[indices[:train_size]]
        test_data = data[indices[train_size:]]
        train_funcs = [funcs[i] for i in indices[:train_size]]
        test_evals = func_evals[indices[train_size:]]
        perfs += eval_model(train_data, test_data, train_funcs, test_evals, alg)
    return sum(perfs) / len(perfs)

if OPTIMIZE_FEATURE_WEIGHTS:
    # Import a generic minimization algorithm
    from util.optimize import minimize
    subdata = data[data[h_test] == "readers"]
    # Convert that subset of "data" to a numeric-valued matrix
    numeric_data = np.vstack((subdata[h] for h in h_config+h_values)).T
    # Normalize the configuration columns of the numeric data
    config_shift = np.min(numeric_data, axis=0)[:num_configs]
    numeric_data[:,:num_configs] -= config_shift
    config_scale = np.max(numeric_data, axis=0)[:num_configs]
    numeric_data[:,:num_configs] /= config_scale
    # Optimization preparation
    bounds = np.array([(0.,2.)]*num_configs)
    init_solution = np.ones(num_configs, dtype=np.float64)
    # Calculate the linearly-interpolated CDF fit functions for all
    # of the rows of numerical data (excluding configuration columns)
    cdf_funcs = [cdf_fit_func(row[num_configs:]) for row in numeric_data]
    # Calculate the "Truth" CDF fit function values for each row.
    func_evaluations = np.array([list(f(row[num_configs:])) for (f,row) in zip(cdf_funcs, numeric_data)])
    # Settings for splitting the data
    train_perc = DEFAULT_TRAINING_PERC
    train_size = round((train_perc/100) * len(numeric_data))
    opt_train_perc = DEFAULT_TRAINING_PERC
    opt_train_size = round(train_size*(opt_train_perc/100))
    print("Train perc:",train_perc)
    print("Train size:",train_size)
    print("Test size: ",len(numeric_data)-train_size)
    print("Opt train perc:",opt_train_perc)
    print("Opt train size:",opt_train_size)
    print("Opt test size: ",train_size - opt_train_size)
    for alg in ALGORITHMS:
        print()
        print(CLASS_NAME(alg))
        # Seed each algorithm with the same seed number for repeatability
        np.random.seed(RANDOM_SEED)
        # Keep an array of selected indices for trianing / testing
        indices = np.arange(len(numeric_data))
        train_data = numeric_data[indices[:train_size]]
        test_data = numeric_data[indices[train_size:]]
        train_funcs = [cdf_funcs[i] for i in indices[:train_size]]
        func_evals = func_evaluations[indices[:train_size]]
        test_evals = func_evaluations[indices[train_size:]]
        # ============================================================
        # Make an optimization function that estimates validation
        # error given a set of feature-weights and returns the KS-stat
        # to estimate the performance of a model.
        def opt_func(weights):
            opt_data = train_data.copy()
            opt_data[:,:num_configs] *= np.array(weights)
            print("","",("%.2e  "*len(weights))%tuple(weights))
            return folded_evaluation(opt_data, train_funcs, func_evals, alg)
        # ============================================================
        if FIND_NEW_OPTIMAL_SOLUTIONS:
            print("Minimizing...")
            weight = minimize(opt_func, init_solution.copy(), bounds.copy(),
                              max_time=MAX_COMPUTE_TIME, display=True)
            print("Found weighting:", list(weight))
        else:
            weight = best_weights[CLASS_NAME(alg)]
        # Analyze the performance of each model with different sets of weights
        for alg_name, weight in sorted(best_weights.items()):
            print("",alg_name)
            # Calculate validation and testing accuracy
            validation_score = opt_func(weight)
            # Copy the actual data (as not to tamper with it)
            train = train_data.copy()
            test = test_data.copy()
            # Get unchanged testing scores
            testing_scores = eval_model(train, test, train_funcs, test_evals, alg)
            unscaled_testing_score = sum(testing_scores)/len(testing_scores)
            # Print out the KS fail rates for each p value
            print("","","Unweighted:",unscaled_testing_score)
            for ks,p in zip(KS_VALUES,P_VALUES):
                print("","","",p,round(100*sum(np.where(np.array(testing_scores)>ks,1,0))/len(testing_scores),2))
            # Rescale according to optimal weights
            train[:,:num_configs] *= np.array(weight)
            test[:,:num_configs] *= np.array(weight)
            # Evaluate for testing the scaled version
            testing_scores = eval_model(train, test, train_funcs, test_evals, alg)
            testing_score = sum(testing_scores)/len(testing_scores)
            print("","","Validation:", validation_score)
            print("","","Testing:   ",testing_score)
            # Print out the KS fail rates for each p value
            for ks,p in zip(KS_VALUES,P_VALUES):
                print("","","",p,round(100*sum(np.where(np.array(testing_scores)>ks,1,0))/len(testing_scores),2))

    # Delaunay: [ 0.0103  1.9844  2.0000  0.3233]  0.14
    # MBM:      [ 0.0024  1.7115  0.8017  0.1252]  0.18
    # VM:       [ 0.0012  2.0000  1.7918  1.5316]  0.11

    #  0.1568      0.05   
    #  0.1879      0.01   
    #  0.2251      0.001  
    #  0.3110      1.0e-6 


if DELAUNAY_EXAMPLE_PREDICTION:
    print(len(data))
    np.random.seed(RANDOM_SEED)
    subdata = data[data["Test"] == "readers"]
    median_point = np.array([(2000000, 16384, 8192, 16)])
    # Extract the configurations and normalize them
    normalized_configs = np.vstack([subdata[h] for h in h_config + h_values]).T
    # Remove the median point from the configurations
    to_remove = [i for i,row in enumerate(normalized_configs) if
                 tuple(row[:len(h_config)]) == tuple(median_point[0])]
    median_row = normalized_configs[to_remove][0]
    normalized_configs = np.delete(normalized_configs, to_remove, axis=0)
    # Normalize the configurations
    shift = np.min(normalized_configs[:,:len(h_config)], axis=0)
    normalized_configs[:,:len(h_config)] -= shift
    scale = np.max(normalized_configs[:,:len(h_config)], axis=0)
    normalized_configs[:,:len(h_config)] /= scale
    median_point = (median_point - shift) / scale
    # Reduce the set of training points
    indices = np.arange(len(normalized_configs))
    np.random.shuffle(indices)
    np.random.shuffle(indices)
    np.random.shuffle(indices)
    np.random.shuffle(indices)
    train_size = round(len(indices) * DEFAULT_TRAINING_PERC / 100)
    normalized_configs = normalized_configs[indices[:train_size]]
    # Build the model and generate a prediction
    model = Delaunay()
    model.fit(normalized_configs[:,:len(h_config)])
    pts, wts = model.points_and_weights(median_point)
    print(pts)
    print(wts)
    normalized_configs[:,:len(h_config)] *= scale
    normalized_configs[:,:len(h_config)] += shift
    median_values = np.array(sorted(median_row[len(h_config):]))
    with open(EXAMPLE_PREDICTION_FILE,"w") as f:
        print("true",0,*median_row, sep=",", file=f)
        guesses = []
        for i,(p,w) in enumerate(zip(pts[0],wts[0])):
            if (w <= 0): continue
            print("source",w,*normalized_configs[p], sep=",", file=f)
            fit = cdf_fit_func(normalized_configs[p,len(h_config):])
            guesses.append(w * fit(median_values))
        guesses = np.sum(np.array(guesses), axis=0)
        print("guess",1,*(median_row[:len(h_config)]), *guesses,sep=",",file=f)
    true_fit = cdf_fit_func(median_values)
    error = guesses - true_fit(median_values)
    worst_idx = np.argmax(abs(error))
    print(max(abs(error)), median_values[worst_idx],
          guesses[worst_idx], true_fit(median_values)[worst_idx])
    # for (h,v) in zip(h_config, median_point):
    #     subdata = subdata[subdata[h] == v]
    # print(len(subdata))


# All the following code should be general so that I can plug in the
# old and new varsys data.

# Convert "test" into a numeric value (based on a summary statistic)

# Optimize over the weights on the dimensions to improve validation
# performance, see if this improves testing performance.

# Compare with performance over restricted models with just one test.

# Do a light stastical analysis of the differnces between different
# tests, different threads, frequencies, and file + record sizes.

# Do variance predictions and demonstrate difficulty in predicting
# mean and variance in throughput.


