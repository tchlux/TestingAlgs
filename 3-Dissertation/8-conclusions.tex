
This body of work reveals many interesting findings related to modeling variability in computer systems and approximation in general. While generic ``off the shelf'' techniques from machine learning are immediately capable of being applied to the prediction of variability summary statistics, a little extra effort can lend significant improvements in accuracy and precision. The major takeaways from this work are threefold:

\begin{enumerate}
\item computer performance data is often \textit{not} normally distributed and accurate descriptions of performance should rely on nonparametric methods or mixtures of parametric distributions;
\item interpolants such as Delaunay and Shepard methods methods tend to perform better than regressors when used to predict variability-related performance measures, especially when the amount of available data is limited; and
\item in medium to high dimension regressors and interpolants produce equally accurate approximations on analytic test functions, but interpolants favor applications where data is either limited or rapidly changing (i.e., real time systems), because they do not have the \textit{fit} time required by regressors.
\end{enumerate}

Acknowledging these takeaways, there is some room to speculate on the most promising directions for future research related to variability approximation. Specifically, consider systems that need to operate in real time, making approximations in less than a tenth of a millisecond with dynamically updating data. For problems in less than tens of dimensions and with at most thousands of points, it appears best to use Delaunay to make predictions of either means, variances, or full distributions. When the number of points or dimension goes up to tens / hundreds of dimensions with tens of thousands of points, Shepard methods remain particularly well suited to making variability predictions. Finally if there are hundreds / thousands or more dimensions and tens to hundreds of thousands of data points, then a fast $k$ nearest neighbor lookup mixed with a Shepard method could give good results. Notably, all of these approximation methods are interpolants that (given nondegenerate data) require no concept of a \textit{fit}. In addition, all of these interpolants are readily capable of predicting distribution outcomes.

One of the most popular approximation methods in the research community at present is neural networks. While they show incredible promise in computer vision, reinforcement learning, and stream (text sequence) processing, they have failed to maintain superior performance when applied specifically to computer performance modeling. However, the most interesting line of research that ties together interpolants, real time systems, and neural networks is the application of a neural network as a preconditioner for interpolation methods. For instance, train a neural network on computer system data offline, then remove the final layer and instead use the transformed representation of data in the last internal layer as a new vector representation for standard interpolation. Fundamentally this takes advantage of the best components from each method, the structure processing / compositional nature of neural networks, and the ability to quickly make real time predictions from recently generated (or otherwise rapidly changing) runtime data.

Lastly, the improvements in distribution approximation provided by the \texttt{MQSI} software package are not immediately acquired for any data. There is work to be done investigating the best mixture of sampling and smoothing methods to generate a reasonable set of empirical distribution function (EDF) points that can be interpolated by a monotone quintic spline. While initial experiments offered the selection of uniformly distribution EDF points, this may not be optimal and the question of how many points is best remains open. Traditionally Gaussian smoothing or some other method may be used to translate an EDF into a good distribution approximation, however these methods require the (often arbitrary) selection of multiple parameters. Ideally, one would refine the EDF to a small set of points with high confidence that would afford the best possible estimates of first and second derivatives by the quadratic facet model in \texttt{MQSI}.

This work provides a deep dive into the best methodologies for approximating variability in computer systems. By comparing known approximation techniques, proposing new approximations, extending variability modeling from real-valued (parametric) prediction to distribution-valued (nonparametric) prediction, proving a novel error bound for piecewise linear interpolants, and writing a mathematical software package for constructing monotone quintic spline interpolants, this work has significantly contributed to the study of computer system variability and approximation by interpolants. The body of this work was possible largely due to funding and support from the National Science Foundation, and support and guidance from the faculty and staff of Virginia Polytechnic Institute and State University.

