\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{viii}{chapter*.2}}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{xv}{chapter*.3}}
\citation{grobelny2007fase,wang2009simulation,wang2013towards}
\citation{grobelny2007fase}
\citation{snavely2002framework,bailey2005performance,barker2009using,ye2010analyzing}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}The Importance and Applications of Variability}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:apps}{{1}{1}{The Importance and Applications of Variability}{chapter.1}{}}
\citation{beckman2008benchmarking,de2007identifying}
\citation{bailey2005performance}
\citation{lofstead2010managing}
\citation{de2008field}
\citation{lazos2014optimisation}
\citation{cortez2008using,lux2016applications}
\citation{cortez2007data}
\citation{tsanas2010accurate}
\citation{williams2009rattle}
\citation{pozzolo2015calibrating}
\citation{lux2018nonparametric}
\citation{cheney2009course}
\citation{de1978practical}
\citation{unther1996interpolating}
\citation{de2013box}
\citation{lux2018novel}
\citation{dennis1996numerical}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Broader Applications of Approximation}{3}{section.1.1}}
\citation{lilliefors1967kolmogorov}
\citation{clevert2015fast}
\citation{grobelny2007fase,wang2009simulation,wang2013towards}
\citation{snavely2002framework,bailey2005performance,barker2009using,ye2010analyzing}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}An Initial Application of Approximation}{5}{section.1.2}}
\citation{bailey2005performance}
\citation{beckman2008benchmarking,de2007identifying}
\citation{lofstead2010managing}
\citation{friedman1991multivariate}
\citation{stanford1993fast}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Algorithms for Constructing Approximations}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:algs}{{2}{7}{Algorithms for Constructing Approximations}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Multivariate Regression}{7}{section.2.1}}
\newlabel{sec:regression}{{2.1}{7}{Multivariate Regression}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Multivariate Adaptive Regression Splines}{7}{subsection.2.1.1}}
\newlabel{sec:mars}{{2.1.1}{7}{Multivariate Adaptive Regression Splines}{subsection.2.1.1}{}}
\citation{rudy2017pyearth}
\citation{basak2007support}
\citation{scikit-learn}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Support Vector Regressor}{8}{subsection.2.1.2}}
\newlabel{sec:svr}{{2.1.2}{8}{Support Vector Regressor}{subsection.2.1.2}{}}
\citation{hornik1989multilayer,rumelhart1988learning}
\citation{dahl2013improving}
\citation{goh2017why,moller1993scaled,robbins1951stochastic}
\citation{tensorflow2015-whitepaper,chollet2015keras}
\citation{lee1980two}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Multilayer Perceptron Regressor}{9}{subsection.2.1.3}}
\newlabel{sec:mlp}{{2.1.3}{9}{Multilayer Perceptron Regressor}{subsection.2.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Multivariate Interpolation}{9}{section.2.2}}
\newlabel{sec:interpolation}{{2.2}{9}{Multivariate Interpolation}{section.2.2}{}}
\citation{chang2018polynomial}
\citation{cover1967nearest}
\citation{gordon1978shepard,shepard1968two}
\citation{thacker2010algorithm}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Delaunay}{10}{subsection.2.2.1}}
\newlabel{sec:delaunay}{{2.2.1}{10}{Delaunay}{subsection.2.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Modified Shepard}{10}{subsection.2.2.2}}
\newlabel{sec:modified-shepard}{{2.2.2}{10}{Modified Shepard}{subsection.2.2.2}{}}
\citation{thacker2010algorithm}
\citation{thacker2010algorithm}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Linear Shepard}{11}{subsection.2.2.3}}
\citation{iozone}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Na\"{\i }ve Approximations of Variability}{12}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:naive}{{3}{12}{Na\"{\i }ve Approximations of Variability}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}I/O Data}{12}{section.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces A description of the system parameters being considered in the IOzone tests. Record size must not be greater than file size and hence there are only six valid combinations of the two. In total there are $6 \times 9 \times 16 = 864$ unique system configurations.\relax }}{13}{table.caption.4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:data_type}{{3.1}{13}{A description of the system parameters being considered in the IOzone tests. Record size must not be greater than file size and hence there are only six valid combinations of the two. In total there are $6 \times 9 \times 16 = 864$ unique system configurations.\relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Dimensional Analysis}{13}{subsection.3.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{Multidimensional Analysis}{13}{section*.6}}
\citation{amos2014algorithm}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Histograms of 100-bin reductions of the PMF of I/O throughput mean (top) and I/O throughput variance (bottom). In the mean plot, the first 1\% bin (truncated in plot) has a probability mass of .45. In the variance plot, the second 1\% bin has a probability mass of .58. It can be seen that the distributions of throughputs are primarily of lower magnitude with occasional extreme outliers.\relax }}{14}{figure.caption.5}}
\newlabel{fig:raw_throughput}{{3.1}{14}{Histograms of 100-bin reductions of the PMF of I/O throughput mean (top) and I/O throughput variance (bottom). In the mean plot, the first 1\% bin (truncated in plot) has a probability mass of .45. In the variance plot, the second 1\% bin has a probability mass of .58. It can be seen that the distributions of throughputs are primarily of lower magnitude with occasional extreme outliers.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Na\"{\i }ve Variability Modeling Results}{16}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}I/O Throughput Mean}{16}{subsection.3.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces These box plots show the prediction error of mean with increasing dimension. The top box whisker for SVR is 40, 80, 90 for dimensions 2, 3, and 4, respectively. Notice that each model consistently experiences greater magnitude error with increasing dimension. Results for all training percentages are aggregated.\relax }}{17}{figure.caption.7}}
\newlabel{fig:mean_dim}{{3.2}{17}{These box plots show the prediction error of mean with increasing dimension. The top box whisker for SVR is 40, 80, 90 for dimensions 2, 3, and 4, respectively. Notice that each model consistently experiences greater magnitude error with increasing dimension. Results for all training percentages are aggregated.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}I/O Throughput Variance}{17}{subsection.3.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces These box plots show the prediction error of mean with increasing amounts of training data provided to the models. Notice that MARS is the only model whose primary spread of performance increases with more training data. Recall that the response values being predicted span three orders of magnitude and hence relative errors should certainly remain within that range. For SVR the top box whisker goes from around 100 to 50 from left to right and is truncated in order to maintain focus on better models. Results for all dimensions are aggregated. Max training percentage is 96\% due to rounding training set size.\relax }}{18}{figure.caption.8}}
\newlabel{fig:mean_tt_ratio}{{3.3}{18}{These box plots show the prediction error of mean with increasing amounts of training data provided to the models. Notice that MARS is the only model whose primary spread of performance increases with more training data. Recall that the response values being predicted span three orders of magnitude and hence relative errors should certainly remain within that range. For SVR the top box whisker goes from around 100 to 50 from left to right and is truncated in order to maintain focus on better models. Results for all dimensions are aggregated. Max training percentage is 96\% due to rounding training set size.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces These box plots show the prediction error of variance with increasing amounts of training data provided to the models. The response values being predicted span six orders of magnitude. For SVR the top box whisker goes from around 6000 to 400 (decreasing by factors of 2) from left to right and is truncated in order to maintain focus on better models. Results for all dimensions are aggregated. Max training percentage is 96\% due to rounding training set size.\relax }}{19}{figure.caption.9}}
\newlabel{fig:var_tt_ratio}{{3.4}{19}{These box plots show the prediction error of variance with increasing amounts of training data provided to the models. The response values being predicted span six orders of magnitude. For SVR the top box whisker goes from around 6000 to 400 (decreasing by factors of 2) from left to right and is truncated in order to maintain focus on better models. Results for all dimensions are aggregated. Max training percentage is 96\% due to rounding training set size.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Increasing Dimension and Decreasing Training Data}{19}{subsection.3.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Discussion of Na\"{\i }ve Approximations}{20}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Modeling the System}{20}{subsection.3.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Extending the Analysis}{21}{subsection.3.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Takeaway From Na\"{\i }ve Approximation}{21}{section.3.4}}
\newlabel{sec:conclusion}{{3.4}{21}{Takeaway From Na\"{\i }ve Approximation}{section.3.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Box Splines: Uses, Constructions, and Applications}{22}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:boxes}{{4}{22}{Box Splines: Uses, Constructions, and Applications}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Box Splines}{22}{section.4.1}}
\newlabel{sec_box_splines}{{4.1}{22}{Box Splines}{section.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces 1D linear (order 2) and quadratic (order 3) box splines with direction vector sets $\mathopen {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1 \ 1 \mathclose {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$ and $\mathopen {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1 \ 1 \ 1 \mathclose {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$ respectively. Notice that these direction vector sets form the B-Spline analogues, order 2 composed of two linear components and order 3 composed of 3 quadratic components (colored and styled in plot).\relax }}{23}{figure.caption.10}}
\newlabel{fig_1D_boxes}{{4.1}{23}{1D linear (order 2) and quadratic (order 3) box splines with direction vector sets $\bigl ( 1 \ 1 \bigr )$ and $\bigl ( 1 \ 1 \ 1 \bigr )$ respectively. Notice that these direction vector sets form the B-Spline analogues, order 2 composed of two linear components and order 3 composed of 3 quadratic components (colored and styled in plot).\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces 2D linear (order 2) and quadratic (order 3) box splines with direction vector sets $\mathopen {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } I \ I \mathclose {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$ and $\mathopen {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } I \ I \ I \mathclose {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$ respectively, where $I$ is the identity matrix in two dimensions. Notice that these direction vector sets also produce boxes with $\text  {order}^2$ subregions (colored in plot).\relax }}{23}{figure.caption.11}}
\newlabel{fig_2D_boxes}{{4.2}{23}{2D linear (order 2) and quadratic (order 3) box splines with direction vector sets $\bigl ( I \ I \bigr )$ and $\bigl ( I \ I \ I \bigr )$ respectively, where $I$ is the identity matrix in two dimensions. Notice that these direction vector sets also produce boxes with $\text {order}^2$ subregions (colored in plot).\relax }{figure.caption.11}{}}
\citation{de2013box}
\newlabel{eq_box_base}{{4.1}{24}{Box Splines}{equation.4.1.1}{}}
\newlabel{eq_box_recursive}{{4.2}{24}{Box Splines}{equation.4.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces An example box in two dimensions with anchor $c$, upper widths $u^c_1$, $u^c_2$, and lower widths $l^c_1$, $l^c_2$. Notice that $c$ is not required to be equidistant from opposing sides of the box, that is $u^c_i \not = l^c_i$ is allowed.\relax }}{25}{figure.caption.12}}
\newlabel{fig_example_box}{{4.3}{25}{An example box in two dimensions with anchor $c$, upper widths $u^c_1$, $u^c_2$, and lower widths $l^c_1$, $l^c_2$. Notice that $c$ is not required to be equidistant from opposing sides of the box, that is $u^c_i \not = l^c_i$ is allowed.\relax }{figure.caption.12}{}}
\citation{amos2014algorithm}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Max Box Mesh}{26}{section.4.2}}
\newlabel{step_init}{{1}{26}{Max Box Mesh}{Item.6}{}}
\newlabel{step_closest}{{2}{26}{Max Box Mesh}{Item.7}{}}
\newlabel{step_shrink}{{3}{26}{Max Box Mesh}{Item.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Iterative Box Mesh}{28}{section.4.3}}
\citation{cover1967nearest}
\citation{dirichlet1850reduction}
\newlabel{step_add_box}{{2}{29}{Iterative Box Mesh}{Item.13}{}}
\citation{dutour2009complexity}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Voronoi Mesh}{30}{section.4.4}}
\citation{iozone}
\newlabel{step_add_control}{{2}{31}{Voronoi Mesh}{Item.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Data and Analysis}{31}{section.4.5}}
\citation{cortez2007data}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Histograms of Parkinsons (total UPDRS), forest fire (area), and HPC I/O (mean throughput) response values respectively. Notice that both the forest fire and HPC I/O data sets are heavily skewed. \vspace  {-.5cm}\relax }}{32}{figure.caption.13}}
\newlabel{fig_response_hists}{{4.4}{32}{Histograms of Parkinsons (total UPDRS), forest fire (area), and HPC I/O (mean throughput) response values respectively. Notice that both the forest fire and HPC I/O data sets are heavily skewed. \vspace {-.5cm}\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}High Performance Computing I/O ($n = 532, d = 4$)}{32}{subsection.4.5.1}}
\citation{tsanas2010accurate}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Time required to generate model fits for each technique with varying relative error tolerance during bootstrapping. \vspace  {-.3cm}\relax }}{33}{figure.caption.14}}
\newlabel{fig_eval_times}{{4.5}{33}{Time required to generate model fits for each technique with varying relative error tolerance during bootstrapping. \vspace {-.3cm}\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Forest Fire ($n = 517, d = 12$)}{33}{subsection.4.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Parkinson's Telemonitoring ($n = 468, d = 16$)}{33}{subsection.4.5.3}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces The optimal error tolerance bootstrapping parameters for each technique and each data set as well as the average absolute relative errors achieved by that tolerance. Notice that large relative error tolerances occasionally yield even lower evaluation errors, demonstrating the benefits of approximation over interpolation for noisy data sets. \vspace  {-.5cm}\relax }}{34}{table.caption.15}}
\newlabel{tab_optimal_tolerance}{{4.1}{34}{The optimal error tolerance bootstrapping parameters for each technique and each data set as well as the average absolute relative errors achieved by that tolerance. Notice that large relative error tolerances occasionally yield even lower evaluation errors, demonstrating the benefits of approximation over interpolation for noisy data sets. \vspace {-.5cm}\relax }{table.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.4}Performance Analysis}{34}{subsection.4.5.4}}
\newlabel{sec_performance_analysis}{{4.5.4}{34}{Performance Analysis}{subsection.4.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces The performance of all three techniques with varied relative error tolerance for the bootstrapping parameter. The columns are for Max Box Mesh, Iterative Box Mesh, and Voronoi Mesh, respectively. The rows are for HPC I/O, Forest Fire, and Parkinson's respectively. Notice the techniques' behavior on the Parkinson's and Forest Fire data sets, performance increases with larger error tolerance.\relax }}{35}{figure.caption.16}}
\newlabel{fig_all_performance}{{4.6}{35}{The performance of all three techniques with varied relative error tolerance for the bootstrapping parameter. The columns are for Max Box Mesh, Iterative Box Mesh, and Voronoi Mesh, respectively. The rows are for HPC I/O, Forest Fire, and Parkinson's respectively. Notice the techniques' behavior on the Parkinson's and Forest Fire data sets, performance increases with larger error tolerance.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces A sample of relative errors for all three techniques with optimal selections of error tolerance. The columns are for Max Box Mesh, Iterative Box Mesh, and Voronoi Mesh, respectively. The rows are for HPC I/O, Forest Fire, and Parkinson's respectively. \vspace  {-.3cm}\relax }}{36}{figure.caption.17}}
\newlabel{fig_perf_sample}{{4.7}{36}{A sample of relative errors for all three techniques with optimal selections of error tolerance. The columns are for Max Box Mesh, Iterative Box Mesh, and Voronoi Mesh, respectively. The rows are for HPC I/O, Forest Fire, and Parkinson's respectively. \vspace {-.3cm}\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Discussion of Mesh Approximations}{37}{section.4.6}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Implications of Quasi-Mesh Results}{38}{section.4.7}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Stronger Approximations of Variability}{39}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:strong}{{5}{39}{Stronger Approximations of Variability}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Measuring Error}{39}{section.5.1}}
\newlabel{sec:error}{{5.1}{39}{Measuring Error}{section.5.1}{}}
\citation{lilliefors1967kolmogorov}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces In this HPC I/O example, the general methodology for predicting a CDF and evaluating error can be seen. The Delaunay method chose three source distributions (dotted lines) and assigned weights \{.3, .4, .3\} (top to bottom at arrow). The weighted sum of the three known CDFs produces the predicted CDF (dashed line). The KS Statistic (arrow) computed between the true CDF (solid line) and predicted CDF (dashed line) is 0.2 for this example. The KS test null hypothesis is rejected at $p$-value 0.01, however it is not rejected at $p$-value 0.001.\relax }}{40}{figure.caption.18}}
\newlabel{fig:prediction-example}{{5.1}{40}{In this HPC I/O example, the general methodology for predicting a CDF and evaluating error can be seen. The Delaunay method chose three source distributions (dotted lines) and assigned weights \{.3, .4, .3\} (top to bottom at arrow). The weighted sum of the three known CDFs produces the predicted CDF (dashed line). The KS Statistic (arrow) computed between the true CDF (solid line) and predicted CDF (dashed line) is 0.2 for this example. The KS test null hypothesis is rejected at $p$-value 0.01, however it is not rejected at $p$-value 0.001.\relax }{figure.caption.18}{}}
\citation{guyon2003introduction}
\citation{pudil1994floating}
\citation{ferri1994comparative}
\citation{iozone}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces A description of system parameters considered for IOzone. Record size must be $\leq $ file size during execution.\relax }}{41}{table.caption.19}}
\newlabel{tab:data_description}{{5.1}{41}{A description of system parameters considered for IOzone. Record size must be $\leq $ file size during execution.\relax }{table.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Feature Weighting}{41}{subsection.5.1.1}}
\newlabel{sec:feature_weighting}{{5.1.1}{41}{Feature Weighting}{subsection.5.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Histogram of the raw throughput values recorded during all IOzone tests across all system configurations. The distribution is skewed right, with few tests having significantly higher throughput than most others.\relax }}{42}{figure.caption.20}}
\newlabel{fig:throughput_histogram}{{5.2}{42}{Histogram of the raw throughput values recorded during all IOzone tests across all system configurations. The distribution is skewed right, with few tests having significantly higher throughput than most others.\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Variability Data}{42}{section.5.2}}
\newlabel{sec:data}{{5.2}{42}{Variability Data}{section.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Distribution Prediction Results}{43}{section.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Histograms of the prediction error for each modeling algorithm from ten random splits when trained with 80\% of the data aggregated over all different test types. The distributions show the KS statistics for the predicted throughput distribution versus the actual throughput distribution. The four vertical red lines represent commonly used $p$-values \{0.05, 0.01, 0.001, 1.0e-6\} respectively. All predictions to the right of a red line represent CDF predictions that are significantly different (by respective $p$-value) from the actual distribution according to the KS test.\relax }}{44}{figure.caption.21}}
\newlabel{fig:ks_histogram_80_20}{{5.3}{44}{Histograms of the prediction error for each modeling algorithm from ten random splits when trained with 80\% of the data aggregated over all different test types. The distributions show the KS statistics for the predicted throughput distribution versus the actual throughput distribution. The four vertical red lines represent commonly used $p$-values \{0.05, 0.01, 0.001, 1.0e-6\} respectively. All predictions to the right of a red line represent CDF predictions that are significantly different (by respective $p$-value) from the actual distribution according to the KS test.\relax }{figure.caption.21}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Percent of null hypothesis rejections rate by the KS-test when provided different selections of $p$-values. These accompany the percent of null hypothesis rejection results from Figure \ref  {fig:ks_histogram_80_20}.\relax }}{45}{table.caption.22}}
\newlabel{tab:p_value_failure_rate}{{5.2}{45}{Percent of null hypothesis rejections rate by the KS-test when provided different selections of $p$-values. These accompany the percent of null hypothesis rejection results from Figure \ref {fig:ks_histogram_80_20}.\relax }{table.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces The performance of each algorithm on the KS test ($p=0.001$) with increasing amounts of training data averaged over all IOzone test types and ten random splits of the data. The training percentages range from 5\% to 95\% in increments of 5\%. Delaunay is the best performer until 95\% of data is used for training, at which Max Box mesh becomes the best performer by a fraction of a percent.\relax }}{46}{figure.caption.23}}
\newlabel{fig:ks_failure_by_training}{{5.4}{46}{The performance of each algorithm on the KS test ($p=0.001$) with increasing amounts of training data averaged over all IOzone test types and ten random splits of the data. The training percentages range from 5\% to 95\% in increments of 5\%. Delaunay is the best performer until 95\% of data is used for training, at which Max Box mesh becomes the best performer by a fraction of a percent.\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Discussion of Distribution Prediction}{47}{section.5.4}}
\newlabel{sec:discussion}{{5.4}{47}{Discussion of Distribution Prediction}{section.5.4}{}}
\citation{patel2009service}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces The null hypothesis rejection rates for various $p$-values with the KS-test. These results are strictly for the ``readers'' IOzone test type and show unweighted results as well as the results with weights tuned for minimum error (KS statistic) by 300 iterations of simulated annealing. Notice that the weights identified for the Delaunay model cause data dependent tuning, reducing performance. MaxBoxMesh performance is improved by a negligible amount. VoronoiMesh performance is notably improved.\relax }}{48}{table.caption.24}}
\newlabel{tab:optimized_p_value_failure_rate}{{5.3}{48}{The null hypothesis rejection rates for various $p$-values with the KS-test. These results are strictly for the ``readers'' IOzone test type and show unweighted results as well as the results with weights tuned for minimum error (KS statistic) by 300 iterations of simulated annealing. Notice that the weights identified for the Delaunay model cause data dependent tuning, reducing performance. MaxBoxMesh performance is improved by a negligible amount. VoronoiMesh performance is notably improved.\relax }{table.caption.24}{}}
\citation{ali2015security}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces The percentage of null hypothesis rejections for predictions made by each algorithm on the KS test ($p=0.001$) over different IOzone test types with increasing amounts of training data. Each percentage of null hypothesis rejections is an average over ten random splits of the data. The training percentages range from 5\% to 95\% in increments of 5\%. The read test types tend to allow lower rejection rates than the write test types.\relax }}{50}{figure.caption.25}}
\newlabel{fig:ks_failure_by_training_and_test}{{5.5}{50}{The percentage of null hypothesis rejections for predictions made by each algorithm on the KS test ($p=0.001$) over different IOzone test types with increasing amounts of training data. Each percentage of null hypothesis rejections is an average over ten random splits of the data. The training percentages range from 5\% to 95\% in increments of 5\%. The read test types tend to allow lower rejection rates than the write test types.\relax }{figure.caption.25}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}The Power of Distribution Prediction}{51}{section.5.5}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}An Error Bound on Piecewise Linear Interpolation}{52}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:error}{{6}{52}{An Error Bound on Piecewise Linear Interpolation}{chapter.6}{}}
\newlabel{lemma:1}{{6.1}{52}{}{proposition.6.1}{}}
\newlabel{lemma:2}{{6.2}{53}{}{proposition.6.2}{}}
\newlabel{lemma:3}{{6.3}{54}{}{proposition.6.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Data and Empirical Analysis}{55}{section.6.1}}
\newlabel{sec:error_data}{{6.1}{55}{Data and Empirical Analysis}{section.6.1}{}}
\citation{kohavi1995study}
\citation{kohavi1995study}
\citation{bengio2004no}
\citation{cortez2007data}
\citation{tsanas2010accurate}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Histogram of forest fire area burned under recorded weather conditions. The data is presented on a $\qopname  \relax o{ln}$ scale because most values are small with exponentially fewer fires on record that burn large areas.\relax }}{57}{figure.caption.26}}
\newlabel{fig:hist-forest-fire}{{6.1}{57}{Histogram of forest fire area burned under recorded weather conditions. The data is presented on a $\ln $ scale because most values are small with exponentially fewer fires on record that burn large areas.\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Forest Fire ($n = 504$, $d = 12$)}{57}{subsection.6.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Parkinson's Telemonitoring ($n = 5875$, $d = 19$)}{57}{subsection.6.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces All models are applied to approximate the amount of area that would be burned given environment conditions. $10$-fold cross validation as described in the beginning of Section \ref  {sec:error_data} is used to evaluated each algorithm. This results in exactly one prediction from each algorithm for each data point. These boxes depict the median (middle bar), median $95\%$ confidence interval (cones), quartiles (box edges), fences at $3/2$ interquartile range (whiskers), and outliers (dots) of absolute prediction error for each model. Similar to Figure \ref  {fig:hist-forest-fire}, the errors are presented on a $\qopname  \relax o{ln}$ scale. The numerical data corresponding to this figure is provided in Table \ref  {table:error-forest-fire} in the Appendix.\relax }}{58}{figure.caption.27}}
\newlabel{fig:error-forest-fire}{{6.2}{58}{All models are applied to approximate the amount of area that would be burned given environment conditions. $10$-fold cross validation as described in the beginning of Section \ref {sec:error_data} is used to evaluated each algorithm. This results in exactly one prediction from each algorithm for each data point. These boxes depict the median (middle bar), median $95\%$ confidence interval (cones), quartiles (box edges), fences at $3/2$ interquartile range (whiskers), and outliers (dots) of absolute prediction error for each model. Similar to Figure \ref {fig:hist-forest-fire}, the errors are presented on a $\ln $ scale. The numerical data corresponding to this figure is provided in Table \ref {table:error-forest-fire} in the Appendix.\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Histogram of the Parkinson's patient total UPDRS clinical scores that will be approximated by each algorithm.\relax }}{58}{figure.caption.28}}
\newlabel{fig:hist-parkinsons}{{6.3}{58}{Histogram of the Parkinson's patient total UPDRS clinical scores that will be approximated by each algorithm.\relax }{figure.caption.28}{}}
\citation{williams2009rattle}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces All models are applied to approximate the total UPDRS score given audio features from patients' home life, using $10$-fold cross validation. These boxes depict the median (middle bar), median $95\%$ confidence interval (cones), quartiles (box edges), fences at $3/2$ interquartile range (whiskers), and outliers (dots) of absolute prediction error for each model. The numerical data corresponding to this figure is provided in Table \ref  {table:error-parkinsons} in the Appendix.\relax }}{59}{figure.caption.29}}
\newlabel{fig:error-parkinsons}{{6.4}{59}{All models are applied to approximate the total UPDRS score given audio features from patients' home life, using $10$-fold cross validation. These boxes depict the median (middle bar), median $95\%$ confidence interval (cones), quartiles (box edges), fences at $3/2$ interquartile range (whiskers), and outliers (dots) of absolute prediction error for each model. The numerical data corresponding to this figure is provided in Table \ref {table:error-parkinsons} in the Appendix.\relax }{figure.caption.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Australian Daily Rainfall Volume ($n = 2609$, $d = 23$)}{59}{subsection.6.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Histogram of daily rainfall in Sydney, Australia, presented on a $\qopname  \relax o{ln}$ scale because the frequency of larger amounts of rainfall is significantly less. There is a peak in occurrence of the value $0$, which has a notable effect on the resulting model performance.\relax }}{60}{figure.caption.30}}
\newlabel{fig:hist-weather}{{6.5}{60}{Histogram of daily rainfall in Sydney, Australia, presented on a $\ln $ scale because the frequency of larger amounts of rainfall is significantly less. There is a peak in occurrence of the value $0$, which has a notable effect on the resulting model performance.\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces All models are applied to approximate the amount of rainfall expected on the next calendar day given various sources of local meteorological data, using $10$-fold cross validation. These boxes depict the median (middle bar), median $95\%$ confidence interval (cones), quartiles (box edges), fences at $3/2$ interquartile range (whiskers), and outliers (dots) of absolute prediction error for each model. The errors are presented on a $\qopname  \relax o{ln}$ scale, mimicking the presentation in Figure \ref  {fig:hist-weather}. The numerical data corresponding to this figure is provided in Table \ref  {table:error-weather} in the Appendix.\relax }}{60}{figure.caption.31}}
\newlabel{fig:error-weather}{{6.6}{60}{All models are applied to approximate the amount of rainfall expected on the next calendar day given various sources of local meteorological data, using $10$-fold cross validation. These boxes depict the median (middle bar), median $95\%$ confidence interval (cones), quartiles (box edges), fences at $3/2$ interquartile range (whiskers), and outliers (dots) of absolute prediction error for each model. The errors are presented on a $\ln $ scale, mimicking the presentation in Figure \ref {fig:hist-weather}. The numerical data corresponding to this figure is provided in Table \ref {table:error-weather} in the Appendix.\relax }{figure.caption.31}{}}
\citation{pozzolo2015calibrating}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Histogram of credit card transaction amounts, presented on a $\qopname  \relax o{ln}$ scale. The data contains a notable frequency peak around $\$1$ transactions. Fewer large purchases are made, but some large purchases are in excess of five orders of magnitude greater than the smallest purchases.\relax }}{61}{figure.caption.32}}
\newlabel{fig:hist-credit-card}{{6.7}{61}{Histogram of credit card transaction amounts, presented on a $\ln $ scale. The data contains a notable frequency peak around $\$1$ transactions. Fewer large purchases are made, but some large purchases are in excess of five orders of magnitude greater than the smallest purchases.\relax }{figure.caption.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.4}Credit Card Transaction Amount ($n = 5562$, $d = 28$)}{61}{subsection.6.1.4}}
\citation{cameron2019moana}
\citation{iozone}
\citation{fritsch1980monotone}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces All models are applied to approximate the expected transaction amount given transformed (and obfuscated) vendor and customer-descriptive features, using $10$-fold cross validation. These boxes depict the median (middle bar), median $95\%$ confidence interval (cones), quartiles (box edges), fences at $3/2$ interquartile range (whiskers), and outliers (dots) of absolute prediction error for each model. The absolute errors in transaction amount predictions are presented on a $\qopname  \relax o{ln}$ scale, just as in Figure \ref  {fig:hist-credit-card}. The numerical data corresponding to this figure is provided in Table \ref  {table:error-credit-card} in the Appendix.\relax }}{62}{figure.caption.33}}
\newlabel{fig:error-credit-card}{{6.8}{62}{All models are applied to approximate the expected transaction amount given transformed (and obfuscated) vendor and customer-descriptive features, using $10$-fold cross validation. These boxes depict the median (middle bar), median $95\%$ confidence interval (cones), quartiles (box edges), fences at $3/2$ interquartile range (whiskers), and outliers (dots) of absolute prediction error for each model. The absolute errors in transaction amount predictions are presented on a $\ln $ scale, just as in Figure \ref {fig:hist-credit-card}. The numerical data corresponding to this figure is provided in Table \ref {table:error-credit-card} in the Appendix.\relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.5}High Performance Computing I/O ($n = 3016$, $d = 4$)}{62}{subsection.6.1.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Histogram of the raw throughput values recorded during all IOzone tests across all system configurations. The distribution is skewed right, with few tests having significantly higher throughput than most others. The data is presented on a $\qopname  \relax o{ln}$ scale.\relax }}{63}{figure.caption.34}}
\newlabel{fig:hist-throughput}{{6.9}{63}{Histogram of the raw throughput values recorded during all IOzone tests across all system configurations. The distribution is skewed right, with few tests having significantly higher throughput than most others. The data is presented on a $\ln $ scale.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces The models directly capable of predicting distributions are applied to predicting the expected CDF of I/O throughput at a previously unseen system configuration, using $10$-fold cross validation. The KS statistic between the observed distribution at each system configuration and the predicted distribution is recorded and presented above. Note that the above figure is \textit  {not} log-scaled like Figure \ref  {fig:hist-throughput}. The numerical data corresponding to this figure is provided in Table \ref  {table:error-throughput} in the Appendix.\relax }}{63}{figure.caption.35}}
\newlabel{fig:error-throughput}{{6.10}{63}{The models directly capable of predicting distributions are applied to predicting the expected CDF of I/O throughput at a previously unseen system configuration, using $10$-fold cross validation. The KS statistic between the observed distribution at each system configuration and the predicted distribution is recorded and presented above. Note that the above figure is \textit {not} log-scaled like Figure \ref {fig:hist-throughput}. The numerical data corresponding to this figure is provided in Table \ref {table:error-throughput} in the Appendix.\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces Histograms of the prediction error for each interpolant that produces predictions as convex combinations of observed data, using $10$-fold cross validation. The histograms show the KS statistics for the predicted throughput distribution versus the actual throughput distribution. The four vertical lines represent cutoff KS statistics given $150$ samples for commonly used $p$-values 0.05, 0.01, 0.001, $10^{-6}$, respectively. All predictions to the right of a vertical line represent CDF predictions that are significantly different (by respective $p$-value) from the actual distribution according to the KS test. The numerical counterpart to this figure is presented in Table \ref  {table:null-hypothesis-results}.\relax }}{64}{figure.caption.36}}
\newlabel{fig:throughput-prediction}{{6.11}{64}{Histograms of the prediction error for each interpolant that produces predictions as convex combinations of observed data, using $10$-fold cross validation. The histograms show the KS statistics for the predicted throughput distribution versus the actual throughput distribution. The four vertical lines represent cutoff KS statistics given $150$ samples for commonly used $p$-values 0.05, 0.01, 0.001, $10^{-6}$, respectively. All predictions to the right of a vertical line represent CDF predictions that are significantly different (by respective $p$-value) from the actual distribution according to the KS test. The numerical counterpart to this figure is presented in Table \ref {table:null-hypothesis-results}.\relax }{figure.caption.36}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Numerical counterpart of the histogram data presented in Figure \ref  {fig:throughput-prediction}. The columns display the percent of null hypothesis rejections by the KS-test when provided different selections of $p$-values for each algorithm. The algorithm with the lowest rejection rate at each $p$ is boldface, while the second lowest is italicized.\relax }}{65}{table.caption.37}}
\newlabel{table:null-hypothesis-results}{{6.1}{65}{Numerical counterpart of the histogram data presented in Figure \ref {fig:throughput-prediction}. The columns display the percent of null hypothesis rejections by the KS-test when provided different selections of $p$-values for each algorithm. The algorithm with the lowest rejection rate at each $p$ is boldface, while the second lowest is italicized.\relax }{table.caption.37}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Discussion of Empirical Results}{65}{section.6.2}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces This average of Appendix Tables \ref  {table:best-forest-fire}, \ref  {table:best-parkinsons}, \ref  {table:best-weather}, and \ref  {table:best-credit-card} provides a gross summary of overall results. The columns display (weighted equally by data set, \textit  {not} points) the average frequency with which any algorithm provided the lowest absolute error approximation, the average time to fit/prepare, and the average time required to approximate one point. The times have been rounded to one significant digit, as reasonably large fluctuations may be observed due to implementation hardware. Interpolants provide the lowest error approximation for nearly one third of all data, while regressors occupy the other two thirds. This result is obtained without any customized tuning or preprocessing to maximize the performance of any given algorithm. In practice, tuning and preprocessing may have large effects on approximation performance.\relax }}{66}{table.caption.38}}
\newlabel{table:avg-performance}{{6.2}{66}{This average of Appendix Tables \ref {table:best-forest-fire}, \ref {table:best-parkinsons}, \ref {table:best-weather}, and \ref {table:best-credit-card} provides a gross summary of overall results. The columns display (weighted equally by data set, \textit {not} points) the average frequency with which any algorithm provided the lowest absolute error approximation, the average time to fit/prepare, and the average time required to approximate one point. The times have been rounded to one significant digit, as reasonably large fluctuations may be observed due to implementation hardware. Interpolants provide the lowest error approximation for nearly one third of all data, while regressors occupy the other two thirds. This result is obtained without any customized tuning or preprocessing to maximize the performance of any given algorithm. In practice, tuning and preprocessing may have large effects on approximation performance.\relax }{table.caption.38}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Takeaway from Empirical Tests}{67}{section.6.3}}
\citation{knott2012interpolating}
\citation{fritsch1980monotone,gregory1985shape}
\citation{ramsay1988monotone}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Improving Variability Estimates with Monotone $C^2$ Splines}{68}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:splines}{{7}{68}{Improving Variability Estimates with Monotone $C^2$ Splines}{chapter.7}{}}
\citation{fritsch1980monotone}
\citation{carlson1985monotone}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Related Work}{69}{section.7.1}}
\citation{ulrich1994positivity,hess1994positive}
\citation{fritsch1980monotone}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces These are the feasible regions of monotonicity for cubic splines.\relax }}{70}{figure.caption.39}}
\newlabel{fig:feasible_region}{{7.1}{70}{These are the feasible regions of monotonicity for cubic splines.\relax }{figure.caption.39}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Achieved Progress}{71}{section.7.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces A demonstration of projection onto the feasible region for cubic splines.\relax }}{71}{figure.caption.40}}
\newlabel{fig:demo_projection}{{7.2}{71}{A demonstration of projection onto the feasible region for cubic splines.\relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces A demonstration fit with a cubic spline.\relax }}{72}{figure.caption.41}}
\newlabel{fig:demo_fit}{{7.3}{72}{A demonstration fit with a cubic spline.\relax }{figure.caption.41}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Research Goal}{72}{section.7.3}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Timeline}{72}{section.7.4}}
\bibdata{prelim}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces The derivative of Figure \ref  {fig:demo_projection}.\relax }}{73}{figure.caption.42}}
\newlabel{fig:demo_fit_deriv}{{7.4}{73}{The derivative of Figure \ref {fig:demo_projection}.\relax }{figure.caption.42}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Final Remarks}{73}{section.7.5}}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces This table depicts my expected timeline going forward. For details on when I achieved previous milestones, please contact me and I can provide the list. I refrained from including it here for brevity.\relax }}{73}{table.caption.43}}
\newlabel{tab:timeline}{{7.1}{73}{This table depicts my expected timeline going forward. For details on when I achieved previous milestones, please contact me and I can provide the list. I refrained from including it here for brevity.\relax }{table.caption.43}{}}
\bibcite{tensorflow2015-whitepaper}{{1}{2015}{{Abadi et~al.}}{{Abadi, Agarwal, Barham, Brevdo, Chen, Citro, Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia, Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah, Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan, Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and Zheng}}}
\bibcite{ali2015security}{{2}{2015}{{Ali et~al.}}{{Ali, Khan, and Vasilakos}}}
\bibcite{amos2014algorithm}{{3}{2014}{{Amos et~al.}}{{Amos, Easterling, Watson, Thacker, Castle, and Trosset}}}
\bibcite{bailey2005performance}{{4}{2005}{{Bailey and Snavely}}{{}}}
\bibcite{barker2009using}{{5}{2009}{{Barker et~al.}}{{Barker, Davis, Hoisie, Kerbyson, Lang, Pakin, and Sancho}}}
\bibcite{basak2007support}{{6}{2007}{{Basak et~al.}}{{Basak, Pal, and Patranabis}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{74}{chapter*.44}}
\bibcite{beckman2008benchmarking}{{7}{2008}{{Beckman et~al.}}{{Beckman, Iskra, Yoshii, Coghlan, and Nataraj}}}
\bibcite{bengio2004no}{{8}{2004}{{Bengio and Grandvalet}}{{}}}
\bibcite{cameron2019moana}{{9}{2019}{{{Cameron} et~al.}}{{{Cameron}, {Anwar}, {Cheng}, {Xu}, {Li}, {Ananth}, {Bernard}, {Jearls}, {Lux}, {Hong}, {Watson}, and {Butt}}}}
\bibcite{carlson1985monotone}{{10}{1985}{{Carlson and Fritsch}}{{}}}
\bibcite{chang2018polynomial}{{11}{2018}{{Chang et~al.}}{{Chang, Watson, Lux, Li, Xu, Butt, Cameron, and Hong}}}
\bibcite{cheney2009course}{{12}{2009}{{Cheney and Light}}{{}}}
\bibcite{chollet2015keras}{{13}{2015}{{Chollet et~al.}}{{}}}
\bibcite{clevert2015fast}{{14}{2015}{{Clevert et~al.}}{{Clevert, Unterthiner, and Hochreiter}}}
\bibcite{cortez2007data}{{15}{2007}{{Cortez and Morais}}{{}}}
\bibcite{cortez2008using}{{16}{2008}{{Cortez and Silva}}{{}}}
\bibcite{cover1967nearest}{{17}{1967}{{Cover and Hart}}{{}}}
\bibcite{dahl2013improving}{{18}{2013}{{Dahl et~al.}}{{Dahl, Sainath, and Hinton}}}
\bibcite{de2007identifying}{{19}{2007}{{De et~al.}}{{De, Kothari, and Mann}}}
\bibcite{de1978practical}{{20}{1978}{{De~Boor et~al.}}{{De~Boor, De~Boor, Math{\'e}maticien, De~Boor, and De~Boor}}}
\bibcite{de2013box}{{21}{2013}{{De~Boor et~al.}}{{De~Boor, H{\"o}llig, and Riemenschneider}}}
\bibcite{de2008field}{{22}{2008}{{De~Vito et~al.}}{{De~Vito, Massera, Piga, Martinotto, and Di~Francia}}}
\bibcite{dennis1996numerical}{{23}{1996}{{Dennis~Jr and Schnabel}}{{}}}
\bibcite{dirichlet1850reduction}{{24}{1850}{{Dirichlet}}{{}}}
\bibcite{dutour2009complexity}{{25}{2009}{{Dutour~Sikiri{\'c} et~al.}}{{Dutour~Sikiri{\'c}, Sch{\"u}rmann, and Vallentin}}}
\bibcite{ferri1994comparative}{{26}{1994}{{Ferri et~al.}}{{Ferri, Pudil, Hatef, and Kittler}}}
\bibcite{friedman1991multivariate}{{27}{1991}{{Friedman}}{{}}}
\bibcite{stanford1993fast}{{28}{1993}{{Friedman and the Computational Statistics Laboritory~of Stanford~University}}{{}}}
\bibcite{fritsch1980monotone}{{29}{1980}{{Fritsch and Carlson}}{{}}}
\bibcite{goh2017why}{{30}{2017}{{Goh}}{{}}}
\bibcite{gordon1978shepard}{{31}{1978}{{Gordon and Wixom}}{{}}}
\bibcite{gregory1985shape}{{32}{1985}{{Gregory}}{{}}}
\bibcite{grobelny2007fase}{{33}{2007}{{Grobelny et~al.}}{{Grobelny, Bueno, Troxel, George, and Vetter}}}
\bibcite{guyon2003introduction}{{34}{2003}{{Guyon and Elisseeff}}{{}}}
\bibcite{hess1994positive}{{35}{1994}{{Hess and Schmidt}}{{}}}
\bibcite{hornik1989multilayer}{{36}{1989}{{Hornik et~al.}}{{Hornik, Stinchcombe, and White}}}
\bibcite{knott2012interpolating}{{37}{2012}{{Knott}}{{}}}
\bibcite{kohavi1995study}{{38}{1995}{{Kohavi et~al.}}{{}}}
\bibcite{lazos2014optimisation}{{39}{2014}{{Lazos et~al.}}{{Lazos, Sproul, and Kay}}}
\bibcite{lee1980two}{{40}{1980}{{Lee and Schachter}}{{}}}
\bibcite{lilliefors1967kolmogorov}{{41}{1967}{{Lilliefors}}{{}}}
\bibcite{lofstead2010managing}{{42}{2010}{{Lofstead et~al.}}{{Lofstead, Zheng, Liu, Klasky, Oldfield, Kordenbrock, Schwan, and Wolf}}}
\bibcite{lux2016applications}{{43}{2016}{{Lux et~al.}}{{Lux, Pittman, Shende, and Shende}}}
\bibcite{lux2018nonparametric}{{44}{2018{}}{{Lux et~al.}}{{Lux, Watson, Chang, Bernard, Li, Yu, Xu, Back, Butt, Cameron, et~al.}}}
\bibcite{lux2018novel}{{45}{2018{}}{{Lux et~al.}}{{Lux, Watson, Chang, Bernard, Li, Yu, Xu, Back, Butt, Cameron, et~al.}}}
\bibcite{moller1993scaled}{{46}{1993}{{M{\o }ller}}{{}}}
\bibcite{iozone}{{47}{2017}{{Norcott}}{{}}}
\bibcite{patel2009service}{{48}{2009}{{Patel et~al.}}{{Patel, Ranabahu, and Sheth}}}
\bibcite{scikit-learn}{{49}{2011}{{Pedregosa et~al.}}{{Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, and Duchesnay}}}
\bibcite{pozzolo2015calibrating}{{50}{2015}{{Pozzolo et~al.}}{{Pozzolo, Caelen, Johnson, and Bontempi}}}
\bibcite{pudil1994floating}{{51}{1994}{{Pudil et~al.}}{{Pudil, Novovi{\v {c}}ov{\'a}, and Kittler}}}
\bibcite{ramsay1988monotone}{{52}{1988}{{Ramsay et~al.}}{{}}}
\bibcite{robbins1951stochastic}{{53}{1951}{{Robbins and Monro}}{{}}}
\bibcite{rudy2017pyearth}{{54}{2017}{{Rudy and Cherti}}{{}}}
\bibcite{rumelhart1988learning}{{55}{1988}{{Rumelhart et~al.}}{{Rumelhart, Hinton, Williams, et~al.}}}
\bibcite{shepard1968two}{{56}{1968}{{Shepard}}{{}}}
\bibcite{snavely2002framework}{{57}{2002}{{Snavely et~al.}}{{Snavely, Carrington, Wolter, Labarta, Badia, and Purkayastha}}}
\bibcite{thacker2010algorithm}{{58}{2010}{{Thacker et~al.}}{{Thacker, Zhang, Watson, Birch, Iyer, and Berry}}}
\bibcite{tsanas2010accurate}{{59}{2010}{{Tsanas et~al.}}{{Tsanas, Little, McSharry, and Ramig}}}
\bibcite{ulrich1994positivity}{{60}{1994}{{Ulrich and Watson}}{{}}}
\bibcite{unther1996interpolating}{{61}{1996}{{unther Greiner and Hormann}}{{}}}
\bibcite{wang2009simulation}{{62}{2009}{{Wang et~al.}}{{Wang, Butt, Pandey, and Gupta}}}
\bibcite{wang2013towards}{{63}{2013}{{Wang et~al.}}{{Wang, Khasymski, Krish, and Butt}}}
\bibcite{williams2009rattle}{{64}{2009}{{Williams}}{{}}}
\bibcite{ye2010analyzing}{{65}{2010}{{Ye et~al.}}{{Ye, Jiang, Chen, Huang, and Wang}}}
\bibstyle{plainnat}
\@writefile{toc}{\contentsline {chapter}{Appendices}{83}{section*.45}}
\@writefile{toc}{\contentsline {chapter}{Appendix \numberline {A}Error Bound Appendices}{84}{Appendix.a.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:error}{{A}{84}{Error Bound Appendices}{Appendix.a.A}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces This numerical data accompanies the visual provided in Figure \ref  {fig:error-forest-fire}. The columns of absolute error percentiles correspond to the minimum, first quartile, median, third quartile, and maximum absolute errors respectively. The minimum of each column is boldface, while the second lowest value is italicized. All values are rounded to three significant digits.\relax }}{85}{table.caption.46}}
\newlabel{table:error-forest-fire}{{A.1}{85}{This numerical data accompanies the visual provided in Figure \ref {fig:error-forest-fire}. The columns of absolute error percentiles correspond to the minimum, first quartile, median, third quartile, and maximum absolute errors respectively. The minimum of each column is boldface, while the second lowest value is italicized. All values are rounded to three significant digits.\relax }{table.caption.46}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.2}{\ignorespaces The left above shows how often each algorithm had the lowest absolute error approximating forest fire data in Table \ref  {table:error-forest-fire}. On the right columns are median fit time of 454 points, median time for one approximation, and median time approximating 50 points.\relax }}{85}{table.caption.47}}
\newlabel{table:best-forest-fire}{{A.2}{85}{The left above shows how often each algorithm had the lowest absolute error approximating forest fire data in Table \ref {table:error-forest-fire}. On the right columns are median fit time of 454 points, median time for one approximation, and median time approximating 50 points.\relax }{table.caption.47}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.3}{\ignorespaces This numerical data accompanies the visual provided in Figure \ref  {fig:error-parkinsons}. The columns of absolute error percentiles correspond to the minimum, first quartile, median, third quartile, and maximum absolute errors respectively. The minimum of each column is boldface, while the second lowest value is italicized. All values are rounded to three significant digits.\relax }}{86}{table.caption.48}}
\newlabel{table:error-parkinsons}{{A.3}{86}{This numerical data accompanies the visual provided in Figure \ref {fig:error-parkinsons}. The columns of absolute error percentiles correspond to the minimum, first quartile, median, third quartile, and maximum absolute errors respectively. The minimum of each column is boldface, while the second lowest value is italicized. All values are rounded to three significant digits.\relax }{table.caption.48}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.4}{\ignorespaces The left above shows how often each algorithm had the lowest absolute error approximating Parkinson's data in Table \ref  {table:error-parkinsons}. On the right columns are median fit time of 5288 points, median time for one approximation, and median time approximating 587 points.\relax }}{86}{table.caption.49}}
\newlabel{table:best-parkinsons}{{A.4}{86}{The left above shows how often each algorithm had the lowest absolute error approximating Parkinson's data in Table \ref {table:error-parkinsons}. On the right columns are median fit time of 5288 points, median time for one approximation, and median time approximating 587 points.\relax }{table.caption.49}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.5}{\ignorespaces This numerical data accompanies the visual provided in Figure \ref  {fig:error-weather}. The columns of absolute error percentiles correspond to the minimum, first quartile, median, third quartile, and maximum absolute errors respectively. The minimum value of each column is boldface, while the second lowest is italicized. All values are rounded to three significant digits.\relax }}{86}{table.caption.50}}
\newlabel{table:error-weather}{{A.5}{86}{This numerical data accompanies the visual provided in Figure \ref {fig:error-weather}. The columns of absolute error percentiles correspond to the minimum, first quartile, median, third quartile, and maximum absolute errors respectively. The minimum value of each column is boldface, while the second lowest is italicized. All values are rounded to three significant digits.\relax }{table.caption.50}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.6}{\ignorespaces Left table shows how often each algorithm had the lowest absolute error approximating Sydney rainfall data in Table \ref  {table:error-weather}. On the right columns are median fit time of 2349 points, median time for one approximation, and median time approximating 260 points.\relax }}{87}{table.caption.51}}
\newlabel{table:best-weather}{{A.6}{87}{Left table shows how often each algorithm had the lowest absolute error approximating Sydney rainfall data in Table \ref {table:error-weather}. On the right columns are median fit time of 2349 points, median time for one approximation, and median time approximating 260 points.\relax }{table.caption.51}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.7}{\ignorespaces This numerical data accompanies the visual provided in Figure \ref  {fig:error-credit-card}. The columns of absolute error percentiles correspond to the minimum, first quartile, median, third quartile, and maximum absolute errors respectively. The minimum value of each column is boldface, while the second lowest is italicized. All values are rounded to three significant digits.\relax }}{87}{table.caption.52}}
\newlabel{table:error-credit-card}{{A.7}{87}{This numerical data accompanies the visual provided in Figure \ref {fig:error-credit-card}. The columns of absolute error percentiles correspond to the minimum, first quartile, median, third quartile, and maximum absolute errors respectively. The minimum value of each column is boldface, while the second lowest is italicized. All values are rounded to three significant digits.\relax }{table.caption.52}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.8}{\ignorespaces The left above shows how often each algorithm had the lowest absolute error approximating credit card transaction data in Table \ref  {table:error-credit-card}. On the right columns are median fit time of 5006 points, median time for one approximation, and median time approximating 556 points.\relax }}{87}{table.caption.53}}
\newlabel{table:best-credit-card}{{A.8}{87}{The left above shows how often each algorithm had the lowest absolute error approximating credit card transaction data in Table \ref {table:error-credit-card}. On the right columns are median fit time of 5006 points, median time for one approximation, and median time approximating 556 points.\relax }{table.caption.53}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.9}{\ignorespaces This numerical data accompanies the visual provided in Figure \ref  {fig:error-throughput}. The columns of absolute error percentiles correspond to the minimum, first quartile, median, third quartile, and maximum KS statistics respectively between truth and guess for models predicting the distribution of I/O throughput that will be observed at previously unseen system configurations. The minimum value of each column is boldface, while the second lowest is italicized. All values are rounded to three significant digits.\relax }}{88}{table.caption.54}}
\newlabel{table:error-throughput}{{A.9}{88}{This numerical data accompanies the visual provided in Figure \ref {fig:error-throughput}. The columns of absolute error percentiles correspond to the minimum, first quartile, median, third quartile, and maximum KS statistics respectively between truth and guess for models predicting the distribution of I/O throughput that will be observed at previously unseen system configurations. The minimum value of each column is boldface, while the second lowest is italicized. All values are rounded to three significant digits.\relax }{table.caption.54}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.10}{\ignorespaces The left above shows how often each algorithm had the lowest KS statistic on the I/O throughput distribution data in Table \ref  {table:error-throughput}. On the right columns are median fit time of 2715 points, median time for one approximation, and median time approximating 301 points.\relax }}{88}{table.caption.55}}
\newlabel{table:best-throughput}{{A.10}{88}{The left above shows how often each algorithm had the lowest KS statistic on the I/O throughput distribution data in Table \ref {table:error-throughput}. On the right columns are median fit time of 2715 points, median time for one approximation, and median time approximating 301 points.\relax }{table.caption.55}{}}
