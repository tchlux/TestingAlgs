\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Histograms of 100-bin reductions of the PMF of I/O throughput mean (top) and I/O throughput variance (bottom). In the mean plot, the first 1\% bin (truncated in plot) has a probability mass of .45. In the variance plot, the second 1\% bin has a probability mass of .58. It can be seen that the distributions of throughputs are primarily of lower magnitude with occasional extreme outliers.\relax }}{14}{figure.caption.5}
\contentsline {figure}{\numberline {3.2}{\ignorespaces These box plots show the prediction error of mean with increasing dimension. The top box whisker for SVR is 40, 80, 90 for dimensions 2, 3, and 4, respectively. Notice that each model consistently experiences greater magnitude error with increasing dimension. Results for all training percentages are aggregated.\relax }}{17}{figure.caption.7}
\contentsline {figure}{\numberline {3.3}{\ignorespaces These box plots show the prediction error of mean with increasing amounts of training data provided to the models. Notice that MARS is the only model whose primary spread of performance increases with more training data. Recall that the response values being predicted span three orders of magnitude and hence relative errors should certainly remain within that range. For SVR the top box whisker goes from around 100 to 50 from left to right and is truncated in order to maintain focus on better models. Results for all dimensions are aggregated. Max training percentage is 96\% due to rounding training set size.\relax }}{18}{figure.caption.8}
\contentsline {figure}{\numberline {3.4}{\ignorespaces These box plots show the prediction error of variance with increasing amounts of training data provided to the models. The response values being predicted span six orders of magnitude. For SVR the top box whisker goes from around 6000 to 400 (decreasing by factors of 2) from left to right and is truncated in order to maintain focus on better models. Results for all dimensions are aggregated. Max training percentage is 96\% due to rounding training set size.\relax }}{19}{figure.caption.9}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces 1D linear (order 2) and quadratic (order 3) box splines with direction vector sets $\mathopen {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1 \ 1 \mathclose {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$ and $\mathopen {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1 \ 1 \ 1 \mathclose {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$ respectively. Notice that these direction vector sets form the B-Spline analogues, order 2 composed of two linear components and order 3 composed of 3 quadratic components (colored and styled in plot).\relax }}{23}{figure.caption.10}
\contentsline {figure}{\numberline {4.2}{\ignorespaces 2D linear (order 2) and quadratic (order 3) box splines with direction vector sets $\mathopen {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } I \ I \mathclose {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$ and $\mathopen {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } I \ I \ I \mathclose {\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$ respectively, where $I$ is the identity matrix in two dimensions. Notice that these direction vector sets also produce boxes with $\text {order}^2$ subregions (colored in plot).\relax }}{23}{figure.caption.11}
\contentsline {figure}{\numberline {4.3}{\ignorespaces An example box in two dimensions with anchor $c$, upper widths $u^c_1$, $u^c_2$, and lower widths $l^c_1$, $l^c_2$. Notice that $c$ is not required to be equidistant from opposing sides of the box, that is $u^c_i \not = l^c_i$ is allowed.\relax }}{25}{figure.caption.12}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Histograms of Parkinsons (total UPDRS), forest fire (area), and HPC I/O (mean throughput) response values respectively. Notice that both the forest fire and HPC I/O data sets are heavily skewed. \vspace {-.5cm}\relax }}{32}{figure.caption.13}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Time required to generate model fits for each technique with varying relative error tolerance during bootstrapping. \vspace {-.3cm}\relax }}{33}{figure.caption.14}
\contentsline {figure}{\numberline {4.6}{\ignorespaces The performance of all three techniques with varied relative error tolerance for the bootstrapping parameter. The columns are for Max Box Mesh, Iterative Box Mesh, and Voronoi Mesh, respectively. The rows are for HPC I/O, Forest Fire, and Parkinson's respectively. Notice the techniques' behavior on the Parkinson's and Forest Fire data sets, performance increases with larger error tolerance.\relax }}{35}{figure.caption.16}
\contentsline {figure}{\numberline {4.7}{\ignorespaces A sample of relative errors for all three techniques with optimal selections of error tolerance. The columns are for Max Box Mesh, Iterative Box Mesh, and Voronoi Mesh, respectively. The rows are for HPC I/O, Forest Fire, and Parkinson's respectively. \vspace {-.3cm}\relax }}{36}{figure.caption.17}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces In this HPC I/O example, the general methodology for predicting a CDF and evaluating error can be seen. The Delaunay method chose three source distributions (dotted lines) and assigned weights \{.3, .4, .3\} (top to bottom at arrow). The weighted sum of the three known CDFs produces the predicted CDF (dashed line). The KS Statistic (arrow) computed between the true CDF (solid line) and predicted CDF (dashed line) is 0.2 for this example. The KS test null hypothesis is rejected at $p$-value 0.01, however it is not rejected at $p$-value 0.001.\relax }}{40}{figure.caption.18}
\contentsline {figure}{\numberline {5.2}{\ignorespaces Histogram of the raw throughput values recorded during all IOzone tests across all system configurations. The distribution is skewed right, with few tests having significantly higher throughput than most others.\relax }}{42}{figure.caption.20}
\contentsline {figure}{\numberline {5.3}{\ignorespaces Histograms of the prediction error for each modeling algorithm from ten random splits when trained with 80\% of the data aggregated over all different test types. The distributions show the KS statistics for the predicted throughput distribution versus the actual throughput distribution. The four vertical red lines represent commonly used $p$-values \{0.05, 0.01, 0.001, 1.0e-6\} respectively. All predictions to the right of a red line represent CDF predictions that are significantly different (by respective $p$-value) from the actual distribution according to the KS test.\relax }}{44}{figure.caption.21}
\contentsline {figure}{\numberline {5.4}{\ignorespaces The performance of each algorithm on the KS test ($p=0.001$) with increasing amounts of training data averaged over all IOzone test types and ten random splits of the data. The training percentages range from 5\% to 95\% in increments of 5\%. Delaunay is the best performer until 95\% of data is used for training, at which Max Box mesh becomes the best performer by a fraction of a percent.\relax }}{46}{figure.caption.23}
\contentsline {figure}{\numberline {5.5}{\ignorespaces The percentage of null hypothesis rejections for predictions made by each algorithm on the KS test ($p=0.001$) over different IOzone test types with increasing amounts of training data. Each percentage of null hypothesis rejections is an average over ten random splits of the data. The training percentages range from 5\% to 95\% in increments of 5\%. The read test types tend to allow lower rejection rates than the write test types.\relax }}{50}{figure.caption.25}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces Histogram of forest fire area burned under recorded weather conditions. The data is presented on a $\qopname \relax o{ln}$ scale because most values are small with exponentially fewer fires on record that burn large areas.\relax }}{57}{figure.caption.26}
\contentsline {figure}{\numberline {6.2}{\ignorespaces All models are applied to approximate the amount of area that would be burned given environment conditions. $10$-fold cross validation as described in the beginning of Section \ref {sec:error_data} is used to evaluated each algorithm. This results in exactly one prediction from each algorithm for each data point. These boxes depict the median (middle bar), median $95\%$ confidence interval (cones), quartiles (box edges), fences at $3/2$ interquartile range (whiskers), and outliers (dots) of absolute prediction error for each model. Similar to Figure \ref {fig:hist-forest-fire}, the errors are presented on a $\qopname \relax o{ln}$ scale. The numerical data corresponding to this figure is provided in Table \ref {table:error-forest-fire} in the Appendix.\relax }}{58}{figure.caption.27}
\contentsline {figure}{\numberline {6.3}{\ignorespaces Histogram of the Parkinson's patient total UPDRS clinical scores that will be approximated by each algorithm.\relax }}{58}{figure.caption.28}
\contentsline {figure}{\numberline {6.4}{\ignorespaces All models are applied to approximate the total UPDRS score given audio features from patients' home life, using $10$-fold cross validation. These boxes depict the median (middle bar), median $95\%$ confidence interval (cones), quartiles (box edges), fences at $3/2$ interquartile range (whiskers), and outliers (dots) of absolute prediction error for each model. The numerical data corresponding to this figure is provided in Table \ref {table:error-parkinsons} in the Appendix.\relax }}{59}{figure.caption.29}
\contentsline {figure}{\numberline {6.5}{\ignorespaces Histogram of daily rainfall in Sydney, Australia, presented on a $\qopname \relax o{ln}$ scale because the frequency of larger amounts of rainfall is significantly less. There is a peak in occurrence of the value $0$, which has a notable effect on the resulting model performance.\relax }}{60}{figure.caption.30}
\contentsline {figure}{\numberline {6.6}{\ignorespaces All models are applied to approximate the amount of rainfall expected on the next calendar day given various sources of local meteorological data, using $10$-fold cross validation. These boxes depict the median (middle bar), median $95\%$ confidence interval (cones), quartiles (box edges), fences at $3/2$ interquartile range (whiskers), and outliers (dots) of absolute prediction error for each model. The errors are presented on a $\qopname \relax o{ln}$ scale, mimicking the presentation in Figure \ref {fig:hist-weather}. The numerical data corresponding to this figure is provided in Table \ref {table:error-weather} in the Appendix.\relax }}{60}{figure.caption.31}
\contentsline {figure}{\numberline {6.7}{\ignorespaces Histogram of credit card transaction amounts, presented on a $\qopname \relax o{ln}$ scale. The data contains a notable frequency peak around $\$1$ transactions. Fewer large purchases are made, but some large purchases are in excess of five orders of magnitude greater than the smallest purchases.\relax }}{61}{figure.caption.32}
\contentsline {figure}{\numberline {6.8}{\ignorespaces All models are applied to approximate the expected transaction amount given transformed (and obfuscated) vendor and customer-descriptive features, using $10$-fold cross validation. These boxes depict the median (middle bar), median $95\%$ confidence interval (cones), quartiles (box edges), fences at $3/2$ interquartile range (whiskers), and outliers (dots) of absolute prediction error for each model. The absolute errors in transaction amount predictions are presented on a $\qopname \relax o{ln}$ scale, just as in Figure \ref {fig:hist-credit-card}. The numerical data corresponding to this figure is provided in Table \ref {table:error-credit-card} in the Appendix.\relax }}{62}{figure.caption.33}
\contentsline {figure}{\numberline {6.9}{\ignorespaces Histogram of the raw throughput values recorded during all IOzone tests across all system configurations. The distribution is skewed right, with few tests having significantly higher throughput than most others. The data is presented on a $\qopname \relax o{ln}$ scale.\relax }}{63}{figure.caption.34}
\contentsline {figure}{\numberline {6.10}{\ignorespaces The models directly capable of predicting distributions are applied to predicting the expected CDF of I/O throughput at a previously unseen system configuration, using $10$-fold cross validation. The KS statistic between the observed distribution at each system configuration and the predicted distribution is recorded and presented above. Note that the above figure is \textit {not} log-scaled like Figure \ref {fig:hist-throughput}. The numerical data corresponding to this figure is provided in Table \ref {table:error-throughput} in the Appendix.\relax }}{63}{figure.caption.35}
\contentsline {figure}{\numberline {6.11}{\ignorespaces Histograms of the prediction error for each interpolant that produces predictions as convex combinations of observed data, using $10$-fold cross validation. The histograms show the KS statistics for the predicted throughput distribution versus the actual throughput distribution. The four vertical lines represent cutoff KS statistics given $150$ samples for commonly used $p$-values 0.05, 0.01, 0.001, $10^{-6}$, respectively. All predictions to the right of a vertical line represent CDF predictions that are significantly different (by respective $p$-value) from the actual distribution according to the KS test. The numerical counterpart to this figure is presented in Table \ref {table:null-hypothesis-results}.\relax }}{64}{figure.caption.36}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {7.1}{\ignorespaces These are the feasible regions of monotonicity for cubic splines.\relax }}{70}{figure.caption.39}
\contentsline {figure}{\numberline {7.2}{\ignorespaces A demonstration of projection onto the feasible region for cubic splines.\relax }}{71}{figure.caption.40}
\contentsline {figure}{\numberline {7.3}{\ignorespaces A demonstration fit with a cubic spline.\relax }}{72}{figure.caption.41}
\contentsline {figure}{\numberline {7.4}{\ignorespaces The derivative of Figure \ref {fig:demo_projection}.\relax }}{73}{figure.caption.42}
\addvspace {10\p@ }
