
Computational variability presents itself in a variety of forms. Processes that are apparently identical in a cloud computing or high performance computing (HPC) environment may take different amounts of time to complete the same job. This variability can cause unintentional violations of service level agreements in cloud computing applications or indicate suboptimal performance in HPC applications. The sources of variability, however, are distributed throughout the system stack and often difficult to identify. The methodology presented in this work is applicable to modeling the expected variability of useful computer system performance measures without any prior knowledge of system architecture. Some examples of interesting performance measures that could be modeled with the techniques in this work include computational throughput, power consumption, processor idle time, number of context switches, and RAM usage, as well as any other numeric measure of performance.

Predicting performance variability in a computer system is a challenging problem that has primarily been attempted in one of two ways: (1) build a statistical model of the performance data collected by running experiments on the system at select settings, or (2) run artificial experiments using a simplified simulation of the target system to estimate architecture and application bottlenecks. In this work, modeling techniques rest in the first category and represent a notable increase in the ability to model precise characteristics of variability.

Many previous works attempting to model system performance have used simulated environments \cite{grobelny2007fase,wang2009simulation,wang2013towards}. \citet{grobelny2007fase} refer to statistical models as being oversimplified and not capable of capturing the true complexity of the underlying system. Historically, statistical models have been limited to modelling at most one or two system parameters and have therefore not been capable of modeling the complexity of the underlying system \cite{snavely2002framework,bailey2005performance,barker2009using,ye2010analyzing}. These limited statistical models have provided satisfactory performance in narrow application settings. However these techniques require additional code annotations, hardware abstractions, or additional assumptions about the behavior of the application in order to generate models. In contrast, the approaches that are presented here require no modifications of applications, no architectural abstractions, nor any structural descriptions of the input data being modeled. The techniques used in this work only require performance data as input.

Most existing work on performance variability has focused on operating system (OS) induced variability \cite{beckman2008benchmarking,de2007identifying}. Yet, system I/O variability has been particularly difficult for statistical models to capture \cite{bailey2005performance}. The prior work attempting to model I/O variability has been similarly limited to one or two system parameters \cite{lofstead2010managing}.

Chapters \ref{ch:naive} and \ref{ch:strong} present and evaluate applications of multivariate interpolation to the domain of computer system I/O throughput. Interpolants and regressors are used to predict the different measures of variability (e.g., mean, variance, cumulative distribution functions) for the expected I/O throughput on a system with previously unseen configurations. Beyond these I/O case studies, the techniques discussed can tractably model tens of interacting system parameters with tens of thousands of known configurations. A major contribution of this work is a modeling framework that uses multivariate interpolation to capture precise characteristics (via cumulative distribution functions) of arbitrary performance measure on any type of computer system.


\section{Broader Applications of Approximation}

Regression and interpolation are problems of considerable importance
that find applications across many fields of science. Pollution and
air quality analysis \cite{de2008field}, energy consumption management
\cite{lazos2014optimisation}, and student performance prediction
\cite{cortez2008using,lux2016applications} are a few of many
interdisciplinary applications of multivariate regression for
predictive analysis. As discussed later, these techniques can also be
applied to prediction problems related to forest fire risk assessment
\cite{cortez2007data}, Parkinson's patient clinical evaluations
\cite{tsanas2010accurate}, local rainfall and weather
\cite{williams2009rattle}, credit card transactions
\cite{pozzolo2015calibrating}, and high performance computing (HPC)
file input/output (I/O) \cite{lux2018nonparametric}.

Regression and interpolation have a considerable theoretical base in
one dimension \cite{cheney2009course}. Splines in particular are well
understood as an interpolation technique in one dimension
\cite{de1978practical}, particularly B-splines. Tensor products of
B-splines \cite{unther1996interpolating} or other basis functions have
an unfortunate exponential scaling with increasing
dimension. Exponential scaling prohibits tensor products from being
reasonably applied beyond three-dimensional data. In order to address
this dimensional scaling challenge, C. de Boor and others proposed box
splines \cite{de2013box}, of which one of the approximation techniques
in Chapter \ref{ch:boxes} of this work is composed \cite{lux2018novel}.

The theoretical foundation of low dimensional interpolation allows the
construction of strong error bounds that are absent from high
dimensional problems. Chapter \ref{ch:error} extends some known
results regarding the secant method \cite{dennis1996numerical} to
construct an interpolation error bound for problems of arbitrary
dimension. These error bounds are useful, considering the same cannot
be said for regression algorithms in general. The maximum complexity
of an interpolant is bounded by the amount of data available, while
the maximum complexity of a regressor is bounded by both the amount of
data and the chosen parametric form. For this reason, generic uniform
bounds are largely unobtainable for regression techniques on arbitrary
approximation problems, even when the approximation domain is
bounded. These generic bounds imply that interpolants may be suitable
for a broader class of approximation problems than (heuristically
chosen) regression techniques.

Aside from theoretical motivation for the use of interpolants, there
are often computational advantages as well. Interpolants do not have
the need for \textit{fitting} data, or minimizing error with respect
to model parameters. In applications where the amount of data is large
and the relative number of predictions that need to be made for a
given collection of data is small, the direct application of an
interpolant is much less computationally expensive.

In this work, multivariate interpolation is defined given a closed
convex subset $Y$ of a metrizable topological vector space with metric
$s$, some function $f:\mathbb{R}^d \rightarrow Y$ and a set of points
$X = \bigl\{x^{(1)}$, $\ldots$, $x^{(n)}\bigr\} \subset \mathbb{R}^d$,
along with associated function values $f\bigl(x^{(i)}\bigr)$. The goal
is to construct an approximation $\hat f: \mathbb{R}^d \rightarrow Y$
such that $\hat f\bigl(x^{(i)}\bigr) = f\bigl(x^{(i)}\bigr)$ for all
$i = 1$, $\ldots$, $n$. It is often the case that the form of the true
underlying function $f$ is unknown, however it is still desirable to
construct an approximation $\hat f$ with small approximation error at
$y \notin X$. The two metric spaces that will be discussed in this
work are the real numbers with metric $s(x,y) = |x-y|$, and the set of
cumulative distribution functions (CDFs) with the Kolmogorov-Smirnov
(KS) statistic \cite{lilliefors1967kolmogorov} as metric.

Multivariate regression is often used when the underlying function is
presumed to be stochastic, or stochastic error is introduced in the
evaluation of $f$. Hence, multivariate regression relaxes the
conditions of interpolation by choosing parameters $P$ defining $\hat
f(x;P)$ to minimize the error vector $\Bigl( \bigl | \hat f
\bigl(x^{(1)};P\bigr) - f\bigl(x^{(1)}\bigr) \bigr|$, $\ldots$, $\bigl
| \hat f \bigl(x^{(n)}; P\bigr) - f\bigl(x^{(n)}\bigr) \bigr | \Bigr)$
in some norm. The difficult question in the case of regression is
often what parametric form to adopt for any given application.

The most challenging problem when scaling in dimension is that the
number of possible interactions between dimensions grows
exponentially. Quantifying all possible interactions becomes
intractable, and hence beyond three-dimensional data mostly linear
models are used. That is not to say nonlinear models are absent, but
nonlinearities are often either preconceived or model pairwise
interactions between dimensions at most. Even globally nonlinear
approximations such as neural networks are constructed from
compositions of summed low-interaction functions
\cite{clevert2015fast}.

Provided the theoretical and practical motivations for exploring
interpolants, this work aims to study the empirical performance
differences between a set of scalable (moderately) interpolation
techniques and a set of common regression techniques. These techniques
are applied to a collection of moderately high dimensional problems
($5 \le d \le 30$) and the empirical results are discussed.

\section{An Initial Application of Approximation}

Performance tuning is often an experimentally complex and time
intensive chore necessary for configuring High Performance Computing
(HPC) systems. The procedures for this tuning vary largely from system
to system and are often subjectively guided by the system
engineer(s). Once a desired level of performance is achieved, an HPC
system may only be incrementally reconfigured as required by updates
or specific jobs. In the case that a system has changing workloads or
nonstationary performance objectives that range from maximizing
computational throughput to minimizing power consumption and system
variability, it becomes clear that a more effective and automated tool
is needed for configuring systems. This scenario presents a
challenging and important application of multivariate approximation
and interpolation techniques.

Among the statistical models presented in prior works for modeling computer systems,
\cite{bailey2005performance} specifically mention that it is difficult
for simplified models to capture variability introduced by
I/O. System variability in general has become a problem of increasing
interest to the HPC and systems communities, however most of the work
has focused on operating system (OS) induced variability
\cite{beckman2008benchmarking,de2007identifying}. The work that has
focused on managing I/O variability does not use any sophisticated
modeling techniques \cite{lofstead2010managing}. Hence, Chapter
\ref{ch:naive} presents a case study applying advanced mathematical
and statistical modeling techniques to the domain of HPC I/O
characteristics. The models are used to predict the mean throughput of
a system and the variance in throughput of a system. The discussion
section that follows outlines how the techniques presented can be
applied to any performance metric and any system.
