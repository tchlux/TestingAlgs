\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces On the left above is a depiction of a Delaunay triangulation over four points, notice that the circumball (shaded circle) for the left simplex does not contain the fourth point. On the right above, a non-Delaunay mesh is depicted. Notice that the circumball for the top simplex (shaded circle, clipped at bottom edge of the visual) contains the fourth point which violates the Delaunay condition for a simplex. \vspace {-.1cm}\relax }}{7}{figure.caption.4}% 
\contentsline {figure}{\numberline {2.2}{\ignorespaces On the left above is a depiction of the radius of influence for three chosen points of a collection in two dimensions using the modified Shepard criteria. On the right a third axis shows the relative weight for the center most interpolation point $x^{(i)}$ with the solid line representing its radius of influence, where $W_i(x)$ is $0$ for $\delimiter "026B30D x-x^{(i)}\delimiter "026B30D _2 \geq r_i$ and $W_i(x) / \DOTSB \sum@ \slimits@ _{k=1}^n W_k(x) \to 1$ as $x \to x^{(i)}$. \vspace {-.1cm}\relax }}{8}{figure.caption.5}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Histograms of 100-bin reductions of the PMF of I/O throughput mean (top) and I/O throughput variance (bottom). In the mean plot, the first 1\% bin (truncated in plot) has a probability mass of .45. In the variance plot, the second 1\% bin has a probability mass of .58. It can be seen that the distributions of throughputs are primarily of lower magnitude with occasional extreme outliers.\relax }}{11}{figure.caption.7}% 
\contentsline {figure}{\numberline {3.2}{\ignorespaces These box plots show the prediction error of mean with increasing dimension. The top box whisker for SVR is 40, 80, 90 for dimensions 2, 3, and 4, respectively. Notice that each model consistently experiences greater magnitude error with increasing dimension. Results for all training percentages are aggregated.\relax }}{14}{figure.caption.9}% 
\contentsline {figure}{\numberline {3.3}{\ignorespaces These box plots show the prediction error of mean with increasing amounts of training data provided to the models. Notice that MARS is the only model whose primary spread of performance increases with more training data. Recall that the response values being predicted span three orders of magnitude and hence relative errors should certainly remain within that range. For SVR the top box whisker goes from around 100 to 50 from left to right and is truncated in order to maintain focus on better models. Results for all dimensions are aggregated. Max training percentage is 96\% due to rounding training set size.\relax }}{15}{figure.caption.10}% 
\contentsline {figure}{\numberline {3.4}{\ignorespaces These box plots show the prediction error of variance with increasing amounts of training data provided to the models. The response values being predicted span six orders of magnitude. For SVR the top box whisker goes from around 6000 to 400 (decreasing by factors of 2) from left to right and is truncated in order to maintain focus on better models. Results for all dimensions are aggregated. Max training percentage is 96\% due to rounding training set size.\relax }}{16}{figure.caption.11}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces 1D linear (order 2) and quadratic (order 3) box splines with direction vector sets $\mathopen \leavevmode@ifvmode {\setbox \z@ \hbox {\mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1 \ 1 \mathclose \leavevmode@ifvmode {\setbox \z@ \hbox {\mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$ and $\mathopen \leavevmode@ifvmode {\setbox \z@ \hbox {\mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1 \ 1 \ 1 \mathclose \leavevmode@ifvmode {\setbox \z@ \hbox {\mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$ respectively. Notice that these direction vector sets form the B-Spline analogues, order 2 composed of two linear components and order 3 composed of 3 quadratic components (colored and styled in plot).\relax }}{19}{figure.caption.12}% 
\contentsline {figure}{\numberline {4.2}{\ignorespaces 2D linear (order 2) and quadratic (order 3) box splines with direction vector sets $\mathopen \leavevmode@ifvmode {\setbox \z@ \hbox {\mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } I \ I \mathclose \leavevmode@ifvmode {\setbox \z@ \hbox {\mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$ and $\mathopen \leavevmode@ifvmode {\setbox \z@ \hbox {\mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } I \ I \ I \mathclose \leavevmode@ifvmode {\setbox \z@ \hbox {\mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$ respectively, where $I$ is the identity matrix in two dimensions. Notice that these direction vector sets also produce boxes with $\text {order}^2$ subregions (colored in plot).\relax }}{19}{figure.caption.13}% 
\contentsline {figure}{\numberline {4.3}{\ignorespaces An example box in two dimensions with anchor $c$, upper widths $u^c_1$, $u^c_2$, and lower widths $l^c_1$, $l^c_2$. Notice that $c$ is not required to be equidistant from opposing sides of the box, that is $u^c_i \not = l^c_i$ is allowed.\relax }}{20}{figure.caption.14}% 
\contentsline {figure}{\numberline {4.4}{\ignorespaces Above is a depiction of the Voronoi cell boundaries (dashed lines) about a set of interpolation points (dots) in two dimensions. In this example, the Voronoi mesh basis function about the center most point has nonzero weight in the shaded region and transitions from a value of one at the point to zero at the boundary of the twice expanded Voronoi cell (solid line). \vspace {-.1cm}\relax }}{24}{figure.caption.15}% 
\contentsline {figure}{\numberline {4.5}{\ignorespaces Histograms of Parkinsons (total UPDRS), forest fire (area), and HPC I/O (mean throughput) response values respectively. Notice that both the forest fire and HPC I/O data sets are heavily skewed. \vspace {-.5cm}\relax }}{25}{figure.caption.16}% 
\contentsline {figure}{\numberline {4.6}{\ignorespaces Time required to generate model fits for each technique with varying relative error tolerance during bootstrapping. \vspace {-.3cm}\relax }}{26}{figure.caption.17}% 
\contentsline {figure}{\numberline {4.7}{\ignorespaces The performance of all three techniques with varied relative error tolerance for the bootstrapping parameter. The columns are for Max Box Mesh, Iterative Box Mesh, and Voronoi Mesh, respectively. The rows are for HPC I/O, Forest Fire, and Parkinson's respectively. Notice the techniques' behavior on the Parkinson's and Forest Fire data sets, performance increases with larger error tolerance.\relax }}{28}{figure.caption.19}% 
\contentsline {figure}{\numberline {4.8}{\ignorespaces A sample of relative errors for all three techniques with optimal selections of error tolerance. The columns are for Max Box Mesh, Iterative Box Mesh, and Voronoi Mesh, respectively. The rows are for HPC I/O, Forest Fire, and Parkinson's respectively. \vspace {-.3cm}\relax }}{29}{figure.caption.20}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces In this HPC I/O example, the general methodology for predicting a CDF and evaluating error can be seen, where M means $\times 10^6$. The Delaunay method chose three source distributions (dotted lines) and assigned weights \{.1, .3, .6\} (top to bottom at middle). The weighted sum of the three known CDFs produces the predicted CDF (dashed line). The KS Statistic (vertical line) computed between the true CDF (solid line) and predicted CDF (dashed line) is 0.2 for this example. For this example the KS test null hypothesis is rejected at $p$-value 0.01, however it is not rejected at $p$-value 0.001. \vspace {-.1cm}\relax }}{32}{figure.caption.21}% 
\contentsline {figure}{\numberline {5.2}{\ignorespaces Histogram of the raw throughput values recorded during all IOzone tests across all system configurations. The distribution is skewed right, with few tests having significantly higher throughput than most others.\relax }}{34}{figure.caption.23}% 
\contentsline {figure}{\numberline {5.3}{\ignorespaces Histograms of the prediction error for each modeling algorithm from ten random splits when trained with 80\% of the data aggregated over all different test types. The distributions show the KS statistics for the predicted throughput distribution versus the actual throughput distribution. The four vertical red lines represent commonly used $p$-values \{0.05, 0.01, 0.001, 1.0e-6\} respectively. All predictions to the right of a red line represent CDF predictions that are significantly different (by respective $p$-value) from the actual distribution according to the KS test.\relax }}{35}{figure.caption.24}% 
\contentsline {figure}{\numberline {5.4}{\ignorespaces The performance of each algorithm on the KS test ($p=0.001$) with increasing amounts of training data averaged over all IOzone test types and ten random splits of the data. The training percentages range from 5\% to 95\% in increments of 5\%. Delaunay is the best performer until 95\% of data is used for training, at which Max Box mesh becomes the best performer by a fraction of a percent.\relax }}{37}{figure.caption.26}% 
\contentsline {figure}{\numberline {5.5}{\ignorespaces The percentage of null hypothesis rejections for predictions made by each algorithm on the KS test ($p=0.001$) over different IOzone test types with increasing amounts of training data. Each percentage of null hypothesis rejections is an average over ten random splits of the data. The training percentages range from 5\% to 95\% in increments of 5\%. The read test types tend to allow lower rejection rates than the write test types.\relax }}{40}{figure.caption.28}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces Three different scenarios visualizing {\it Lemma \ref {lemma:3}}, where $g(t)$ is the difference between a piecewise linear interpolant and the approximated function along a normalized line segment between interpolation points, $g'(t)$ is $\gamma _g$-Lipschitz continuous, and $w$ and $\mathaccentV {tilde}07Et$ are defined in the proof. Leftmost is a randomly chosen permissible shape of $g$ and $g'$. The middle is the only possible shape of $g$ and $g'$ given $g'(0) = \gamma _g/ 2$, establishing the case of equality in the lemma. Rightmost is the resulting contradiction when $g'(0) > \gamma _g/ 2$, notice it is impossible to ensure $g'(t)$ is $\gamma _g$-Lipschitz continuous and satisfy $g(1) = 0$ (highlighted with red circle on the right). \vspace {-.1cm}\relax }}{44}{figure.caption.29}% 
\contentsline {figure}{\numberline {6.2}{\ignorespaces Delaunay and MLP approximations are constructed from Fekete points over the unit cube evaluating the test function $f(x) = \qopname \relax o{cos}(\delimiter "026B30D x\delimiter "026B30D _2)$ for $x \in \mathbb {R}^2$. The figure shows the first/third quartiles at the box bottom/top, the second quartile (median) at the white bar, median 95\% confidence interval (cones, barely visible in figure), and whiskers at 3/2 of the adjacent interquartile ranges, for the absolute prediction error for each model at $1000$ random evaluation points. The left plot observes a perfect interpolation problem with exact evaluations of $f.$ The right plot observes a regression problem with uniform random noise giving values in $[.9 f(x),\ 1.1f(x)]$ for each $x.$ Both axes are log scaled.\relax }}{46}{figure.caption.30}% 
\contentsline {figure}{\numberline {6.3}{\ignorespaces Delaunay and MLP approximations are constructed from Fekete points over the unit cube evaluating the test function $f(x) = \qopname \relax o{cos}(\delimiter "026B30D x\delimiter "026B30D _2)$ for $x \in \mathbb {R}^{20}$. The details are the same as for Fig. \ref {fig:convergence-2d}.\relax }}{48}{figure.caption.31}% 
\contentsline {figure}{\numberline {6.4}{\ignorespaces The distribution of absolute error, distance to the nearest data point, smallest singular value (SV) and the longest edge of the simplex containing each approximation point in the tests from Fig. \ref {fig:convergence-2d} and Fig. \ref {fig:convergence-20d} for Delaunay. In two dimensions it can be seen that $\delimiter "026B30D z - x_0\delimiter "026B30D _2$, $\sigma _d$, and $k$ all shrink at the same rate for well-spaced approximation points, resulting in a faster rate of decrease for approximation error. Notice that in higher dimension the data remains sparse even with thousands of data points, and the decay in data spacing is more prominant. The relatively small reduction in $k$ along with the decrease in $\sigma _d$ explain the minimal reduction in error seen by Delaunay in Fig. \ref {fig:convergence-20d}.\relax }}{49}{figure.caption.32}% 
\contentsline {figure}{\numberline {6.5}{\ignorespaces Histogram of forest fire area burned under recorded weather conditions. The data is presented on a $\qopname \relax o{ln}$ scale because most values are small with exponentially fewer fires on record that burn large areas.\relax }}{51}{figure.caption.33}% 
\contentsline {figure}{\numberline {6.6}{\ignorespaces All models are applied to approximate the amount of area that would be burned given environment conditions. $10$-fold cross validation as described in the beginning of Section \ref {sec:error_data} is used to evaluated each algorithm. This results in exactly one prediction from each algorithm for each data point. These boxes depict the median (middle bar), median $95\%$ confidence interval (cones), quartiles (box edges), fences at $3/2$ interquartile range (whiskers), and outliers (dots) of absolute prediction error for each model. Similar to Figure \ref {fig:hist-forest-fire}, the errors are presented on a $\qopname \relax o{ln}$ scale. The numerical data corresponding to this figure is provided in Table \ref {table:error-forest-fire} in the Appendix.\relax }}{51}{figure.caption.33}% 
\contentsline {figure}{\numberline {6.7}{\ignorespaces Histogram of the Parkinson's patient total UPDRS clinical scores that will be approximated by each algorithm.\relax }}{53}{figure.caption.34}% 
\contentsline {figure}{\numberline {6.8}{\ignorespaces All models are applied to approximate the total UPDRS score given audio features from patients' home life, using $10$-fold cross validation. These boxes depict the median (middle bar), median $95\%$ confidence interval (cones), quartiles (box edges), fences at $3/2$ interquartile range (whiskers), and outliers (dots) of absolute prediction error for each model. The numerical data corresponding to this figure is provided in Table \ref {table:error-parkinsons} in the Appendix.\relax }}{53}{figure.caption.34}% 
\contentsline {figure}{\numberline {6.9}{\ignorespaces Histogram of daily rainfall in Sydney, Australia, presented on a $\qopname \relax o{ln}$ scale because the frequency of larger amounts of rainfall is significantly less. There is a peak in occurrence of the value $0$, which has a notable effect on the resulting model performance.\relax }}{54}{figure.caption.35}% 
\contentsline {figure}{\numberline {6.10}{\ignorespaces All models are applied to approximate the amount of rainfall expected on the next calendar day given various sources of local meteorological data, using $10$-fold cross validation. These boxes depict the median (middle bar), median $95\%$ confidence interval (cones), quartiles (box edges), fences at $3/2$ interquartile range (whiskers), and outliers (dots) of absolute prediction error for each model. The errors are presented on a $\qopname \relax o{ln}$ scale, mimicking the presentation in Figure \ref {fig:hist-weather}. The numerical data corresponding to this figure is provided in Table \ref {table:error-weather} in the Appendix.\relax }}{54}{figure.caption.35}% 
\contentsline {figure}{\numberline {6.11}{\ignorespaces Histogram of credit card transaction amounts, presented on a $\qopname \relax o{ln}$ scale. The data contains a notable frequency peak around $\$1$ transactions. Fewer large purchases are made, but some large purchases are in excess of five orders of magnitude greater than the smallest purchases.\relax }}{55}{figure.caption.36}% 
\contentsline {figure}{\numberline {6.12}{\ignorespaces All models are applied to approximate the expected transaction amount given transformed (and obfuscated) vendor and customer-descriptive features, using $10$-fold cross validation. These boxes depict the median (middle bar), median $95\%$ confidence interval (cones), quartiles (box edges), fences at $3/2$ interquartile range (whiskers), and outliers (dots) of absolute prediction error for each model. The absolute errors in transaction amount predictions are presented on a $\qopname \relax o{ln}$ scale, just as in Figure \ref {fig:hist-credit-card}. The numerical data corresponding to this figure is provided in Table \ref {table:error-credit-card} in the Appendix.\relax }}{55}{figure.caption.36}% 
\contentsline {figure}{\numberline {6.13}{\ignorespaces Histogram of the raw throughput values recorded during all IOzone tests across all system configurations. The distribution is skewed right, with few tests having significantly higher throughput than most others. The data is presented on a $\qopname \relax o{ln}$ scale.\relax }}{56}{figure.caption.37}% 
\contentsline {figure}{\numberline {6.14}{\ignorespaces The models directly capable of predicting distributions are applied to predicting the expected CDF of I/O throughput at a previously unseen system configuration, using $10$-fold cross validation. The KS statistic (max norm) between the observed distribution at each system configuration and the predicted distribution is recorded and presented above. Note that the above figure is \textit {not} log-scaled like Figure \ref {fig:hist-throughput}. The numerical data corresponding to this figure is provided in Table \ref {table:error-throughput} in the Appendix.\relax }}{57}{figure.caption.38}% 
\contentsline {figure}{\numberline {6.15}{\ignorespaces Histograms of the prediction error for each interpolant that produces predictions as convex combinations of observed data, using $10$-fold cross validation. The histograms show the KS statistics for the predicted throughput distribution versus the actual throughput distribution. The four vertical lines represent cutoff KS statistics given $150$ samples for commonly used $p$-values 0.05, 0.01, 0.001, $10^{-6}$, respectively. All predictions to the right of a vertical line represent CDF predictions that are significantly different (by respective $p$-value) from the actual distribution according to the KS test. The numerical counterpart to this figure is presented in Table \ref {table:null-hypothesis-results}.\relax }}{58}{figure.caption.39}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {7.1}{\ignorespaces A demonstration of the quadratic facet model's sensitivity to small data perturbations. This example is composed of two quadratic functions $f_1(x) = x^2$ over points $\{1$, $2$, $5/2\}$, and $f_2(x) = (x-2)^2 + 6$ over points $\{5/2$, $3$, $4\}$. Notably, $f_1(5/2) = f_2(5/2)$ and $f_1$, $f_2$ have the same curvature. Given the exact five data points seen above, the quadratic facet model produces the slope seen in the solid blue line at $x = 5/2$. However, by subtracting the value of $f_3$ $= \epsilon (x-2)^2$ from points at $x$ $= \{3$, $4\}$, where $\epsilon $ is the machine precision ($2^{-52}$ for an IEEE 64-bit real), the quadratic facet model produces the slope seen in the dashed red line at $x = 5/2$. This is the nature of a facet model and a side effect of associating data with local facets.\relax }}{68}{figure.caption.42}% 
\contentsline {figure}{\numberline {7.2}{\ignorespaces {\tt MQSI} results for three of the functions in the included test suite. The {\it piecewise polynomial} function (top) shows the interpolant capturing local linear segments, local flats, and alternating extreme points. The {\it large tangent} (middle) problem demonstrates outcomes on rapidly changing segments of data. The {\it signal decay} (bottom) alternates between extreme values of steadily decreasing magnitude.\relax }}{69}{figure.caption.43}% 
\contentsline {figure}{\numberline {7.3}{\ignorespaces {\tt MQSI} results when approximating the cumulative distribution function of system throughput (bytes per second) data for a computer with a 3.2 GHz CPU performing file read operations from Cameron et al. \cite {cameron2019moana}. The empirical distribution of 30 thousand throughput values is shown in the red dashed line, while the solid line with stylized markers denotes the approximation made with MQSI given equally spaced empirical distribution points from a sample of size 100.\relax }}{70}{figure.caption.44}% 
\contentsline {figure}{\numberline {7.4}{\ignorespaces The {\it random monotone} test poses a particularly challenging problem with large variations in slope. Notice that despite drastic shifts in slope, the resulting monotone quintic spline interpolant provides smooth and reasonable estimates to function values between data.\relax }}{70}{figure.caption.45}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {A.1}{\ignorespaces Scatter plots for predicted versus actual values for the top three models on each of the four real valued approximation problems. Top left is forest fire data, top right is Parkinson's data, bottom left is rainfall data, and bottom right is credit card transaction data. The blue circles correspond to the best model, green diamonds to the second best, and red crosses to the third best model for each data set. There are a large of number of 0-valued entries in the forest fire and rainfall data sets that are not included in the visuals making the true ranking of the models appear to disagree with the observed outcomes.\relax }}{86}{figure.caption.48}% 
