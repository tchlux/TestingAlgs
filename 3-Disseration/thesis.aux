\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{vii}{chapter*.2}\protected@file@percent }
\citation{cameron2019moana}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{xiii}{chapter*.3}\protected@file@percent }
\citation{grobelny2007fase,wang2009simulation,wang2013towards}
\citation{grobelny2007fase}
\citation{snavely2002framework,bailey2005performance,barker2009using,ye2010analyzing}
\citation{beckman2008benchmarking,de2007identifying}
\citation{bailey2005performance}
\citation{lofstead2010managing}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}The Importance and Applications of Variability}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:apps}{{1}{1}{The Importance and Applications of Variability}{chapter.1}{}}
\citation{de2008field}
\citation{lazos2014optimisation}
\citation{cortez2008using,lux2016applications}
\citation{cortez2007data}
\citation{tsanas2010accurate}
\citation{williams2009rattle}
\citation{pozzolo2015calibrating}
\citation{lux2018nonparametric}
\citation{cheney2009course}
\citation{de1978practical}
\citation{unther1996interpolating}
\citation{de2013box}
\citation{lux2018novel}
\citation{dennis1996numerical}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Broader Applications of Approximation}{2}{section.1.1}\protected@file@percent }
\citation{lilliefors1967kolmogorov}
\citation{clevert2015fast}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}An Initial Application of Approximation}{3}{section.1.2}\protected@file@percent }
\citation{bailey2005performance}
\citation{beckman2008benchmarking,de2007identifying}
\citation{lofstead2010managing}
\citation{friedman1991multivariate}
\citation{stanford1993fast}
\citation{rudy2017pyearth}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Algorithms for Constructing Approximations}{5}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:algs}{{2}{5}{Algorithms for Constructing Approximations}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Multivariate Regression}{5}{section.2.1}\protected@file@percent }
\newlabel{sec:regression}{{2.1}{5}{Multivariate Regression}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Multivariate Adaptive Regression Splines}{5}{subsection.2.1.1}\protected@file@percent }
\newlabel{sec:mars}{{2.1.1}{5}{Multivariate Adaptive Regression Splines}{subsection.2.1.1}{}}
\citation{basak2007support}
\citation{scikit-learn}
\citation{hornik1989multilayer,rumelhart1988learning}
\citation{dahl2013improving}
\citation{goh2017why,moller1993scaled,robbins1951stochastic}
\citation{tensorflow2015-whitepaper,chollet2015keras}
\citation{lee1980two}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Support Vector Regressor}{6}{subsection.2.1.2}\protected@file@percent }
\newlabel{sec:svr}{{2.1.2}{6}{Support Vector Regressor}{subsection.2.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Multilayer Perceptron Regressor}{6}{subsection.2.1.3}\protected@file@percent }
\newlabel{sec:mlp}{{2.1.3}{6}{Multilayer Perceptron Regressor}{subsection.2.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Multivariate Interpolation}{6}{section.2.2}\protected@file@percent }
\newlabel{sec:interpolation}{{2.2}{6}{Multivariate Interpolation}{section.2.2}{}}
\citation{chang2018polynomial}
\citation{cover1967nearest}
\citation{gordon1978shepard,shepard1968two}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces On the left above is a depiction of a Delaunay triangulation over four points, notice that the circumball (shaded circle) for the left simplex does not contain the fourth point. On the right above, a non-Delaunay mesh is depicted. Notice that the circumball for the top simplex (shaded circle, clipped at bottom edge of the visual) contains the fourth point which violates the Delaunay condition for a simplex. \vspace  {-.1cm}\relax }}{7}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:delaunay}{{2.1}{7}{On the left above is a depiction of a Delaunay triangulation over four points, notice that the circumball (shaded circle) for the left simplex does not contain the fourth point. On the right above, a non-Delaunay mesh is depicted. Notice that the circumball for the top simplex (shaded circle, clipped at bottom edge of the visual) contains the fourth point which violates the Delaunay condition for a simplex. \vspace {-.1cm}\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Delaunay}{7}{subsection.2.2.1}\protected@file@percent }
\newlabel{sec:delaunay}{{2.2.1}{7}{Delaunay}{subsection.2.2.1}{}}
\citation{thacker2010algorithm}
\citation{thacker2010algorithm}
\citation{thacker2010algorithm}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces On the left above is a depiction of the radius of influence for three chosen points of a collection in two dimensions using the modified Shepard criteria. On the right a third axis shows the relative weight for the center most interpolation point $x^{(i)}$ with the solid line representing its radius of influence, where $W_i(x)$ is $0$ for $\delimiter "026B30D x-x^{(i)}\delimiter "026B30D _2 \geq r_i$ and $W_i(x) / \DOTSB \sum@ \slimits@ _{k=1}^n W_k(x) \to 1$ as $x \to x^{(i)}$. \vspace  {-.1cm}\relax }}{8}{figure.caption.5}\protected@file@percent }
\newlabel{fig:shepard}{{2.2}{8}{On the left above is a depiction of the radius of influence for three chosen points of a collection in two dimensions using the modified Shepard criteria. On the right a third axis shows the relative weight for the center most interpolation point $x^{(i)}$ with the solid line representing its radius of influence, where $W_i(x)$ is $0$ for $\|x-x^{(i)}\|_2 \geq r_i$ and $W_i(x) / \sum _{k=1}^n W_k(x) \to 1$ as $x \to x^{(i)}$. \vspace {-.1cm}\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Modified Shepard}{8}{subsection.2.2.2}\protected@file@percent }
\newlabel{sec:modified-shepard}{{2.2.2}{8}{Modified Shepard}{subsection.2.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Linear Shepard}{8}{subsection.2.2.3}\protected@file@percent }
\citation{iozone}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Na\"{\i }ve Approximations of Variability}{10}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:naive}{{3}{10}{Na\"{\i }ve Approximations of Variability}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}I/O Data}{10}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Dimensional Analysis}{10}{subsection.3.1.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces A description of the system parameters being considered in the IOzone tests. Record size must not be greater than file size and hence there are only six valid combinations of the two. In total there are $6 \times 9 \times 16 = 864$ unique system configurations.\relax }}{11}{table.caption.6}\protected@file@percent }
\newlabel{tab:data_type}{{3.1}{11}{A description of the system parameters being considered in the IOzone tests. Record size must not be greater than file size and hence there are only six valid combinations of the two. In total there are $6 \times 9 \times 16 = 864$ unique system configurations.\relax }{table.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Histograms of 100-bin reductions of the PMF of I/O throughput mean (top) and I/O throughput variance (bottom). In the mean plot, the first 1\% bin (truncated in plot) has a probability mass of .45. In the variance plot, the second 1\% bin has a probability mass of .58. It can be seen that the distributions of throughputs are primarily of lower magnitude with occasional extreme outliers.\relax }}{11}{figure.caption.7}\protected@file@percent }
\newlabel{fig:raw_throughput}{{3.1}{11}{Histograms of 100-bin reductions of the PMF of I/O throughput mean (top) and I/O throughput variance (bottom). In the mean plot, the first 1\% bin (truncated in plot) has a probability mass of .45. In the variance plot, the second 1\% bin has a probability mass of .58. It can be seen that the distributions of throughputs are primarily of lower magnitude with occasional extreme outliers.\relax }{figure.caption.7}{}}
\citation{amos2014algorithm}
\@writefile{toc}{\contentsline {subsubsection}{Multidimensional Analysis}{12}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Na\"{\i }ve Variability Modeling Results}{13}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}I/O Throughput Mean}{13}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}I/O Throughput Variance}{13}{subsection.3.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces These box plots show the prediction error of mean with increasing dimension. The top box whisker for SVR is 40, 80, 90 for dimensions 2, 3, and 4, respectively. Notice that each model consistently experiences greater magnitude error with increasing dimension. Results for all training percentages are aggregated.\relax }}{14}{figure.caption.9}\protected@file@percent }
\newlabel{fig:mean_dim}{{3.2}{14}{These box plots show the prediction error of mean with increasing dimension. The top box whisker for SVR is 40, 80, 90 for dimensions 2, 3, and 4, respectively. Notice that each model consistently experiences greater magnitude error with increasing dimension. Results for all training percentages are aggregated.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Increasing Dimension and Decreasing Training Data}{14}{subsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Discussion of Na\"{\i }ve Approximations}{14}{section.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces These box plots show the prediction error of mean with increasing amounts of training data provided to the models. Notice that MARS is the only model whose primary spread of performance increases with more training data. Recall that the response values being predicted span three orders of magnitude and hence relative errors should certainly remain within that range. For SVR the top box whisker goes from around 100 to 50 from left to right and is truncated in order to maintain focus on better models. Results for all dimensions are aggregated. Max training percentage is 96\% due to rounding training set size.\relax }}{15}{figure.caption.10}\protected@file@percent }
\newlabel{fig:mean_tt_ratio}{{3.3}{15}{These box plots show the prediction error of mean with increasing amounts of training data provided to the models. Notice that MARS is the only model whose primary spread of performance increases with more training data. Recall that the response values being predicted span three orders of magnitude and hence relative errors should certainly remain within that range. For SVR the top box whisker goes from around 100 to 50 from left to right and is truncated in order to maintain focus on better models. Results for all dimensions are aggregated. Max training percentage is 96\% due to rounding training set size.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces These box plots show the prediction error of variance with increasing amounts of training data provided to the models. The response values being predicted span six orders of magnitude. For SVR the top box whisker goes from around 6000 to 400 (decreasing by factors of 2) from left to right and is truncated in order to maintain focus on better models. Results for all dimensions are aggregated. Max training percentage is 96\% due to rounding training set size.\relax }}{16}{figure.caption.11}\protected@file@percent }
\newlabel{fig:var_tt_ratio}{{3.4}{16}{These box plots show the prediction error of variance with increasing amounts of training data provided to the models. The response values being predicted span six orders of magnitude. For SVR the top box whisker goes from around 6000 to 400 (decreasing by factors of 2) from left to right and is truncated in order to maintain focus on better models. Results for all dimensions are aggregated. Max training percentage is 96\% due to rounding training set size.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Modeling the System}{16}{subsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Extending the Analysis}{17}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Takeaway From Na\"{\i }ve Approximation}{17}{section.3.4}\protected@file@percent }
\newlabel{sec:conclusion}{{3.4}{17}{Takeaway From Na\"{\i }ve Approximation}{section.3.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Box Splines: Uses, Constructions, and Applications}{18}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:boxes}{{4}{18}{Box Splines: Uses, Constructions, and Applications}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Box Splines}{18}{section.4.1}\protected@file@percent }
\newlabel{sec_box_splines}{{4.1}{18}{Box Splines}{section.4.1}{}}
\newlabel{eq_box_base}{{4.1}{18}{Box Splines}{equation.4.1.1}{}}
\newlabel{eq_box_recursive}{{4.2}{18}{Box Splines}{equation.4.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces 1D linear (order 2) and quadratic (order 3) box splines with direction vector sets $\mathopen \leavevmode@ifvmode {\setbox \z@ \hbox {\mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1 \ 1 \mathclose \leavevmode@ifvmode {\setbox \z@ \hbox {\mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$ and $\mathopen \leavevmode@ifvmode {\setbox \z@ \hbox {\mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } 1 \ 1 \ 1 \mathclose \leavevmode@ifvmode {\setbox \z@ \hbox {\mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$ respectively. Notice that these direction vector sets form the B-Spline analogues, order 2 composed of two linear components and order 3 composed of 3 quadratic components (colored and styled in plot).\relax }}{19}{figure.caption.12}\protected@file@percent }
\newlabel{fig_1D_boxes}{{4.1}{19}{1D linear (order 2) and quadratic (order 3) box splines with direction vector sets $\bigl ( 1 \ 1 \bigr )$ and $\bigl ( 1 \ 1 \ 1 \bigr )$ respectively. Notice that these direction vector sets form the B-Spline analogues, order 2 composed of two linear components and order 3 composed of 3 quadratic components (colored and styled in plot).\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces 2D linear (order 2) and quadratic (order 3) box splines with direction vector sets $\mathopen \leavevmode@ifvmode {\setbox \z@ \hbox {\mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } I \ I \mathclose \leavevmode@ifvmode {\setbox \z@ \hbox {\mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$ and $\mathopen \leavevmode@ifvmode {\setbox \z@ \hbox {\mathsurround \z@ $\nulldelimiterspace \z@ \left (\vcenter to\@ne \big@size {}\right .$}\box \z@ } I \ I \ I \mathclose \leavevmode@ifvmode {\setbox \z@ \hbox {\mathsurround \z@ $\nulldelimiterspace \z@ \left )\vcenter to\@ne \big@size {}\right .$}\box \z@ }$ respectively, where $I$ is the identity matrix in two dimensions. Notice that these direction vector sets also produce boxes with $\text  {order}^2$ subregions (colored in plot).\relax }}{19}{figure.caption.13}\protected@file@percent }
\newlabel{fig_2D_boxes}{{4.2}{19}{2D linear (order 2) and quadratic (order 3) box splines with direction vector sets $\bigl ( I \ I \bigr )$ and $\bigl ( I \ I \ I \bigr )$ respectively, where $I$ is the identity matrix in two dimensions. Notice that these direction vector sets also produce boxes with $\text {order}^2$ subregions (colored in plot).\relax }{figure.caption.13}{}}
\citation{de2013box}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces An example box in two dimensions with anchor $c$, upper widths $u^c_1$, $u^c_2$, and lower widths $l^c_1$, $l^c_2$. Notice that $c$ is not required to be equidistant from opposing sides of the box, that is $u^c_i \not = l^c_i$ is allowed.\relax }}{20}{figure.caption.14}\protected@file@percent }
\newlabel{fig_example_box}{{4.3}{20}{An example box in two dimensions with anchor $c$, upper widths $u^c_1$, $u^c_2$, and lower widths $l^c_1$, $l^c_2$. Notice that $c$ is not required to be equidistant from opposing sides of the box, that is $u^c_i \not = l^c_i$ is allowed.\relax }{figure.caption.14}{}}
\citation{amos2014algorithm}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Max Box Mesh}{21}{section.4.2}\protected@file@percent }
\newlabel{step_init}{{1}{21}{Max Box Mesh}{Item.6}{}}
\newlabel{step_closest}{{2}{21}{Max Box Mesh}{Item.7}{}}
\newlabel{step_shrink}{{3}{21}{Max Box Mesh}{Item.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Iterative Box Mesh}{22}{section.4.3}\protected@file@percent }
\citation{cover1967nearest}
\citation{dirichlet1850reduction}
\citation{dutour2009complexity}
\newlabel{step_add_box}{{2}{23}{Iterative Box Mesh}{Item.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Voronoi Mesh}{23}{section.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Above is a depiction of the Voronoi cell boundaries (dashed lines) about a set of interpolation points (dots) in two dimensions. In this example, the Voronoi mesh basis function about the center most point has nonzero weight in the shaded region and transitions from a value of one at the point to zero at the boundary of the twice expanded Voronoi cell (solid line). \vspace  {-.1cm}\relax }}{24}{figure.caption.15}\protected@file@percent }
\newlabel{fig:voronoi-basis}{{4.4}{24}{Above is a depiction of the Voronoi cell boundaries (dashed lines) about a set of interpolation points (dots) in two dimensions. In this example, the Voronoi mesh basis function about the center most point has nonzero weight in the shaded region and transitions from a value of one at the point to zero at the boundary of the twice expanded Voronoi cell (solid line). \vspace {-.1cm}\relax }{figure.caption.15}{}}
\newlabel{step_add_control}{{2}{24}{Voronoi Mesh}{Item.16}{}}
\citation{iozone}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Histograms of Parkinsons (total UPDRS), forest fire (area), and HPC I/O (mean throughput) response values respectively. Notice that both the forest fire and HPC I/O data sets are heavily skewed. \vspace  {-.5cm}\relax }}{25}{figure.caption.16}\protected@file@percent }
\newlabel{fig_response_hists}{{4.5}{25}{Histograms of Parkinsons (total UPDRS), forest fire (area), and HPC I/O (mean throughput) response values respectively. Notice that both the forest fire and HPC I/O data sets are heavily skewed. \vspace {-.5cm}\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Data and Analysis}{25}{section.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}High Performance Computing I/O ($n = 532, d = 4$)}{25}{subsection.4.5.1}\protected@file@percent }
\citation{cortez2007data}
\citation{tsanas2010accurate}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Time required to generate model fits for each technique with varying relative error tolerance during bootstrapping. \vspace  {-.3cm}\relax }}{26}{figure.caption.17}\protected@file@percent }
\newlabel{fig_eval_times}{{4.6}{26}{Time required to generate model fits for each technique with varying relative error tolerance during bootstrapping. \vspace {-.3cm}\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Forest Fire ($n = 517, d = 12$)}{26}{subsection.4.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Parkinson's Telemonitoring ($n = 468, d = 16$)}{26}{subsection.4.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.4}Performance Analysis}{26}{subsection.4.5.4}\protected@file@percent }
\newlabel{sec_performance_analysis}{{4.5.4}{26}{Performance Analysis}{subsection.4.5.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces The optimal error tolerance bootstrapping parameters for each technique and each data set as well as the average absolute relative errors achieved by that tolerance. Notice that large relative error tolerances occasionally yield even lower evaluation errors, demonstrating the benefits of approximation over interpolation for noisy data sets. \vspace  {-.5cm}\relax }}{27}{table.caption.18}\protected@file@percent }
\newlabel{tab_optimal_tolerance}{{4.1}{27}{The optimal error tolerance bootstrapping parameters for each technique and each data set as well as the average absolute relative errors achieved by that tolerance. Notice that large relative error tolerances occasionally yield even lower evaluation errors, demonstrating the benefits of approximation over interpolation for noisy data sets. \vspace {-.5cm}\relax }{table.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces The performance of all three techniques with varied relative error tolerance for the bootstrapping parameter. The columns are for Max Box Mesh, Iterative Box Mesh, and Voronoi Mesh, respectively. The rows are for HPC I/O, Forest Fire, and Parkinson's respectively. Notice the techniques' behavior on the Parkinson's and Forest Fire data sets, performance increases with larger error tolerance.\relax }}{28}{figure.caption.19}\protected@file@percent }
\newlabel{fig_all_performance}{{4.7}{28}{The performance of all three techniques with varied relative error tolerance for the bootstrapping parameter. The columns are for Max Box Mesh, Iterative Box Mesh, and Voronoi Mesh, respectively. The rows are for HPC I/O, Forest Fire, and Parkinson's respectively. Notice the techniques' behavior on the Parkinson's and Forest Fire data sets, performance increases with larger error tolerance.\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces A sample of relative errors for all three techniques with optimal selections of error tolerance. The columns are for Max Box Mesh, Iterative Box Mesh, and Voronoi Mesh, respectively. The rows are for HPC I/O, Forest Fire, and Parkinson's respectively. \vspace  {-.3cm}\relax }}{29}{figure.caption.20}\protected@file@percent }
\newlabel{fig_perf_sample}{{4.8}{29}{A sample of relative errors for all three techniques with optimal selections of error tolerance. The columns are for Max Box Mesh, Iterative Box Mesh, and Voronoi Mesh, respectively. The rows are for HPC I/O, Forest Fire, and Parkinson's respectively. \vspace {-.3cm}\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Discussion of Mesh Approximations}{30}{section.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Implications of Quasi-Mesh Results}{30}{section.4.7}\protected@file@percent }
\citation{lilliefors1967kolmogorov}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Stronger Approximations of Variability}{31}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:strong}{{5}{31}{Stronger Approximations of Variability}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Measuring Error}{31}{section.5.1}\protected@file@percent }
\newlabel{sec:error}{{5.1}{31}{Measuring Error}{section.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces In this HPC I/O example, the general methodology for predicting a CDF and evaluating error can be seen, where M means $\times 10^6$. The Delaunay method chose three source distributions (dotted lines) and assigned weights \{.1, .3, .6\} (top to bottom at middle). The weighted sum of the three known CDFs produces the predicted CDF (dashed line). The KS Statistic (vertical line) computed between the true CDF (solid line) and predicted CDF (dashed line) is 0.2 for this example. For this example the KS test null hypothesis is rejected at $p$-value 0.01, however it is not rejected at $p$-value 0.001. \vspace  {-.1cm}\relax }}{32}{figure.caption.21}\protected@file@percent }
\newlabel{fig:prediction-example}{{5.1}{32}{In this HPC I/O example, the general methodology for predicting a CDF and evaluating error can be seen, where M means $\times 10^6$. The Delaunay method chose three source distributions (dotted lines) and assigned weights \{.1, .3, .6\} (top to bottom at middle). The weighted sum of the three known CDFs produces the predicted CDF (dashed line). The KS Statistic (vertical line) computed between the true CDF (solid line) and predicted CDF (dashed line) is 0.2 for this example. For this example the KS test null hypothesis is rejected at $p$-value 0.01, however it is not rejected at $p$-value 0.001. \vspace {-.1cm}\relax }{figure.caption.21}{}}
\citation{guyon2003introduction}
\citation{pudil1994floating}
\citation{ferri1994comparative}
\citation{iozone}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces A description of system parameters considered for IOzone. Record size must be $\leq $ file size during execution.\relax }}{33}{table.caption.22}\protected@file@percent }
\newlabel{tab:data_description}{{5.1}{33}{A description of system parameters considered for IOzone. Record size must be $\leq $ file size during execution.\relax }{table.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Feature Weighting}{33}{subsection.5.1.1}\protected@file@percent }
\newlabel{sec:feature_weighting}{{5.1.1}{33}{Feature Weighting}{subsection.5.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Variability Data}{33}{section.5.2}\protected@file@percent }
\newlabel{sec:data}{{5.2}{33}{Variability Data}{section.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Histogram of the raw throughput values recorded during all IOzone tests across all system configurations. The distribution is skewed right, with few tests having significantly higher throughput than most others.\relax }}{34}{figure.caption.23}\protected@file@percent }
\newlabel{fig:throughput_histogram}{{5.2}{34}{Histogram of the raw throughput values recorded during all IOzone tests across all system configurations. The distribution is skewed right, with few tests having significantly higher throughput than most others.\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Distribution Prediction Results}{34}{section.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Histograms of the prediction error for each modeling algorithm from ten random splits when trained with 80\% of the data aggregated over all different test types. The distributions show the KS statistics for the predicted throughput distribution versus the actual throughput distribution. The four vertical red lines represent commonly used $p$-values \{0.05, 0.01, 0.001, 1.0e-6\} respectively. All predictions to the right of a red line represent CDF predictions that are significantly different (by respective $p$-value) from the actual distribution according to the KS test.\relax }}{35}{figure.caption.24}\protected@file@percent }
\newlabel{fig:ks_histogram_80_20}{{5.3}{35}{Histograms of the prediction error for each modeling algorithm from ten random splits when trained with 80\% of the data aggregated over all different test types. The distributions show the KS statistics for the predicted throughput distribution versus the actual throughput distribution. The four vertical red lines represent commonly used $p$-values \{0.05, 0.01, 0.001, 1.0e-6\} respectively. All predictions to the right of a red line represent CDF predictions that are significantly different (by respective $p$-value) from the actual distribution according to the KS test.\relax }{figure.caption.24}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Percent of null hypothesis rejections rate by the KS-test when provided different selections of $p$-values. These accompany the percent of null hypothesis rejection results from Figure \ref  {fig:ks_histogram_80_20}.\relax }}{36}{table.caption.25}\protected@file@percent }
\newlabel{tab:p_value_failure_rate}{{5.2}{36}{Percent of null hypothesis rejections rate by the KS-test when provided different selections of $p$-values. These accompany the percent of null hypothesis rejection results from Figure \ref {fig:ks_histogram_80_20}.\relax }{table.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces The performance of each algorithm on the KS test ($p=0.001$) with increasing amounts of training data averaged over all IOzone test types and ten random splits of the data. The training percentages range from 5\% to 95\% in increments of 5\%. Delaunay is the best performer until 95\% of data is used for training, at which Max Box mesh becomes the best performer by a fraction of a percent.\relax }}{37}{figure.caption.26}\protected@file@percent }
\newlabel{fig:ks_failure_by_training}{{5.4}{37}{The performance of each algorithm on the KS test ($p=0.001$) with increasing amounts of training data averaged over all IOzone test types and ten random splits of the data. The training percentages range from 5\% to 95\% in increments of 5\%. Delaunay is the best performer until 95\% of data is used for training, at which Max Box mesh becomes the best performer by a fraction of a percent.\relax }{figure.caption.26}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces The null hypothesis rejection rates for various $p$-values with the KS-test. These results are strictly for the ``readers'' IOzone test type and show unweighted results as well as the results with weights tuned for minimum error (KS statistic) by 300 iterations of simulated annealing. Notice that the weights identified for the Delaunay model cause data dependent tuning, reducing performance. MaxBoxMesh performance is improved by a negligible amount. VoronoiMesh performance is notably improved.\relax }}{38}{table.caption.27}\protected@file@percent }
\newlabel{tab:optimized_p_value_failure_rate}{{5.3}{38}{The null hypothesis rejection rates for various $p$-values with the KS-test. These results are strictly for the ``readers'' IOzone test type and show unweighted results as well as the results with weights tuned for minimum error (KS statistic) by 300 iterations of simulated annealing. Notice that the weights identified for the Delaunay model cause data dependent tuning, reducing performance. MaxBoxMesh performance is improved by a negligible amount. VoronoiMesh performance is notably improved.\relax }{table.caption.27}{}}
\citation{patel2009service}
\citation{ali2015security}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Discussion of Distribution Prediction}{39}{section.5.4}\protected@file@percent }
\newlabel{sec:discussion}{{5.4}{39}{Discussion of Distribution Prediction}{section.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces The percentage of null hypothesis rejections for predictions made by each algorithm on the KS test ($p=0.001$) over different IOzone test types with increasing amounts of training data. Each percentage of null hypothesis rejections is an average over ten random splits of the data. The training percentages range from 5\% to 95\% in increments of 5\%. The read test types tend to allow lower rejection rates than the write test types.\relax }}{40}{figure.caption.28}\protected@file@percent }
\newlabel{fig:ks_failure_by_training_and_test}{{5.5}{40}{The percentage of null hypothesis rejections for predictions made by each algorithm on the KS test ($p=0.001$) over different IOzone test types with increasing amounts of training data. Each percentage of null hypothesis rejections is an average over ten random splits of the data. The training percentages range from 5\% to 95\% in increments of 5\%. The read test types tend to allow lower rejection rates than the write test types.\relax }{figure.caption.28}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}The Power of Distribution Prediction}{41}{section.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}An Error Bound on Piecewise Linear Interpolation}{42}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:error}{{6}{42}{An Error Bound on Piecewise Linear Interpolation}{chapter.6}{}}
\newlabel{lemma:1}{{1}{42}{An Error Bound on Piecewise Linear Interpolation}{plainlemma.1}{}}
\newlabel{lemma:2}{{2}{42}{An Error Bound on Piecewise Linear Interpolation}{plainlemma.2}{}}
\newlabel{lemma:3}{{3}{43}{An Error Bound on Piecewise Linear Interpolation}{plainlemma.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Three different scenarios visualizing {\it  Lemma \ref  {lemma:3}}, where $g(t)$ is the difference between a piecewise linear interpolant and the approximated function along a normalized line segment between interpolation points, $g'(t)$ is $\gamma _g$-Lipschitz continuous, and $w$ and $\mathaccentV {tilde}07Et$ are defined in the proof. Leftmost is a randomly chosen permissible shape of $g$ and $g'$. The middle is the only possible shape of $g$ and $g'$ given $g'(0) = \gamma _g/ 2$, establishing the case of equality in the lemma. Rightmost is the resulting contradiction when $g'(0) > \gamma _g/ 2$, notice it is impossible to ensure $g'(t)$ is $\gamma _g$-Lipschitz continuous and satisfy $g(1) = 0$ (highlighted with red circle on the right). \vspace  {-.1cm}\relax }}{44}{figure.caption.29}\protected@file@percent }
\newlabel{fig:lemma3}{{6.1}{44}{Three different scenarios visualizing {\it Lemma \ref {lemma:3}}, where $g(t)$ is the difference between a piecewise linear interpolant and the approximated function along a normalized line segment between interpolation points, $g'(t)$ is $\g $-Lipschitz continuous, and $w$ and $\tilde t$ are defined in the proof. Leftmost is a randomly chosen permissible shape of $g$ and $g'$. The middle is the only possible shape of $g$ and $g'$ given $g'(0) = \g / 2$, establishing the case of equality in the lemma. Rightmost is the resulting contradiction when $g'(0) > \g / 2$, notice it is impossible to ensure $g'(t)$ is $\g $-Lipschitz continuous and satisfy $g(1) = 0$ (highlighted with red circle on the right). \vspace {-.1cm}\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Delaunay and MLP approximations are constructed from Fekete points over the unit cube evaluating the test function $f(x) = \qopname  \relax o{cos}(\delimiter "026B30D x\delimiter "026B30D _2)$ for $x \in \mathbb  {R}^2$. The figure shows the first/third quartiles at the box bottom/top, the second quartile (median) at the white bar, median 95\% confidence interval (cones, barely visible in figure), and whiskers at 3/2 of the adjacent interquartile ranges, for the absolute prediction error for each model at $1000$ random evaluation points. The left plot observes a perfect interpolation problem with exact evaluations of $f.$ The right plot observes a regression problem with uniform random noise giving values in $[.9 f(x),\ 1.1f(x)]$ for each $x.$ Both axes are log scaled.\relax }}{46}{figure.caption.30}\protected@file@percent }
\newlabel{fig:convergence-2d}{{6.2}{46}{Delaunay and MLP approximations are constructed from Fekete points over the unit cube evaluating the test function $f(x) = \cos (\|x\|_2)$ for $x \in \mathbb {R}^2$. The figure shows the first/third quartiles at the box bottom/top, the second quartile (median) at the white bar, median 95\% confidence interval (cones, barely visible in figure), and whiskers at 3/2 of the adjacent interquartile ranges, for the absolute prediction error for each model at $1000$ random evaluation points. The left plot observes a perfect interpolation problem with exact evaluations of $f.$ The right plot observes a regression problem with uniform random noise giving values in $[.9 f(x),\ 1.1f(x)]$ for each $x.$ Both axes are log scaled.\relax }{figure.caption.30}{}}
\citation{barthelmann2000high}
\citation{park1994optimal}
\citation{kovari_pommerenke_1968}
\citation{bos2010computing}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Demonstration on an Analytic Test Function}{47}{section.6.1}\protected@file@percent }
\newlabel{sec:analytic}{{6.1}{47}{Demonstration on an Analytic Test Function}{section.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Delaunay and MLP approximations are constructed from Fekete points over the unit cube evaluating the test function $f(x) = \qopname  \relax o{cos}(\delimiter "026B30D x\delimiter "026B30D _2)$ for $x \in \mathbb  {R}^{20}$. The details are the same as for Fig. \ref  {fig:convergence-2d}.\relax }}{48}{figure.caption.31}\protected@file@percent }
\newlabel{fig:convergence-20d}{{6.3}{48}{Delaunay and MLP approximations are constructed from Fekete points over the unit cube evaluating the test function $f(x) = \cos (\|x\|_2)$ for $x \in \mathbb {R}^{20}$. The details are the same as for Fig. \ref {fig:convergence-2d}.\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces The distribution of absolute error, distance to the nearest data point, smallest singular value (SV) and the longest edge of the simplex containing each approximation point in the tests from Fig. \ref  {fig:convergence-2d} and Fig. \ref  {fig:convergence-20d} for Delaunay. In two dimensions it can be seen that $\delimiter "026B30D z - x_0\delimiter "026B30D _2$, $\sigma _d$, and $k$ all shrink at the same rate for well-spaced approximation points, resulting in a faster rate of decrease for approximation error. Notice that in higher dimension the data remains sparse even with thousands of data points, and the decay in data spacing is more prominant. The relatively small reduction in $k$ along with the decrease in $\sigma _d$ explain the minimal reduction in error seen by Delaunay in Fig. \ref  {fig:convergence-20d}.\relax }}{49}{figure.caption.32}\protected@file@percent }
\newlabel{fig:data-spacing}{{6.4}{49}{The distribution of absolute error, distance to the nearest data point, smallest singular value (SV) and the longest edge of the simplex containing each approximation point in the tests from Fig. \ref {fig:convergence-2d} and Fig. \ref {fig:convergence-20d} for Delaunay. In two dimensions it can be seen that $\|z - x_0\|_2$, $\sigma _d$, and $k$ all shrink at the same rate for well-spaced approximation points, resulting in a faster rate of decrease for approximation error. Notice that in higher dimension the data remains sparse even with thousands of data points, and the decay in data spacing is more prominant. The relatively small reduction in $k$ along with the decrease in $\sigma _d$ explain the minimal reduction in error seen by Delaunay in Fig. \ref {fig:convergence-20d}.\relax }{figure.caption.32}{}}
\citation{kohavi1995study}
\citation{kohavi1995study}
\citation{bengio2004no}
\citation{cortez2007data}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Data and Empirical Analysis}{50}{section.6.2}\protected@file@percent }
\newlabel{sec:error_data}{{6.2}{50}{Data and Empirical Analysis}{section.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Forest Fire ($n = 504$, $d = 12$)}{50}{subsection.6.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Histogram of forest fire area burned under recorded weather conditions. The data is presented on a $\qopname  \relax o{ln}$ scale because most values are small with exponentially fewer fires on record that burn large areas.\relax }}{51}{figure.caption.33}\protected@file@percent }
\newlabel{fig:hist-forest-fire}{{6.5}{51}{Histogram of forest fire area burned under recorded weather conditions. The data is presented on a $\ln $ scale because most values are small with exponentially fewer fires on record that burn large areas.\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces All models are applied to approximate the amount of area that would be burned given environment conditions. $10$-fold cross validation as described in the beginning of Section \ref  {sec:error_data} is used to evaluated each algorithm. This results in exactly one prediction from each algorithm for each data point. These boxes depict the median (middle bar), median $95\%$ confidence interval (cones), quartiles (box edges), fences at $3/2$ interquartile range (whiskers), and outliers (dots) of absolute prediction error for each model. Similar to Figure \ref  {fig:hist-forest-fire}, the errors are presented on a $\qopname  \relax o{ln}$ scale. The numerical data corresponding to this figure is provided in Table \ref  {table:error-forest-fire} in the Appendix.\relax }}{51}{figure.caption.33}\protected@file@percent }
\newlabel{fig:error-forest-fire}{{6.6}{51}{All models are applied to approximate the amount of area that would be burned given environment conditions. $10$-fold cross validation as described in the beginning of Section \ref {sec:error_data} is used to evaluated each algorithm. This results in exactly one prediction from each algorithm for each data point. These boxes depict the median (middle bar), median $95\%$ confidence interval (cones), quartiles (box edges), fences at $3/2$ interquartile range (whiskers), and outliers (dots) of absolute prediction error for each model. Similar to Figure \ref {fig:hist-forest-fire}, the errors are presented on a $\ln $ scale. The numerical data corresponding to this figure is provided in Table \ref {table:error-forest-fire} in the Appendix.\relax }{figure.caption.33}{}}
\citation{tsanas2010accurate}
\citation{williams2009rattle}
\citation{pozzolo2015calibrating}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Parkinson's Telemonitoring ($n = 5875$, $d = 19$)}{52}{subsection.6.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}Australian Daily Rainfall Volume ($n = 2609$, $d = 23$)}{52}{subsection.6.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.4}Credit Card Transaction Amount ($n = 5562$, $d = 28$)}{52}{subsection.6.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Histogram of the Parkinson's patient total UPDRS clinical scores that will be approximated by each algorithm.\relax }}{53}{figure.caption.34}\protected@file@percent }
\newlabel{fig:hist-parkinsons}{{6.7}{53}{Histogram of the Parkinson's patient total UPDRS clinical scores that will be approximated by each algorithm.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces All models are applied to approximate the total UPDRS score given audio features from patients' home life, using $10$-fold cross validation. These boxes depict the median (middle bar), median $95\%$ confidence interval (cones), quartiles (box edges), fences at $3/2$ interquartile range (whiskers), and outliers (dots) of absolute prediction error for each model. The numerical data corresponding to this figure is provided in Table \ref  {table:error-parkinsons} in the Appendix.\relax }}{53}{figure.caption.34}\protected@file@percent }
\newlabel{fig:error-parkinsons}{{6.8}{53}{All models are applied to approximate the total UPDRS score given audio features from patients' home life, using $10$-fold cross validation. These boxes depict the median (middle bar), median $95\%$ confidence interval (cones), quartiles (box edges), fences at $3/2$ interquartile range (whiskers), and outliers (dots) of absolute prediction error for each model. The numerical data corresponding to this figure is provided in Table \ref {table:error-parkinsons} in the Appendix.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Histogram of daily rainfall in Sydney, Australia, presented on a $\qopname  \relax o{ln}$ scale because the frequency of larger amounts of rainfall is significantly less. There is a peak in occurrence of the value $0$, which has a notable effect on the resulting model performance.\relax }}{54}{figure.caption.35}\protected@file@percent }
\newlabel{fig:hist-weather}{{6.9}{54}{Histogram of daily rainfall in Sydney, Australia, presented on a $\ln $ scale because the frequency of larger amounts of rainfall is significantly less. There is a peak in occurrence of the value $0$, which has a notable effect on the resulting model performance.\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces All models are applied to approximate the amount of rainfall expected on the next calendar day given various sources of local meteorological data, using $10$-fold cross validation. These boxes depict the median (middle bar), median $95\%$ confidence interval (cones), quartiles (box edges), fences at $3/2$ interquartile range (whiskers), and outliers (dots) of absolute prediction error for each model. The errors are presented on a $\qopname  \relax o{ln}$ scale, mimicking the presentation in Figure \ref  {fig:hist-weather}. The numerical data corresponding to this figure is provided in Table \ref  {table:error-weather} in the Appendix.\relax }}{54}{figure.caption.35}\protected@file@percent }
\newlabel{fig:error-weather}{{6.10}{54}{All models are applied to approximate the amount of rainfall expected on the next calendar day given various sources of local meteorological data, using $10$-fold cross validation. These boxes depict the median (middle bar), median $95\%$ confidence interval (cones), quartiles (box edges), fences at $3/2$ interquartile range (whiskers), and outliers (dots) of absolute prediction error for each model. The errors are presented on a $\ln $ scale, mimicking the presentation in Figure \ref {fig:hist-weather}. The numerical data corresponding to this figure is provided in Table \ref {table:error-weather} in the Appendix.\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces Histogram of credit card transaction amounts, presented on a $\qopname  \relax o{ln}$ scale. The data contains a notable frequency peak around $\$1$ transactions. Fewer large purchases are made, but some large purchases are in excess of five orders of magnitude greater than the smallest purchases.\relax }}{55}{figure.caption.36}\protected@file@percent }
\newlabel{fig:hist-credit-card}{{6.11}{55}{Histogram of credit card transaction amounts, presented on a $\ln $ scale. The data contains a notable frequency peak around $\$1$ transactions. Fewer large purchases are made, but some large purchases are in excess of five orders of magnitude greater than the smallest purchases.\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces All models are applied to approximate the expected transaction amount given transformed (and obfuscated) vendor and customer-descriptive features, using $10$-fold cross validation. These boxes depict the median (middle bar), median $95\%$ confidence interval (cones), quartiles (box edges), fences at $3/2$ interquartile range (whiskers), and outliers (dots) of absolute prediction error for each model. The absolute errors in transaction amount predictions are presented on a $\qopname  \relax o{ln}$ scale, just as in Figure \ref  {fig:hist-credit-card}. The numerical data corresponding to this figure is provided in Table \ref  {table:error-credit-card} in the Appendix.\relax }}{55}{figure.caption.36}\protected@file@percent }
\newlabel{fig:error-credit-card}{{6.12}{55}{All models are applied to approximate the expected transaction amount given transformed (and obfuscated) vendor and customer-descriptive features, using $10$-fold cross validation. These boxes depict the median (middle bar), median $95\%$ confidence interval (cones), quartiles (box edges), fences at $3/2$ interquartile range (whiskers), and outliers (dots) of absolute prediction error for each model. The absolute errors in transaction amount predictions are presented on a $\ln $ scale, just as in Figure \ref {fig:hist-credit-card}. The numerical data corresponding to this figure is provided in Table \ref {table:error-credit-card} in the Appendix.\relax }{figure.caption.36}{}}
\citation{cameron2019moana}
\citation{iozone}
\citation{fritsch1980monotone}
\@writefile{lof}{\contentsline {figure}{\numberline {6.13}{\ignorespaces Histogram of the raw throughput values recorded during all IOzone tests across all system configurations. The distribution is skewed right, with few tests having significantly higher throughput than most others. The data is presented on a $\qopname  \relax o{ln}$ scale.\relax }}{56}{figure.caption.37}\protected@file@percent }
\newlabel{fig:hist-throughput}{{6.13}{56}{Histogram of the raw throughput values recorded during all IOzone tests across all system configurations. The distribution is skewed right, with few tests having significantly higher throughput than most others. The data is presented on a $\ln $ scale.\relax }{figure.caption.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.5}High Performance Computing I/O ($n = 3016$, $d = 4$)}{56}{subsection.6.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.14}{\ignorespaces The models directly capable of predicting distributions are applied to predicting the expected CDF of I/O throughput at a previously unseen system configuration, using $10$-fold cross validation. The KS statistic (max norm) between the observed distribution at each system configuration and the predicted distribution is recorded and presented above. Note that the above figure is \textit  {not} log-scaled like Figure \ref  {fig:hist-throughput}. The numerical data corresponding to this figure is provided in Table \ref  {table:error-throughput} in the Appendix.\relax }}{57}{figure.caption.38}\protected@file@percent }
\newlabel{fig:error-throughput}{{6.14}{57}{The models directly capable of predicting distributions are applied to predicting the expected CDF of I/O throughput at a previously unseen system configuration, using $10$-fold cross validation. The KS statistic (max norm) between the observed distribution at each system configuration and the predicted distribution is recorded and presented above. Note that the above figure is \textit {not} log-scaled like Figure \ref {fig:hist-throughput}. The numerical data corresponding to this figure is provided in Table \ref {table:error-throughput} in the Appendix.\relax }{figure.caption.38}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Numerical counterpart of the histogram data presented in Figure \ref  {fig:throughput-prediction}. The columns display the percent of null hypothesis rejections by the KS-test when provided different selections of $p$-values for each algorithm. The algorithm with the lowest rejection rate at each $p$ is boldface, while the second lowest is italicized.\relax }}{57}{table.caption.40}\protected@file@percent }
\newlabel{table:null-hypothesis-results}{{6.1}{57}{Numerical counterpart of the histogram data presented in Figure \ref {fig:throughput-prediction}. The columns display the percent of null hypothesis rejections by the KS-test when provided different selections of $p$-values for each algorithm. The algorithm with the lowest rejection rate at each $p$ is boldface, while the second lowest is italicized.\relax }{table.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.15}{\ignorespaces Histograms of the prediction error for each interpolant that produces predictions as convex combinations of observed data, using $10$-fold cross validation. The histograms show the KS statistics for the predicted throughput distribution versus the actual throughput distribution. The four vertical lines represent cutoff KS statistics given $150$ samples for commonly used $p$-values 0.05, 0.01, 0.001, $10^{-6}$, respectively. All predictions to the right of a vertical line represent CDF predictions that are significantly different (by respective $p$-value) from the actual distribution according to the KS test. The numerical counterpart to this figure is presented in Table \ref  {table:null-hypothesis-results}.\relax }}{58}{figure.caption.39}\protected@file@percent }
\newlabel{fig:throughput-prediction}{{6.15}{58}{Histograms of the prediction error for each interpolant that produces predictions as convex combinations of observed data, using $10$-fold cross validation. The histograms show the KS statistics for the predicted throughput distribution versus the actual throughput distribution. The four vertical lines represent cutoff KS statistics given $150$ samples for commonly used $p$-values 0.05, 0.01, 0.001, $10^{-6}$, respectively. All predictions to the right of a vertical line represent CDF predictions that are significantly different (by respective $p$-value) from the actual distribution according to the KS test. The numerical counterpart to this figure is presented in Table \ref {table:null-hypothesis-results}.\relax }{figure.caption.39}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces This average of Appendix Tables \ref  {table:best-forest-fire}, \ref  {table:best-parkinsons}, \ref  {table:best-weather}, and \ref  {table:best-credit-card} provides a gross summary of overall results. The columns display (weighted equally by data set, \textit  {not} points) the average frequency with which any algorithm provided the lowest absolute error approximation, the average time to fit/prepare, and the average time required to approximate one point. The times have been rounded to one significant digit, as reasonably large fluctuations may be observed due to implementation hardware. Interpolants provide the lowest error approximation for nearly one third of all data, while regressors occupy the other two thirds. This result is obtained without any customized tuning or preprocessing to maximize the performance of any given algorithm. In practice, tuning and preprocessing may have large effects on approximation performance.\relax }}{59}{table.caption.41}\protected@file@percent }
\newlabel{table:avg-performance}{{6.2}{59}{This average of Appendix Tables \ref {table:best-forest-fire}, \ref {table:best-parkinsons}, \ref {table:best-weather}, and \ref {table:best-credit-card} provides a gross summary of overall results. The columns display (weighted equally by data set, \textit {not} points) the average frequency with which any algorithm provided the lowest absolute error approximation, the average time to fit/prepare, and the average time required to approximate one point. The times have been rounded to one significant digit, as reasonably large fluctuations may be observed due to implementation hardware. Interpolants provide the lowest error approximation for nearly one third of all data, while regressors occupy the other two thirds. This result is obtained without any customized tuning or preprocessing to maximize the performance of any given algorithm. In practice, tuning and preprocessing may have large effects on approximation performance.\relax }{table.caption.41}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Discussion of Empirical Results}{59}{section.6.3}\protected@file@percent }
\newlabel{sec:discussion}{{6.3}{59}{Discussion of Empirical Results}{section.6.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Takeaway from Empirical Results}{60}{section.6.4}\protected@file@percent }
\newlabel{sec:error_conclusion}{{6.4}{60}{Takeaway from Empirical Results}{section.6.4}{}}
\citation{herman2006techniques,quint2003scalable}
\citation{brennan2020measure}
\citation{brennan2020measure}
\citation{berglund2009planning}
\citation{knott2000interpolating}
\citation{fritsch1980monotone,gregory1985shape}
\citation{ramsay1988monotone}
\citation{he1998monotone}
\citation{fritsch1980monotone}
\citation{wang2004rational,abd2011improved,yao2018unconditionally}
\citation{leitenstorfer2007generalized}
\citation{fritsch1982piecewise}
\citation{ulrich1994positivity,hess1994positive}
\citation{lux2020algorithm}
\citation{ulrich1994positivity}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}A Package for Monotone Quintic Spline Interpolation}{61}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:splines}{{7}{61}{A Package for Monotone Quintic Spline Interpolation}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Relevance of Quintic Splines}{61}{section.7.1}\protected@file@percent }
\citation{fritsch1980monotone}
\citation{haralick1981facet}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Monotone Quintic Interpolation}{62}{section.7.2}\protected@file@percent }
\citation{ulrich1994positivity}
\citation{hess1994positive}
\citation{ulrich1994positivity}
\citation{ulrich1994positivity}
\citation{ulrich1994positivity}
\citation{de1978practical}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Spline Representation}{67}{section.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Complexity and Sensitivity}{67}{section.7.4}\protected@file@percent }
\citation{cameron2019moana}
\citation{cameron2019moana}
\citation{cameron2019moana}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces A demonstration of the quadratic facet model's sensitivity to small data perturbations. This example is composed of two quadratic functions $f_1(x) = x^2$ over points $\{1$, $2$, $5/2\}$, and $f_2(x) = (x-2)^2 + 6$ over points $\{5/2$, $3$, $4\}$. Notably, $f_1(5/2) = f_2(5/2)$ and $f_1$, $f_2$ have the same curvature. Given the exact five data points seen above, the quadratic facet model produces the slope seen in the solid blue line at $x = 5/2$. However, by subtracting the value of $f_3$ $= \epsilon (x-2)^2$ from points at $x$ $= \{3$, $4\}$, where $\epsilon $ is the machine precision ($2^{-52}$ for an IEEE 64-bit real), the quadratic facet model produces the slope seen in the dashed red line at $x = 5/2$. This is the nature of a facet model and a side effect of associating data with local facets.\relax }}{68}{figure.caption.42}\protected@file@percent }
\newlabel{splines:fig-1}{{7.1}{68}{A demonstration of the quadratic facet model's sensitivity to small data perturbations. This example is composed of two quadratic functions $f_1(x) = x^2$ over points $\{1$, $2$, $5/2\}$, and $f_2(x) = (x-2)^2 + 6$ over points $\{5/2$, $3$, $4\}$. Notably, $f_1(5/2) = f_2(5/2)$ and $f_1$, $f_2$ have the same curvature. Given the exact five data points seen above, the quadratic facet model produces the slope seen in the solid blue line at $x = 5/2$. However, by subtracting the value of $f_3$ $= \epsilon (x-2)^2$ from points at $x$ $= \{3$, $4\}$, where $\epsilon $ is the machine precision ($2^{-52}$ for an IEEE 64-bit real), the quadratic facet model produces the slope seen in the dashed red line at $x = 5/2$. This is the nature of a facet model and a side effect of associating data with local facets.\relax }{figure.caption.42}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Performance and Applications}{68}{section.7.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces {\tt  MQSI} results for three of the functions in the included test suite. The {\it  piecewise polynomial} function (top) shows the interpolant capturing local linear segments, local flats, and alternating extreme points. The {\it  large tangent} (middle) problem demonstrates outcomes on rapidly changing segments of data. The {\it  signal decay} (bottom) alternates between extreme values of steadily decreasing magnitude.\relax }}{69}{figure.caption.43}\protected@file@percent }
\newlabel{splines:test_funcs}{{7.2}{69}{{\tt MQSI} results for three of the functions in the included test suite. The {\it piecewise polynomial} function (top) shows the interpolant capturing local linear segments, local flats, and alternating extreme points. The {\it large tangent} (middle) problem demonstrates outcomes on rapidly changing segments of data. The {\it signal decay} (bottom) alternates between extreme values of steadily decreasing magnitude.\relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces {\tt  MQSI} results when approximating the cumulative distribution function of system throughput (bytes per second) data for a computer with a 3.2 GHz CPU performing file read operations from Cameron et al. \cite  {cameron2019moana}. The empirical distribution of 30 thousand throughput values is shown in the red dashed line, while the solid line with stylized markers denotes the approximation made with MQSI given equally spaced empirical distribution points from a sample of size 100.\relax }}{70}{figure.caption.44}\protected@file@percent }
\newlabel{splines:cdf}{{7.3}{70}{{\tt MQSI} results when approximating the cumulative distribution function of system throughput (bytes per second) data for a computer with a 3.2 GHz CPU performing file read operations from Cameron et al. \cite {cameron2019moana}. The empirical distribution of 30 thousand throughput values is shown in the red dashed line, while the solid line with stylized markers denotes the approximation made with MQSI given equally spaced empirical distribution points from a sample of size 100.\relax }{figure.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces The {\it  random monotone} test poses a particularly challenging problem with large variations in slope. Notice that despite drastic shifts in slope, the resulting monotone quintic spline interpolant provides smooth and reasonable estimates to function values between data.\relax }}{70}{figure.caption.45}\protected@file@percent }
\newlabel{splines:random}{{7.4}{70}{The {\it random monotone} test poses a particularly challenging problem with large variations in slope. Notice that despite drastic shifts in slope, the resulting monotone quintic spline interpolant provides smooth and reasonable estimates to function values between data.\relax }{figure.caption.45}{}}
\bibdata{thesis}
\bibcite{tensorflow2015-whitepaper}{{1}{2015}{{Abadi et~al.}}{{Abadi, Agarwal, Barham, Brevdo, Chen, Citro, Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia, Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah, Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan, Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and Zheng}}}
\bibcite{ali2015security}{{2}{2015}{{Ali et~al.}}{{Ali, Khan, and Vasilakos}}}
\bibcite{amos2014algorithm}{{3}{2014}{{Amos et~al.}}{{Amos, Easterling, Watson, Thacker, Castle, and Trosset}}}
\bibcite{bailey2005performance}{{4}{2005}{{Bailey and Snavely}}{{}}}
\bibcite{barker2009using}{{5}{2009}{{Barker et~al.}}{{Barker, Davis, Hoisie, Kerbyson, Lang, Pakin, and Sancho}}}
\bibcite{barthelmann2000high}{{6}{2000}{{Barthelmann et~al.}}{{Barthelmann, Novak, and Ritter}}}
\bibcite{basak2007support}{{7}{2007}{{Basak et~al.}}{{Basak, Pal, and Patranabis}}}
\bibcite{beckman2008benchmarking}{{8}{2008}{{Beckman et~al.}}{{Beckman, Iskra, Yoshii, Coghlan, and Nataraj}}}
\bibcite{bengio2004no}{{9}{2004}{{Bengio and Grandvalet}}{{}}}
\bibcite{berglund2009planning}{{10}{2009}{{Berglund et~al.}}{{Berglund, Brodnik, Jonsson, Staffanson, and Soderkvist}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{72}{chapter*.46}\protected@file@percent }
\bibcite{bos2010computing}{{11}{2010}{{Bos et~al.}}{{Bos, De~Marchi, Sommariva, and Vianello}}}
\bibcite{brennan2020measure}{{12}{2020}{{Brennan}}{{}}}
\bibcite{cameron2019moana}{{13}{2019}{{{Cameron} et~al.}}{{{Cameron}, {Anwar}, {Cheng}, {Xu}, {Li}, {Ananth}, {Bernard}, {Jearls}, {Lux}, {Hong}, {Watson}, and {Butt}}}}
\bibcite{chang2018polynomial}{{14}{2018}{{Chang et~al.}}{{Chang, Watson, Lux, Li, Xu, Butt, Cameron, and Hong}}}
\bibcite{cheney2009course}{{15}{2009}{{Cheney and Light}}{{}}}
\bibcite{chollet2015keras}{{16}{2015}{{Chollet et~al.}}{{}}}
\bibcite{clevert2015fast}{{17}{2015}{{Clevert et~al.}}{{Clevert, Unterthiner, and Hochreiter}}}
\bibcite{cortez2007data}{{18}{2007}{{Cortez and Morais}}{{}}}
\bibcite{cortez2008using}{{19}{2008}{{Cortez and Silva}}{{}}}
\bibcite{cover1967nearest}{{20}{1967}{{Cover and Hart}}{{}}}
\bibcite{dahl2013improving}{{21}{2013}{{Dahl et~al.}}{{Dahl, Sainath, and Hinton}}}
\bibcite{de2007identifying}{{22}{2007}{{De et~al.}}{{De, Kothari, and Mann}}}
\bibcite{de1978practical}{{23}{1978}{{de~Boor}}{{}}}
\bibcite{de2013box}{{24}{2013}{{De~Boor et~al.}}{{De~Boor, H{\"o}llig, and Riemenschneider}}}
\bibcite{de2008field}{{25}{2008}{{De~Vito et~al.}}{{De~Vito, Massera, Piga, Martinotto, and Di~Francia}}}
\bibcite{dennis1996numerical}{{26}{1996}{{Dennis~Jr and Schnabel}}{{}}}
\bibcite{dirichlet1850reduction}{{27}{1850}{{Dirichlet}}{{}}}
\bibcite{dutour2009complexity}{{28}{2009}{{Dutour~Sikiri{\'c} et~al.}}{{Dutour~Sikiri{\'c}, Sch{\"u}rmann, and Vallentin}}}
\bibcite{ferri1994comparative}{{29}{1994}{{Ferri et~al.}}{{Ferri, Pudil, Hatef, and Kittler}}}
\bibcite{friedman1991multivariate}{{30}{1991}{{Friedman}}{{}}}
\bibcite{stanford1993fast}{{31}{1993}{{Friedman and the Computational Statistics Laboritory~of Stanford~University}}{{}}}
\bibcite{fritsch1980monotone}{{32}{1980}{{Fritsch and Carlson}}{{}}}
\bibcite{fritsch1982piecewise}{{33}{1982}{{Fritsch}}{{}}}
\bibcite{goh2017why}{{34}{2017}{{Goh}}{{}}}
\bibcite{gordon1978shepard}{{35}{1978}{{Gordon and Wixom}}{{}}}
\bibcite{gregory1985shape}{{36}{1985}{{Gregory}}{{}}}
\bibcite{grobelny2007fase}{{37}{2007}{{Grobelny et~al.}}{{Grobelny, Bueno, Troxel, George, and Vetter}}}
\bibcite{guyon2003introduction}{{38}{2003}{{Guyon and Elisseeff}}{{}}}
\bibcite{haralick1981facet}{{39}{1981}{{Haralick and Watson}}{{}}}
\bibcite{he1998monotone}{{40}{1998}{{He and Shi}}{{}}}
\bibcite{herman2006techniques}{{41}{2006}{{Herman and Oftedal}}{{}}}
\bibcite{hess1994positive}{{42}{1994}{{He{\ss } and Schmidt}}{{}}}
\bibcite{hornik1989multilayer}{{43}{1989}{{Hornik et~al.}}{{Hornik, Stinchcombe, and White}}}
\bibcite{knott2000interpolating}{{44}{2012}{{Knott}}{{}}}
\bibcite{kohavi1995study}{{45}{1995}{{Kohavi et~al.}}{{}}}
\bibcite{kovari_pommerenke_1968}{{46}{1968}{{K\IeC {\"o}vari and Pommerenke}}{{}}}
\bibcite{lazos2014optimisation}{{47}{2014}{{Lazos et~al.}}{{Lazos, Sproul, and Kay}}}
\bibcite{lee1980two}{{48}{1980}{{Lee and Schachter}}{{}}}
\bibcite{leitenstorfer2007generalized}{{49}{2007}{{Leitenstorfer and Tutz}}{{}}}
\bibcite{lilliefors1967kolmogorov}{{50}{1967}{{Lilliefors}}{{}}}
\bibcite{lofstead2010managing}{{51}{2010}{{Lofstead et~al.}}{{Lofstead, Zheng, Liu, Klasky, Oldfield, Kordenbrock, Schwan, and Wolf}}}
\bibcite{lux2016applications}{{52}{2016}{{Lux et~al.}}{{Lux, Pittman, Shende, and Shende}}}
\bibcite{lux2018nonparametric}{{53}{2018{}}{{Lux et~al.}}{{Lux, Watson, Chang, Bernard, Li, Yu, Xu, Back, Butt, Cameron, et~al.}}}
\bibcite{lux2018novel}{{54}{2018{}}{{Lux et~al.}}{{Lux, Watson, Chang, Bernard, Li, Yu, Xu, Back, Butt, Cameron, et~al.}}}
\bibcite{lux2020algorithm}{{55}{2020}{{Lux et~al.}}{{Lux, Watson, Chang, Xu, Wang, and Hong}}}
\bibcite{moller1993scaled}{{56}{1993}{{M{\o }ller}}{{}}}
\bibcite{navidi_2015}{{57}{2015}{{Navidi}}{{}}}
\bibcite{iozone}{{58}{2017}{{Norcott}}{{}}}
\bibcite{park1994optimal}{{59}{1994}{{Park}}{{}}}
\bibcite{patel2009service}{{60}{2009}{{Patel et~al.}}{{Patel, Ranabahu, and Sheth}}}
\bibcite{scikit-learn}{{61}{2011}{{Pedregosa et~al.}}{{Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, and Duchesnay}}}
\bibcite{abd2011improved}{{62}{2011}{{Piah and Unsworth}}{{}}}
\bibcite{pozzolo2015calibrating}{{63}{2015}{{Pozzolo et~al.}}{{Pozzolo, Caelen, Johnson, and Bontempi}}}
\bibcite{pudil1994floating}{{64}{1994}{{Pudil et~al.}}{{Pudil, Novovi{\v {c}}ov{\'a}, and Kittler}}}
\bibcite{quint2003scalable}{{65}{2003}{{Quint}}{{}}}
\bibcite{ramsay1988monotone}{{66}{1988}{{Ramsay et~al.}}{{}}}
\bibcite{robbins1951stochastic}{{67}{1951}{{Robbins and Monro}}{{}}}
\bibcite{rudy2017pyearth}{{68}{2017}{{Rudy and Cherti}}{{}}}
\bibcite{rumelhart1988learning}{{69}{1988}{{Rumelhart et~al.}}{{Rumelhart, Hinton, Williams, et~al.}}}
\bibcite{shepard1968two}{{70}{1968}{{Shepard}}{{}}}
\bibcite{snavely2002framework}{{71}{2002}{{Snavely et~al.}}{{Snavely, Carrington, Wolter, Labarta, Badia, and Purkayastha}}}
\bibcite{thacker2010algorithm}{{72}{2010}{{Thacker et~al.}}{{Thacker, Zhang, Watson, Birch, Iyer, and Berry}}}
\bibcite{tsanas2010accurate}{{73}{2010}{{Tsanas et~al.}}{{Tsanas, Little, McSharry, and Ramig}}}
\bibcite{ulrich1994positivity}{{74}{1994}{{Ulrich and Watson}}{{}}}
\bibcite{unther1996interpolating}{{75}{1996}{{unther Greiner and Hormann}}{{}}}
\bibcite{wang2009simulation}{{76}{2009}{{Wang et~al.}}{{Wang, Butt, Pandey, and Gupta}}}
\bibcite{wang2013towards}{{77}{2013}{{Wang et~al.}}{{Wang, Khasymski, Krish, and Butt}}}
\bibcite{wang2004rational}{{78}{2004}{{Wang and Tan}}{{}}}
\bibcite{williams2009rattle}{{79}{2009}{{Williams}}{{}}}
\bibcite{yao2018unconditionally}{{80}{2018}{{Yao and Nelson}}{{}}}
\bibcite{ye2010analyzing}{{81}{2010}{{Ye et~al.}}{{Ye, Jiang, Chen, Huang, and Wang}}}
\bibstyle{plainnat}
\@writefile{toc}{\contentsline {chapter}{Appendices}{79}{section*.47}\protected@file@percent }
\citation{navidi_2015}
\@writefile{toc}{\contentsline {chapter}{Appendix \numberline {A}Error Bound Appendices}{80}{Appendix.a.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:error}{{A}{80}{Error Bound Appendices}{Appendix.a.A}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces This numerical data accompanies the visual provided in Figure \ref  {fig:error-forest-fire}. The columns of absolute error percentiles correspond to the minimum, first quartile, median, third quartile, and maximum absolute errors respectively. The minimum of each column is boldface, while the second lowest value is italicized. All values are rounded to three significant digits.\relax }}{81}{table.caption.49}\protected@file@percent }
\newlabel{table:error-forest-fire}{{A.1}{81}{This numerical data accompanies the visual provided in Figure \ref {fig:error-forest-fire}. The columns of absolute error percentiles correspond to the minimum, first quartile, median, third quartile, and maximum absolute errors respectively. The minimum of each column is boldface, while the second lowest value is italicized. All values are rounded to three significant digits.\relax }{table.caption.49}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.2}{\ignorespaces The left above shows how often each algorithm had the lowest absolute error approximating forest fire data in Table \ref  {table:error-forest-fire}. On the right columns are median fit time of 454 points, median time for one approximation, and median time approximating 50 points.\relax }}{81}{table.caption.49}\protected@file@percent }
\newlabel{table:best-forest-fire}{{A.2}{81}{The left above shows how often each algorithm had the lowest absolute error approximating forest fire data in Table \ref {table:error-forest-fire}. On the right columns are median fit time of 454 points, median time for one approximation, and median time approximating 50 points.\relax }{table.caption.49}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.3}{\ignorespaces This numerical data accompanies the visual provided in Figure \ref  {fig:error-parkinsons}. The columns of absolute error percentiles correspond to the minimum, first quartile, median, third quartile, and maximum absolute errors respectively. The minimum of each column is boldface, while the second lowest value is italicized. All values are rounded to three significant digits.\relax }}{82}{table.caption.50}\protected@file@percent }
\newlabel{table:error-parkinsons}{{A.3}{82}{This numerical data accompanies the visual provided in Figure \ref {fig:error-parkinsons}. The columns of absolute error percentiles correspond to the minimum, first quartile, median, third quartile, and maximum absolute errors respectively. The minimum of each column is boldface, while the second lowest value is italicized. All values are rounded to three significant digits.\relax }{table.caption.50}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.4}{\ignorespaces The left above shows how often each algorithm had the lowest absolute error approximating Parkinson's data in Table \ref  {table:error-parkinsons}. On the right columns are median fit time of 5288 points, median time for one approximation, and median time approximating 587 points.\relax }}{82}{table.caption.50}\protected@file@percent }
\newlabel{table:best-parkinsons}{{A.4}{82}{The left above shows how often each algorithm had the lowest absolute error approximating Parkinson's data in Table \ref {table:error-parkinsons}. On the right columns are median fit time of 5288 points, median time for one approximation, and median time approximating 587 points.\relax }{table.caption.50}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.5}{\ignorespaces This numerical data accompanies the visual provided in Figure \ref  {fig:error-weather}. The columns of absolute error percentiles correspond to the minimum, first quartile, median, third quartile, and maximum absolute errors respectively. The minimum value of each column is boldface, while the second lowest is italicized. All values are rounded to three significant digits.\relax }}{83}{table.caption.51}\protected@file@percent }
\newlabel{table:error-weather}{{A.5}{83}{This numerical data accompanies the visual provided in Figure \ref {fig:error-weather}. The columns of absolute error percentiles correspond to the minimum, first quartile, median, third quartile, and maximum absolute errors respectively. The minimum value of each column is boldface, while the second lowest is italicized. All values are rounded to three significant digits.\relax }{table.caption.51}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.6}{\ignorespaces Left table shows how often each algorithm had the lowest absolute error approximating Sydney rainfall data in Table \ref  {table:error-weather}. On the right columns are median fit time of 2349 points, median time for one approximation, and median time approximating 260 points.\relax }}{83}{table.caption.51}\protected@file@percent }
\newlabel{table:best-weather}{{A.6}{83}{Left table shows how often each algorithm had the lowest absolute error approximating Sydney rainfall data in Table \ref {table:error-weather}. On the right columns are median fit time of 2349 points, median time for one approximation, and median time approximating 260 points.\relax }{table.caption.51}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.7}{\ignorespaces This numerical data accompanies the visual provided in Figure \ref  {fig:error-credit-card}. The columns of absolute error percentiles correspond to the minimum, first quartile, median, third quartile, and maximum absolute errors respectively. The minimum value of each column is boldface, while the second lowest is italicized. All values are rounded to three significant digits.\relax }}{84}{table.caption.52}\protected@file@percent }
\newlabel{table:error-credit-card}{{A.7}{84}{This numerical data accompanies the visual provided in Figure \ref {fig:error-credit-card}. The columns of absolute error percentiles correspond to the minimum, first quartile, median, third quartile, and maximum absolute errors respectively. The minimum value of each column is boldface, while the second lowest is italicized. All values are rounded to three significant digits.\relax }{table.caption.52}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.8}{\ignorespaces The left above shows how often each algorithm had the lowest absolute error approximating credit card transaction data in Table \ref  {table:error-credit-card}. On the right columns are median fit time of 5006 points, median time for one approximation, and median time approximating 556 points.\relax }}{84}{table.caption.52}\protected@file@percent }
\newlabel{table:best-credit-card}{{A.8}{84}{The left above shows how often each algorithm had the lowest absolute error approximating credit card transaction data in Table \ref {table:error-credit-card}. On the right columns are median fit time of 5006 points, median time for one approximation, and median time approximating 556 points.\relax }{table.caption.52}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.9}{\ignorespaces This numerical data accompanies the visual provided in Figure \ref  {fig:error-throughput}. The columns of absolute error percentiles correspond to the minimum, first quartile, median, third quartile, and maximum KS statistics respectively between truth and guess for models predicting the distribution of I/O throughput that will be observed at previously unseen system configurations. The minimum value of each column is boldface, while the second lowest is italicized. All values are rounded to three significant digits.\relax }}{85}{table.caption.53}\protected@file@percent }
\newlabel{table:error-throughput}{{A.9}{85}{This numerical data accompanies the visual provided in Figure \ref {fig:error-throughput}. The columns of absolute error percentiles correspond to the minimum, first quartile, median, third quartile, and maximum KS statistics respectively between truth and guess for models predicting the distribution of I/O throughput that will be observed at previously unseen system configurations. The minimum value of each column is boldface, while the second lowest is italicized. All values are rounded to three significant digits.\relax }{table.caption.53}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.10}{\ignorespaces The left above shows how often each algorithm had the lowest KS statistic on the I/O throughput distribution data in Table \ref  {table:error-throughput}. On the right columns are median fit time of 2715 points, median time for one approximation, and median time approximating 301 points.\relax }}{85}{table.caption.53}\protected@file@percent }
\newlabel{table:best-throughput}{{A.10}{85}{The left above shows how often each algorithm had the lowest KS statistic on the I/O throughput distribution data in Table \ref {table:error-throughput}. On the right columns are median fit time of 2715 points, median time for one approximation, and median time approximating 301 points.\relax }{table.caption.53}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Scatter plots for predicted versus actual values for the top three models on each of the four real valued approximation problems. Top left is forest fire data, top right is Parkinson's data, bottom left is rainfall data, and bottom right is credit card transaction data. The blue circles correspond to the best model, green diamonds to the second best, and red crosses to the third best model for each data set. There are a large of number of 0-valued entries in the forest fire and rainfall data sets that are not included in the visuals making the true ranking of the models appear to disagree with the observed outcomes.\relax }}{86}{figure.caption.48}\protected@file@percent }
\newlabel{fig:scatter_plots}{{A.1}{86}{Scatter plots for predicted versus actual values for the top three models on each of the four real valued approximation problems. Top left is forest fire data, top right is Parkinson's data, bottom left is rainfall data, and bottom right is credit card transaction data. The blue circles correspond to the best model, green diamonds to the second best, and red crosses to the third best model for each data set. There are a large of number of 0-valued entries in the forest fire and rainfall data sets that are not included in the visuals making the true ranking of the models appear to disagree with the observed outcomes.\relax }{figure.caption.48}{}}
